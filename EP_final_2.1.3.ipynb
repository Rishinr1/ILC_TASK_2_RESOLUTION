{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_2_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_0_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_4_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_5_100.parquetSaved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_3_100.parquet\n",
      "\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_7_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_0_100.parquetSaved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_2_100.parquet\n",
      "\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_5_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_3_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_4_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_7_100.parquet\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_11\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "from decimal import Decimal  # Add this import statement\n",
    "import pyarrow.dataset as ds\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import sqlalchemy as sa\n",
    "import pyodbc\n",
    "import concurrent.futures\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()  # Start time\n",
    "\n",
    "\n",
    "# Function to flush the cache\n",
    "def flush_cache():\n",
    "    gc.collect()\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n",
    "folder_path = r'D:\\RISHIN\\14_2_1ILC_NZFL\\PLT\\Risk_Lob\\GU\\PeriodRange=1-250000'\n",
    "folder_path_gr = r'D:\\RISHIN\\14_2_1ILC_NZFL\\PLT\\Risk_Lob\\GR\\PeriodRange=1-250000'\n",
    "\n",
    "output_folder_path = r\"D:\\RISHIN\\TESTING\\TEST_11\"\n",
    "# folder_path = r'D:\\RISHIN\\13_ILC_resolution\\input\\PARQUET_FILES'\n",
    "# folder_path_gr = r'D:\\RISHIN\\13_ILC_TASK\\input\\PARQUET_FILES_GR'\n",
    "\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# speriod=int(input(\"Enter the simulation period: \"))\n",
    "# samples=int(input(\"Enter the number of samples: \"))\n",
    "# proname=(input(\"enter file suffix example : example ILC2024_NZFL_EP_PLA \"))\n",
    "# region=input(\"enter region example : example NZD  \")\n",
    "# database = input('Enter the database name IED2024_NZFL_PC_NZD_EDM240_ILCRun')\n",
    "\n",
    "speriod=50000\n",
    "samples=5\n",
    "proname=\"ILC2024_NZFL_EP_PLA\"\n",
    "region=\"NZD\"\n",
    "database = \"IED2024_NZFL_PC_NZD_EDM240_ILCRun\"\n",
    "\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "parquet_files_gr = [os.path.join(folder_path_gr, f) for f in os.listdir(folder_path_gr) if f.endswith('.parquet')]\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def delete_folder_and_files(folder_path):\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        # Delete all files inside the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        \n",
    "        # Delete the folder itself\n",
    "        os.rmdir(folder_path)\n",
    "        print(f'Successfully deleted the folder: {folder_path}')\n",
    "    else:\n",
    "        print(f'The folder {folder_path} does not exist.')\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'{proname}_{region}_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio','Admin1','Admin1_Lob','Cresta','Cresta_Lob',]\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            # Filter nested folders for 'PLT'\n",
    "            if subfolder == 'PLT':\n",
    "                filtered_nested_folders = ['Lob', 'Portfolio']\n",
    "            else:\n",
    "                filtered_nested_folders = nested_folders\n",
    "            \n",
    "            for nested_folder in filtered_nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "main_folder_path = os.path.join(output_folder_path, f'{proname}_{region}_Losses')\n",
    "\n",
    "processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "resolution_folder_path = os.path.join(processing_folder_path, 'Resolution Added')\n",
    "resolution_folder_path_gr = os.path.join(processing_folder_path, 'Resolution Added_gr')\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'Partial')   \n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'Concatenated')\n",
    "\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "def connect_to_database(server, database):\n",
    "    connection_string = f'mssql+pyodbc://{server}/{database}?driver=SQL+Server+Native+Client+11.0'\n",
    "    engine = sa.create_engine(connection_string)\n",
    "    connection = engine.connect()\n",
    "    return connection\n",
    "\n",
    "def read_parquet_file(file_path):\n",
    "    table = pq.read_table(file_path)\n",
    "    return table\n",
    "\n",
    "def fetch_database_data(connection):\n",
    "    address_query = 'SELECT ADDRESSID, ADMIN1GEOID AS Admin1Id, Admin1Name, zone1GEOID AS CrestaId, Zone1 AS CrestaName FROM Address'\n",
    "    address_df = pd.read_sql(address_query, connection)\n",
    "    address_table = pa.Table.from_pandas(address_df)\n",
    "    return address_table\n",
    "\n",
    "\n",
    "\n",
    "def join_dataframes(parquet_table, address_table):\n",
    "    parquet_df = parquet_table.to_pandas()\n",
    "    address_df = address_table.to_pandas()\n",
    "    df = parquet_df.merge(address_df, left_on='LocationId', right_on='ADDRESSID', how='left')\n",
    "    return pa.Table.from_pandas(df)\n",
    "\n",
    "def save_joined_dataframe(joined_table, output_file):\n",
    "    pq.write_table(joined_table, output_file)\n",
    "    print(f\"Saved joined file to {output_file}\")\n",
    "\n",
    "def process_file(file, address_table, output_folder):\n",
    "    gc.collect()\n",
    "\n",
    "    parquet_table = read_parquet_file(file)\n",
    "    joined_table = join_dataframes(parquet_table, address_table)\n",
    "    output_file = os.path.join(output_folder, os.path.basename(file))\n",
    "    save_joined_dataframe(joined_table, output_file)\n",
    "    del parquet_table\n",
    "    del joined_table\n",
    "    gc.collect()\n",
    "\n",
    "def process_parquet_files(folder_path, output_folder, server, database, batch_size=3):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    connection = connect_to_database(server, database)\n",
    "    address_table = fetch_database_data(connection)\n",
    "\n",
    "    parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "    connection.close()\n",
    "\n",
    "    # Process files in batches\n",
    "    for i in range(0, len(parquet_files), batch_size):\n",
    "        batch_files = parquet_files[i:i + batch_size]\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_file, file, address_table, output_folder) for file in batch_files]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                future.result()\n",
    "\n",
    "server = 'localhost'\n",
    "\n",
    "process_parquet_files(folder_path, resolution_folder_path, server, database)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "process_parquet_files(folder_path_gr, resolution_folder_path_gr, server, database)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#Admin_1 Lob\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "parquet_files_grp = [os.path.join(resolution_folder_path, f) for f in os.listdir(resolution_folder_path) if f.endswith('.parquet')]\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "parquet_files_grp_gr = [os.path.join(resolution_folder_path_gr, f) for f in os.listdir(resolution_folder_path_gr) if f.endswith('.parquet')]\n",
    "\n",
    "\n",
    "# In[97]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_lobdet_data(server, database):\n",
    "    connection = connect_to_database(server, database)\n",
    "    try:\n",
    "        lobdet_query = 'SELECT LOBNAME, LOBDETID FROM lobdet'\n",
    "        lobdet_df = pd.read_sql(lobdet_query, connection)\n",
    "        lobname_to_lobid_2 = dict(zip(lobdet_df.LOBNAME, lobdet_df.LOBDETID))\n",
    "    finally:\n",
    "        connection.close()\n",
    "    return lobname_to_lobid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobname_to_lobid=fetch_lobdet_data(server, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize an empty dictionary to store LobName to LobId mappings\n",
    "#use this method when no database is invloved\n",
    "# lobname_to_lobid = {}\n",
    "\n",
    "# # Initialize an empty list to store unique rows\n",
    "# unique_rows = []\n",
    "\n",
    "# # Initialize a set to track unique LobName values\n",
    "# unique_lobnames = set()\n",
    "\n",
    "# def find_dic(file_path):\n",
    "#     table = pq.read_table(file_path)\n",
    "#     for row in table.to_pandas().itertuples(index=False):\n",
    "#         lobname = row.LobName\n",
    "#         lobid = row.LobId\n",
    "#         if lobname not in unique_lobnames:\n",
    "#             unique_lobnames.add(lobname)\n",
    "#             lobname_to_lobid[lobname] = lobid\n",
    "#             unique_rows.append(row)\n",
    "\n",
    "# # Process files in parallel\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     executor.map(find_dic, parquet_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lobname_to_lobid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_resolution\\EP_final_2.1.3.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y103sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m rps_values \u001b[39m=\u001b[39m [\u001b[39m10000\u001b[39m, \u001b[39m5000\u001b[39m, \u001b[39m1000\u001b[39m, \u001b[39m500\u001b[39m, \u001b[39m250\u001b[39m, \u001b[39m200\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m25\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m2\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y103sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m \u001b[39m#GU\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y103sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (lobname, lobid) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(lobname_to_lobid\u001b[39m.\u001b[39mitems()):\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y103sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m     parquet_file_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mproname\u001b[39m}\u001b[39;00m\u001b[39m_EP_Admin1_Lob_GU_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.parquet\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y103sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lobname_to_lobid' is not defined"
     ]
    }
   ],
   "source": [
    "#EP_ Admin1 Lob updated below\n",
    "\n",
    "\n",
    "# In[129]:\n",
    "\n",
    "\n",
    "def process_parquet_files_2(parquet_files, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        \n",
    "        # Skip if the filtered table is empty\n",
    "        \n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['Admin1Id']\n",
    "        admin1_name = row['Admin1Name']\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['Admin1Id'], admin1_id))\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "                                \n",
    "                closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "                \n",
    "                # Assign the closest value to the new DataFrames\n",
    "                fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "                fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "                \n",
    "                # Update the closest value to match the rp value exactly\n",
    "                fdataframe_2.at[closest_index_2, 'RPs'] = float(value)\n",
    "                fdataframe_3.at[closest_index_2, 'RPs'] = float(value)\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'Admin1Name', 'Admin1Id']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "        # Add LobID and LobName columns\n",
    "        final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "        final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "        final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "        final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].astype('int64')\n",
    "        final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "        \n",
    "\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('Admin1Id',pa.int64(), nullable=True),\n",
    "            pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "            pa.field('LOBName', pa.string(), nullable=True),\n",
    "            pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "        underscore_count = parquet_file_path.count('_')\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Admin1_Lob',Cat)\n",
    "        parquet_file_path = os.path.join(export_path, f\"{os.path.splitext(parquet_file_path)[0]}_{idx}.parquet\")\n",
    "\n",
    "        # If there are 21 or more underscores, modify the file path\n",
    "        if underscore_count >= 9:\n",
    "            parts = parquet_file_path.split('_')\n",
    "            # Remove the second last part which contains the number and the underscore before it\n",
    "            parts = parts[:-2] + parts[-1:]\n",
    "            parquet_file_path = '_'.join(parts)\n",
    "\n",
    "        # Write the table to the parquet file\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "#GU\n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_EP_Admin1_Lob_GU_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GU\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "#for GR\n",
    "    \n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_EP_Admin1_Lob_GR_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GR\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16616"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_GU_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_EP_Admin1_GR_15.parquet\n"
     ]
    }
   ],
   "source": [
    "#Admin 1 (portfolio)modified according to unique id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_parquet_files_Port_2(parquet_files, speriod, samples, rps_values,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate', 'Admin1Name', 'Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['Admin1Id']\n",
    "        admin1_name = row['Admin1Name']\n",
    "\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['Admin1Id'], admin1_id))\n",
    "\n",
    "        # Convert to pandas DataFrame\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "            fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "            fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs','Admin1Name','Admin1Id']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs','Admin1Name','Admin1Id']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "        final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].astype('int64')\n",
    "        final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('Admin1Id', pa.decimal128(38, 0), nullable=True),\n",
    "            pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "        #FOR GU\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Admin1',Cat)\n",
    "        parquet_file_path = os.path.join(export_path,f'{proname}_EP_Admin1_{Cat}_{idx}.parquet')\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "try:\n",
    "    process_parquet_files_Port_2(parquet_files_grp, speriod, samples, rps_values,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "#FOR GR\n",
    "try:\n",
    "    process_parquet_files_Port_2(parquet_files_grp_gr, speriod, samples, rps_values,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_0_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_1_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_2_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_3_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GU_4_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_0_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_1_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_2_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_3_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_Lob_GR_4_15.parquet\n"
     ]
    }
   ],
   "source": [
    "#EP_ cresta Lob updated below\n",
    "\n",
    "\n",
    "# In[129]:\n",
    "\n",
    "\n",
    "def process_parquet_files_EP_Cresta_lob_2(parquet_files, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        \n",
    "        # Skip if the filtered table is empty\n",
    "        \n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['CrestaName','CrestaId']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['CrestaId']\n",
    "        admin1_name = row['CrestaName']\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['CrestaId'], admin1_id))\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "                                \n",
    "                closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "                \n",
    "                # Assign the closest value to the new DataFrames\n",
    "                fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "                fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "                \n",
    "                # Update the closest value to match the rp value exactly\n",
    "                fdataframe_2.at[closest_index_2, 'RPs'] = float(value)\n",
    "                fdataframe_3.at[closest_index_2, 'RPs'] = float(value)\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs', 'CrestaName','CrestaId']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'CrestaName','CrestaId']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs', 'CrestaName','CrestaId']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "        final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "        # Add LobID and LobName columns\n",
    "        final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "        final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "        final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "        final_df_EP_LOB_GU['CrestaId'] = final_df_EP_LOB_GU['CrestaId'].astype('int64')\n",
    "        final_df_EP_LOB_GU['CrestaId'] = final_df_EP_LOB_GU['CrestaId'].apply(lambda x: Decimal(x))\n",
    "        \n",
    "\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('CrestaId',pa.int64(), nullable=True),\n",
    "            pa.field('CrestaName', pa.string(), nullable=True),\n",
    "            pa.field('LOBName', pa.string(), nullable=True),\n",
    "            pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "        underscore_count = parquet_file_path.count('_')\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Cresta_Lob',Cat)\n",
    "        parquet_file_path = os.path.join(export_path, f\"{os.path.splitext(parquet_file_path)[0]}_{idx}.parquet\")\n",
    "\n",
    "        # If there are 21 or more underscores, modify the file path\n",
    "        if underscore_count >= 9:\n",
    "            parts = parquet_file_path.split('_')\n",
    "            # Remove the second last part which contains the number and the underscore before it\n",
    "            parts = parts[:-2] + parts[-1:]\n",
    "            parquet_file_path = '_'.join(parts)\n",
    "\n",
    "        # Write the table to the parquet file\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "#GU\n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_EP_Cresta_Lob_GU_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_EP_Cresta_lob_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GU\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "#for GR\n",
    "    \n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_EP_Cresta_Lob_GR_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_EP_Cresta_lob_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GR\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "flush_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_15.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "#for cresta portfolio\n",
    "\n",
    "def process_parquet_files_Port_EP_Cresta_2(parquet_files, speriod, samples, rps_values,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate', 'CrestaName','CrestaId']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['CrestaName','CrestaId']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['CrestaId']\n",
    "        admin1_name = row['CrestaName']\n",
    "\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['CrestaId'], admin1_id))\n",
    "\n",
    "        # Convert to pandas DataFrame\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "            fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "            fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs','CrestaName','CrestaId']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs','CrestaName','CrestaId']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "        final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "        final_df_EP_Portfolio_GU['CrestaId'] = final_df_EP_Portfolio_GU['CrestaId'].astype('int64')\n",
    "        final_df_EP_Portfolio_GU['CrestaId'] = final_df_EP_Portfolio_GU['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('CrestaId', pa.decimal128(38, 0), nullable=True),\n",
    "            pa.field('CrestaName', pa.string(), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "        #FOR GU\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Cresta',Cat)\n",
    "        parquet_file_path = os.path.join(export_path,f'{proname}_EP_Cresta_{Cat}_{idx}.parquet')\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "try:\n",
    "    process_parquet_files_Port_EP_Cresta_2(parquet_files_grp, speriod, samples, rps_values, \"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_Port_EP_Cresta_2(parquet_files_grp_gr, speriod, samples, rps_values,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# In[136]:\n",
    "\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n",
    "# In[137]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "# In[46]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_STATS_Admin1_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_STATS_Admin1_Lob_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_STATS_Admin1_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_STATS_Admin1_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_STATS_Cresta_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_STATS_Cresta_Lob_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_STATS_Cresta_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_STATS_Cresta_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "#now for stats\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "#now for stats LOB GU admin\n",
    "\n",
    "\n",
    "def process_lob_stats_Admin1_Lob(parquet_files, parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "    aggregated_tables_lob_stats = []\n",
    "\n",
    "    \n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by(['LobName','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL','Admin1Name','Admin1Id'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    \n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Group the final Table again to ensure all groups are combined\n",
    "    final_grouped = final_table.group_by(['LobName','Admin1Name','Admin1Id']).aggregate([('AAL', 'sum')])\n",
    "\n",
    "    # Sort the final grouped Table by 'AAL' in descending order\n",
    "    final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "    pq.write_table(final_grouped, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    flush_cache()\n",
    "\n",
    "    # Convert the final grouped Table to a Pandas DataFrame\n",
    "    final_df = final_grouped.to_pandas()\n",
    "    \n",
    "\n",
    "    # Map LobName to LobId\n",
    "    final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].astype('int64')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "    # Define the columns with NaN values for 'Std' and 'CV'\n",
    "    final_df_STATS_Lob['Std'] = np.nan\n",
    "    final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "    # Reorder the columns to match the specified format\n",
    "    final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName','Admin1Name','Admin1Id']]\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('Admin1Id', pa.decimal128(38)),\n",
    "        pa.field('Admin1Name', pa.string()),\n",
    "        pa.field('LOBName', pa.string()),\n",
    "        pa.field('LOBId', pa.decimal128(38)),\n",
    "\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1_Lob', 'GU', f'{proname}_STATS_Admin1_Lob_GU_0.parquet')\n",
    "process_lob_stats_Admin1_Lob(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1_Lob', 'GR', f'{proname}_STATS_Admin1_Lob_GR_0.parquet')\n",
    "process_lob_stats_Admin1_Lob(parquet_files_grp_gr, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "#stats admin1\n",
    "\n",
    "def process_portfolio_stats_Admin1(parquet_files, export_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by(['Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select([ 'AAL','Admin1Name','Admin1Id'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        \n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Concatenate all the grouped Tables\n",
    "    final_table=final_table.group_by(['Admin1Name','Admin1Id']).aggregate([('AAL', 'sum')])\n",
    "    final_table=final_table.sort_by([('AAL_sum', 'descending')])\n",
    "    final_table=final_table.rename_columns(['Admin1Name','Admin1Id','AAL'])\n",
    "\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "    final_df=final_df.sort_values(by='AAL')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].astype('int64')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "    final_df['Std']=np.nan\n",
    "    final_df['CV']=np.nan\n",
    "    # Get the exact values of Admin1Id and Admin1Name\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    #total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('Admin1Id', pa.decimal128(38)),\n",
    "        pa.field('Admin1Name', pa.string()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1', 'GU', f'{proname}_STATS_Admin1_GU_0.parquet')\n",
    "process_portfolio_stats_Admin1(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Admin1', 'GR', f'{proname}_STATS_Admin1_GR_0.parquet')\n",
    "process_portfolio_stats_Admin1(parquet_files_grp_gr, parquet_file_path_gr)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "\n",
    "# def clean():\n",
    "#     # List of variables to keep\n",
    "#     variables_to_keep = [\n",
    "#         'os', 'pd', 'pa', 'pq', 'np', 'pc', 'gc', 'Decimal', 'ds', 'shutil', 'time', 'sa', 'pyodbc',\n",
    "#         'start_time', 'flush_cache','country','output_folder_path', 'folder_path', 'folder_path_gr', 'speriod', 'samples',\n",
    "#         'parquet_files', 'parquet_files_gr', 'delete_folder_and_files', 'main_folder_path', 'processing_folder_path',\n",
    "#         'resolution_folder_path', 'resolution_folder_path_gr', 'parquet_files_grp', 'parquet_files_grp_gr'\n",
    "#     ]\n",
    "\n",
    "#     # Delete all variables except the ones to keep\n",
    "#     all_vars = list(globals().keys())\n",
    "#     for var in all_vars:\n",
    "#         if var not in variables_to_keep and not var.startswith(\"__\"):\n",
    "#             del globals()[var]\n",
    "\n",
    "#     # Run garbage collection to free up memory\n",
    "#     gc.collect()\n",
    "\n",
    "#     print(\"Memory cleanup completed.\")\n",
    "\n",
    "\n",
    "# clean()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "#now for stats LOB GU Cresta_Lob\n",
    "\n",
    "\n",
    "def process_lob_stats_Cresta_Lob(parquet_files, parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    aggregated_tables_lob_stats = []\n",
    "\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by(['LobName','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL','CrestaName','CrestaId'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "    # Group the final Table again to ensure all groups are combined\n",
    "    final_grouped = final_table.group_by(['LobName','CrestaName','CrestaId']).aggregate([('AAL', 'sum')])\n",
    "\n",
    "    # Sort the final grouped Table by 'AAL' in descending order\n",
    "    final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "    # Convert the final grouped Table to a Pandas DataFrame\n",
    "    final_df = final_grouped.to_pandas()\n",
    "\n",
    "    # Map LobName to LobId\n",
    "    final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].astype('int64')\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "    # Define the columns with NaN values for 'Std' and 'CV'\n",
    "    final_df_STATS_Lob['Std'] = np.nan\n",
    "    final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "    # Reorder the columns to match the specified format\n",
    "    final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName','CrestaName','CrestaId']]\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('CrestaId', pa.decimal128(38)),\n",
    "        pa.field('CrestaName', pa.string()),\n",
    "        pa.field('LobName', pa.string()),\n",
    "        pa.field('LobId', pa.decimal128(38)),\n",
    "       \n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta_Lob', 'GU', f'{proname}_STATS_Cresta_Lob_GU_0.parquet')\n",
    "process_lob_stats_Cresta_Lob(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta_Lob', 'GR', f'{proname}_STATS_Cresta_Lob_GR_0.parquet')\n",
    "process_lob_stats_Cresta_Lob(parquet_files_grp_gr, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "def process_portfolio_stats_Cresta(parquet_files, export_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by(['CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select([ 'AAL','CrestaName','CrestaId'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "    final_table=final_table.group_by(['CrestaName','CrestaId']).aggregate([('AAL', 'sum')])\n",
    "    final_table=final_table.sort_by([('AAL_sum', 'descending')])\n",
    "    final_table=final_table.rename_columns(['CrestaName','CrestaId','AAL'])\n",
    "\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "    final_df=final_df.sort_values(by='AAL')\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].astype('int64')\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].apply(lambda x: Decimal(x))\n",
    "    final_df['Std']=np.nan\n",
    "    final_df['CV']=np.nan\n",
    "    # Get the exact values of Admin1Id and Admin1Name\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    #total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('CrestaId', pa.decimal128(38)),\n",
    "        pa.field('CrestaName', pa.string()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta', 'GU', f'{proname}_STATS_Cresta_GU_0.parquet')\n",
    "process_portfolio_stats_Cresta(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Cresta', 'GR', f'{proname}_STATS_Cresta_GR_0.parquet')\n",
    "process_portfolio_stats_Cresta(parquet_files_grp_gr, parquet_file_path_gr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "# In[81]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "D:/RISHIN/TESTING/TEST_9/ILC2024_NZFL_EP_PLA_NZD_Losses/processing/Resolution Added/PLT_0_100.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_resolution\\EP_final_2.1.3.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y124sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m parquet_file_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mproname\u001b[39m}\u001b[39;00m\u001b[39m_EP_Lob_GU_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.parquet\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y124sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y124sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     process_parquet_files_EP_lob(parquet_files_grp,export_path, lobname, lobid, speriod, samples, rps_values, parquet_file_path)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y124sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mNameError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y124sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError processing \u001b[39m\u001b[39m{\u001b[39;00mlobname\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\RISHIN\\13_ILC_resolution\\EP_final_2.1.3.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y124sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Process each Parquet file individually\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y124sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m parquet_files:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y124sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Read the Parquet file into a PyArrow Table\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y124sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     table \u001b[39m=\u001b[39m pq\u001b[39m.\u001b[39;49mread_table(file)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y124sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Filter the table based on the filter_string\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.3.ipynb#Y124sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     table \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39mfilter(pc\u001b[39m.\u001b[39mequal(table[\u001b[39m'\u001b[39m\u001b[39mLobName\u001b[39m\u001b[39m'\u001b[39m], filter_string))\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1793\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1787\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1788\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPassing \u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_legacy_dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1789\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand will be removed in a future version.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1790\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m   1792\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1793\u001b[0m     dataset \u001b[39m=\u001b[39m ParquetDataset(\n\u001b[0;32m   1794\u001b[0m         source,\n\u001b[0;32m   1795\u001b[0m         schema\u001b[39m=\u001b[39;49mschema,\n\u001b[0;32m   1796\u001b[0m         filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[0;32m   1797\u001b[0m         partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[0;32m   1798\u001b[0m         memory_map\u001b[39m=\u001b[39;49mmemory_map,\n\u001b[0;32m   1799\u001b[0m         read_dictionary\u001b[39m=\u001b[39;49mread_dictionary,\n\u001b[0;32m   1800\u001b[0m         buffer_size\u001b[39m=\u001b[39;49mbuffer_size,\n\u001b[0;32m   1801\u001b[0m         filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[0;32m   1802\u001b[0m         ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes,\n\u001b[0;32m   1803\u001b[0m         pre_buffer\u001b[39m=\u001b[39;49mpre_buffer,\n\u001b[0;32m   1804\u001b[0m         coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit,\n\u001b[0;32m   1805\u001b[0m         decryption_properties\u001b[39m=\u001b[39;49mdecryption_properties,\n\u001b[0;32m   1806\u001b[0m         thrift_string_size_limit\u001b[39m=\u001b[39;49mthrift_string_size_limit,\n\u001b[0;32m   1807\u001b[0m         thrift_container_size_limit\u001b[39m=\u001b[39;49mthrift_container_size_limit,\n\u001b[0;32m   1808\u001b[0m         page_checksum_verification\u001b[39m=\u001b[39;49mpage_checksum_verification,\n\u001b[0;32m   1809\u001b[0m     )\n\u001b[0;32m   1810\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     \u001b[39m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[39m# module is not available\u001b[39;00m\n\u001b[0;32m   1813\u001b[0m     \u001b[39mif\u001b[39;00m filters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1371\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[1;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[39mif\u001b[39;00m partitioning \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhive\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1368\u001b[0m     partitioning \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mHivePartitioning\u001b[39m.\u001b[39mdiscover(\n\u001b[0;32m   1369\u001b[0m         infer_dictionary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> 1371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mdataset(path_or_paths, filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[0;32m   1372\u001b[0m                            schema\u001b[39m=\u001b[39;49mschema, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mparquet_format,\n\u001b[0;32m   1373\u001b[0m                            partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[0;32m   1374\u001b[0m                            ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\dataset.py:794\u001b[0m, in \u001b[0;36mdataset\u001b[1;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[0;32m    783\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m    784\u001b[0m     schema\u001b[39m=\u001b[39mschema,\n\u001b[0;32m    785\u001b[0m     filesystem\u001b[39m=\u001b[39mfilesystem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m     selector_ignore_prefixes\u001b[39m=\u001b[39mignore_prefixes\n\u001b[0;32m    791\u001b[0m )\n\u001b[0;32m    793\u001b[0m \u001b[39mif\u001b[39;00m _is_path_like(source):\n\u001b[1;32m--> 794\u001b[0m     \u001b[39mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    795\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(source, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[0;32m    796\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(_is_path_like(elem) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, FileInfo) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\dataset.py:476\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[1;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[0;32m    474\u001b[0m         fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[0;32m    475\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 476\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_single_source(source, filesystem)\n\u001b[0;32m    478\u001b[0m options \u001b[39m=\u001b[39m FileSystemFactoryOptions(\n\u001b[0;32m    479\u001b[0m     partitioning\u001b[39m=\u001b[39mpartitioning,\n\u001b[0;32m    480\u001b[0m     partition_base_dir\u001b[39m=\u001b[39mpartition_base_dir,\n\u001b[0;32m    481\u001b[0m     exclude_invalid_files\u001b[39m=\u001b[39mexclude_invalid_files,\n\u001b[0;32m    482\u001b[0m     selector_ignore_prefixes\u001b[39m=\u001b[39mselector_ignore_prefixes\n\u001b[0;32m    483\u001b[0m )\n\u001b[0;32m    484\u001b[0m factory \u001b[39m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[39mformat\u001b[39m, options)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\dataset.py:441\u001b[0m, in \u001b[0;36m_ensure_single_source\u001b[1;34m(path, filesystem)\u001b[0m\n\u001b[0;32m    439\u001b[0m     paths_or_selector \u001b[39m=\u001b[39m [path]\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(path)\n\u001b[0;32m    443\u001b[0m \u001b[39mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: D:/RISHIN/TESTING/TEST_9/ILC2024_NZFL_EP_PLA_NZD_Losses/processing/Resolution Added/PLT_0_100.parquet"
     ]
    }
   ],
   "source": [
    "def process_parquet_files_EP_lob(parquet_files, export_path, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path):\n",
    "    processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(processing_folder_path, exist_ok=True)\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        # Skip if the filtered table is empty\n",
    "        if len(table) == 0:\n",
    "            continue\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Add LobID and LobName columns\n",
    "    final_df_EP_LOB_GU['LobID'] = lob_id\n",
    "    final_df_EP_LOB_GU['LobName'] = filter_string\n",
    "    final_df_EP_LOB_GU['LobID'] = final_df_EP_LOB_GU['LobID'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('LOBID', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LOBName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "    parquet_file_path=os.path.join(export_path,parquet_file_path)\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "#GU\n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    export_path =os.path.join(main_folder_path, 'EP', 'Lob','GU')\n",
    "\n",
    "    parquet_file_path = f'{proname}_EP_Lob_GU_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_EP_lob(parquet_files_grp,export_path, lobname, lobid, speriod, samples, rps_values, parquet_file_path)\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "#for GR\n",
    "    \n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    export_path =os.path.join(main_folder_path, 'EP', 'Lob','GR')\n",
    "\n",
    "    parquet_file_path = f'{proname}_EP_Lob_GR_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_EP_lob(parquet_files_grp,export_path, lobname, lobid, speriod, samples, rps_values, parquet_file_path)\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GU_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "def process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values,parquet_file_path):\n",
    "    processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(processing_folder_path, exist_ok=True)\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path =os.path.join(main_folder_path, 'EP', 'Portfolio','GU')\n",
    "parquet_file_path = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GU_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values, parquet_file_path)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path_GR =os.path.join(main_folder_path,'EP','Portfolio','GR')\n",
    "parquet_file_path_GR = os.path.join(export_path_GR, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GR_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files_gr, export_path_GR, speriod, samples, rps_values, parquet_file_path_GR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Lob\\GU\\ILC2024_NZFL_EP_PLA_STATS_Lob_GU_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Lob\\GR\\ILC2024_NZFL_EP_PLA_STATS_Lob_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "#now for stats LOB GU \n",
    "\n",
    "\n",
    "\n",
    "def process_lob_stats(parquet_files, parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    \n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Group the final Table again to ensure all groups are combined\n",
    "    final_grouped = final_table.group_by('LobName').aggregate([('AAL', 'sum')])\n",
    "\n",
    "    # Sort the final grouped Table by 'AAL' in descending order\n",
    "    final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "    # Convert the final grouped Table to a Pandas DataFrame\n",
    "    final_df = final_grouped.to_pandas()\n",
    "\n",
    "    # Map LobName to LobId\n",
    "    final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "\n",
    "    final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "    # Define the columns with NaN values for 'Std' and 'CV'\n",
    "    final_df_STATS_Lob['Std'] = np.nan\n",
    "    final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "    # Reorder the columns to match the specified format\n",
    "    final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName']]\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('LobID', pa.decimal128(38)),\n",
    "        pa.field('LobName', pa.string())\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GU', f'{proname}_STATS_Lob_GU_0.parquet')\n",
    "process_lob_stats(parquet_files, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "# In[93]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GR', f'{proname}_STATS_Lob_GR_0.parquet')\n",
    "process_lob_stats(parquet_files_gr, parquet_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Portfolio\\GU\\ILC2024_NZFL_EP_PLA_STATS_Portfolio_GU_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Portfolio\\GR\\ILC2024_NZFL_EP_PLA_STATS_Portfolio_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\PLT\\Lob\\GU\\ILC2024_NZFL_EP_PLA_PLT_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\PLT\\Lob\\GR\\ILC2024_NZFL_EP_PLA_PLT_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\PLT\\Portfolio\\GU\\ILC2024_NZFL_EP_PLA_PLT_Portfolio_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\PLT\\Portfolio\\GR\\ILC2024_NZFL_EP_PLA_PLT_Portfolio_GR_0.parquet\n",
      "Process finished in 740.99 minutes\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_9\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\n"
     ]
    }
   ],
   "source": [
    "#Portfolio STATS \n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "def process_portfolio_stats(parquet_files, export_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select(['LobName', 'AAL'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Create a DataFrame with the specified columns\n",
    "    final_df_STATS_Portfolio = pd.DataFrame({\n",
    "        'AAL': [total_aal],\n",
    "        'Std': [np.nan],\n",
    "        'CV': [np.nan],\n",
    "    })\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df_STATS_Portfolio, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[106]:\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GU', f'{proname}_STATS_Portfolio_GU_0.parquet')\n",
    "process_portfolio_stats(parquet_files, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "# In[107]:\n",
    "\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GR', f'{proname}_STATS_Portfolio_GR_0.parquet')\n",
    "process_portfolio_stats(parquet_files_gr, parquet_file_path_gr)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#NOW FOR PLT LOB\n",
    "\n",
    "\n",
    "# In[96]:\n",
    "\n",
    "\n",
    "def process_PLT_lob(parquet_files, export_path):\n",
    "    # Directory to store intermediate results\n",
    "    intermediate_dir = os.path.join(main_folder_path, 'intermediate_results')\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file in chunks and write intermediate results to disk\n",
    "    for i, file in enumerate(parquet_files):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            \n",
    "            # Cast columns to the desired types\n",
    "            table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('LobId'), 'LobId', pa.compute.cast(table['LobId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('LobName'), 'LobName', pa.compute.cast(table['LobName'], pa.string()))\n",
    "            \n",
    "            grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "            pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "    # Read intermediate results and combine them\n",
    "    intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "    intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "    combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "    # Perform the final group by and aggregation\n",
    "    final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "    final_grouped_table = final_grouped_table.sort_by([('Loss_sum_sum', 'descending')])\n",
    "\n",
    "    # Rename the aggregated column\n",
    "    final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "    # Reorder the columns in the desired order\n",
    "    final_grouped_table = final_grouped_table.select(ordered_columns)\n",
    "\n",
    "    # Save the final table to a Parquet file\n",
    "        # Delete intermediate files\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "        # Remove the intermediate directory\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {intermediate_dir}\")\n",
    "    except OSError:\n",
    "        print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "    try:\n",
    "        pq.write_table(final_grouped_table, export_path)\n",
    "        print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Parquet file: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    pa.field('LobID', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('LobName', pa.string(), nullable=True)\n",
    "])\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for GU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GU', f'{proname}_PLT_Lob_GU_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files, export_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GR', f'{proname}_PLT_Lob_GR_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files_gr, export_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#PLT Portfolio\n",
    "\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight']\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight']\n",
    "\n",
    "def process_PLT_portfolio_2(parquet_files, export_path):\n",
    "    # Flush memory at the beginning\n",
    "    gc.collect()\n",
    "\n",
    "    # Directory to store intermediate results\n",
    "    intermediate_dir = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', 'intermediate_results')\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file in chunks and write intermediate results to disk\n",
    "    for i, file in enumerate(parquet_files):\n",
    "        parquet_file = pq.ParquetFile(file)\n",
    "        for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            # Cast columns to the desired types\n",
    "            table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "            \n",
    "\n",
    "\n",
    "            grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "            pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "    # Read intermediate results and combine them\n",
    "    intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "    intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "    combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "    # Perform the final group by and aggregation\n",
    "    final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "\n",
    "    # Rename the aggregated column\n",
    "    final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "    \n",
    "    # Convert the table to the specified schema\n",
    "    final_grouped_table = pa.Table.from_arrays(\n",
    "        [final_grouped_table.column(name).cast(schema.field(name).type) for name in schema.names],\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    final_grouped_table = final_grouped_table.sort_by([('Loss', 'descending')])\n",
    "\n",
    "    # Save the final table to a Parquet file\n",
    "    # Delete intermediate files\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "        # Remove the intermediate directory\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {intermediate_dir}\")\n",
    "    except OSError:\n",
    "        print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "    try:\n",
    "        pq.write_table(final_grouped_table, export_path)\n",
    "        print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Parquet file: {e}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', f'{proname}_PLT_Portfolio_GU_0.parquet')\n",
    "\n",
    "process_PLT_portfolio_2(parquet_files, export_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GR', f'{proname}_PLT_Portfolio_GR_0.parquet')\n",
    "\n",
    "process_PLT_portfolio_2(parquet_files_gr, export_path)\n",
    "\n",
    "\n",
    "#updates made after here \n",
    "\n",
    "# # Define the folder to be zipped\n",
    "# folder_to_zip = main_folder_path  # Change this to your folder path\n",
    "\n",
    "# # Get the parent directory and zip file name\n",
    "# parent_dir, folder_name = os.path.split(folder_to_zip)\n",
    "# output_zip = os.path.join(parent_dir, folder_name)  # Same name as folder\n",
    "\n",
    "# # Create a zip archive\n",
    "# shutil.make_archive(output_zip, 'zip', folder_to_zip)\n",
    "\n",
    "# # Remove the original folder after zipping\n",
    "# shutil.rmtree(folder_to_zip)\n",
    "\n",
    "# print(f\"Replaced '{folder_to_zip}' with '{output_zip}.zip'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()  # End time\n",
    "elapsed_time = (end_time - start_time) / 60  # Convert seconds to minutes\n",
    "\n",
    "print(f\"Process finished in {elapsed_time:.2f} minutes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "delete_folder_and_files(resolution_folder_path)\n",
    "delete_folder_and_files(resolution_folder_path_gr)\n",
    "delete_folder_and_files(processing_folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
