{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_0_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_3_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_2_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_4_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_5_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_7_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_0_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_3_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_2_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_4_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_5_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_7_100.parquet\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "from decimal import Decimal  # Add this import statement\n",
    "import pyarrow.dataset as ds\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import sqlalchemy as sa\n",
    "import pyodbc\n",
    "import concurrent.futures\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()  # Start time\n",
    "\n",
    "\n",
    "# Function to flush the cache\n",
    "def flush_cache():\n",
    "    gc.collect()\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n",
    "folder_path = r'D:\\RISHIN\\14_2_1ILC_NZFL\\PLT\\Risk_Lob\\GU\\PeriodRange=1-250000'\n",
    "folder_path_gr = r'D:\\RISHIN\\14_2_1ILC_NZFL\\PLT\\Risk_Lob\\GR\\PeriodRange=1-250000'\n",
    "\n",
    "output_folder_path = r\"D:\\RISHIN\\TESTING\\TEST_15\"\n",
    "# folder_path = r'D:\\RISHIN\\13_ILC_resolution\\input\\PARQUET_FILES'\n",
    "# folder_path_gr = r'D:\\RISHIN\\13_ILC_TASK\\input\\PARQUET_FILES_GR'\n",
    "\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# speriod=int(input(\"Enter the simulation period: \"))\n",
    "# samples=int(input(\"Enter the number of samples: \"))\n",
    "# proname=(input(\"enter file suffix example : example ILC2024_NZFL_EP_PLA suffix should have only 3 \"_\" ))\n",
    "# region=input(\"enter region example : example NZD  \")\n",
    "# database = input('Enter the database name IED2024_NZFL_PC_NZD_EDM240_ILCRun')\n",
    "\n",
    "speriod=50000\n",
    "samples=5\n",
    "pla1=\"PLA\"\n",
    "proname=f\"ILC2024_NZFL_EP_{pla1}\"\n",
    "currency=\"EUR\"\n",
    "region=currency\n",
    "region=\"NZD\"\n",
    "database = \"IED2024_NZFL_PC_NZD_EDM240_ILCRun\"\n",
    "\n",
    "# proname=\"ILC2024_EUWS_PLA_WI_EP_BE\"\n",
    "# currency=\"EUR\"\n",
    "# region=currency\n",
    "# database = \"IED2024_EUWS_PC_MIX_EDM230_ILC_LOB_UPDATE_20240905\"\n",
    "\n",
    "\n",
    "hiphen_count=proname.count('_') + 7 #delete this if aint working\n",
    "\n",
    "\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "\n",
    "parquet_files_gr = [os.path.join(folder_path_gr, f) for f in os.listdir(folder_path_gr) if f.endswith('.parquet')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def delete_folder_and_files(folder_path):\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        # Delete all files inside the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        \n",
    "        # Delete the folder itself\n",
    "        os.rmdir(folder_path)\n",
    "        print(f'Successfully deleted the folder: {folder_path}')\n",
    "    else:\n",
    "        print(f'The folder {folder_path} does not exist.')\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'{proname}_{region}_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio','Admin1','Admin1_Lob','Cresta','Cresta_Lob',]\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            # Filter nested folders for 'PLT'\n",
    "            if subfolder == 'PLT':\n",
    "                filtered_nested_folders = ['Lob', 'Portfolio']\n",
    "            else:\n",
    "                filtered_nested_folders = nested_folders\n",
    "            \n",
    "            for nested_folder in filtered_nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "main_folder_path = os.path.join(output_folder_path, f'{proname}_{region}_Losses')\n",
    "\n",
    "processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "resolution_folder_path = os.path.join(processing_folder_path, 'Resolution Added')\n",
    "resolution_folder_path_gr = os.path.join(processing_folder_path, 'Resolution Added_gr')\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'Partial')   \n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'Concatenated')\n",
    "\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "def connect_to_database(server, database):\n",
    "    connection_string = f'mssql+pyodbc://{server}/{database}?driver=SQL+Server+Native+Client+11.0'\n",
    "    engine = sa.create_engine(connection_string)\n",
    "    connection = engine.connect()\n",
    "    return connection\n",
    "\n",
    "def read_parquet_file(file_path):\n",
    "    table = pq.read_table(file_path)\n",
    "    return table\n",
    "\n",
    "def fetch_database_data(connection):\n",
    "    address_query = 'SELECT ADDRESSID, ADMIN1GEOID AS Admin1Id, Admin1Name, zone1GEOID AS CrestaId, Zone1 AS CrestaName FROM Address'\n",
    "    address_df = pd.read_sql(address_query, connection)\n",
    "    address_table = pa.Table.from_pandas(address_df)\n",
    "    return address_table\n",
    "\n",
    "\n",
    "\n",
    "def join_dataframes(parquet_table, address_table):\n",
    "    parquet_df = parquet_table.to_pandas()\n",
    "    address_df = address_table.to_pandas()\n",
    "    df = parquet_df.merge(address_df, left_on='LocationId', right_on='ADDRESSID', how='left')\n",
    "    return pa.Table.from_pandas(df)\n",
    "\n",
    "def save_joined_dataframe(joined_table, output_file):\n",
    "    pq.write_table(joined_table, output_file)\n",
    "    print(f\"Saved joined file to {output_file}\")\n",
    "\n",
    "def process_file(file, address_table, output_folder):\n",
    "    gc.collect()\n",
    "\n",
    "    parquet_table = read_parquet_file(file)\n",
    "    joined_table = join_dataframes(parquet_table, address_table)\n",
    "    output_file = os.path.join(output_folder, os.path.basename(file))\n",
    "    save_joined_dataframe(joined_table, output_file)\n",
    "    del parquet_table\n",
    "    del joined_table\n",
    "    gc.collect()\n",
    "\n",
    "def process_parquet_files(folder_path, output_folder, server, database, batch_size=2):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    connection = connect_to_database(server, database)\n",
    "    address_table = fetch_database_data(connection)\n",
    "\n",
    "    parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "    connection.close()\n",
    "\n",
    "    # Process files in batches\n",
    "    for i in range(0, len(parquet_files), batch_size):\n",
    "        batch_files = parquet_files[i:i + batch_size]\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_file, file, address_table, output_folder) for file in batch_files]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                future.result()\n",
    "\n",
    "server = 'localhost'\n",
    "\n",
    "process_parquet_files(folder_path, resolution_folder_path, server, database)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "process_parquet_files(folder_path_gr, resolution_folder_path_gr, server, database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "parquet_files_grp = [os.path.join(resolution_folder_path, f) for f in os.listdir(resolution_folder_path) if f.endswith('.parquet')]\n",
    "parquet_files_grp_gr = [os.path.join(resolution_folder_path_gr, f) for f in os.listdir(resolution_folder_path_gr) if f.endswith('.parquet')]\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "def fetch_lobdet_data(server, database):\n",
    "    connection = connect_to_database(server, database)\n",
    "    try:\n",
    "        lobdet_query = 'SELECT LOBNAME, LOBDETID FROM lobdet'\n",
    "        lobdet_df = pd.read_sql(lobdet_query, connection)\n",
    "        lobname_to_lobid_2 = dict(zip(lobdet_df.LOBNAME, lobdet_df.LOBDETID))\n",
    "    finally:\n",
    "        connection.close()\n",
    "    return lobname_to_lobid_2\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "lobname_to_lobid=fetch_lobdet_data(server, database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_0_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_1_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_2_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_3_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GU_4_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_0_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_1_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_2_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_3_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_Lob_GR_4_15.parquet\n"
     ]
    }
   ],
   "source": [
    "#EP_admin_lob\n",
    "def process_parquet_files_2(parquet_files, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        \n",
    "        # Skip if the filtered table is empty\n",
    "        \n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['Admin1Id']\n",
    "        admin1_name = row['Admin1Name']\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['Admin1Id'], admin1_id))\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        # Compute cumulative rate and RPs\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "\n",
    "        # Given RP values list\n",
    "        rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "        # Find the minimum RP in the dataframe\n",
    "        min_rp = dataframe_2['RPs'].min()\n",
    "\n",
    "        # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "        if min_rp > 2:\n",
    "            # Find the RP values that need to be added\n",
    "            missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "            new_rows = []\n",
    "            for rp in missing_rps:\n",
    "                temp_df = dataframe_2.iloc[[-1]].copy()  # Copy the last row as a template\n",
    "                temp_df['RPs'] = rp\n",
    "                new_rows.append(temp_df)\n",
    "\n",
    "            # Concatenate the original dataframe with new rows\n",
    "            new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "            dataframe_2 = pd.concat([dataframe_2, new_rows_df], ignore_index=True)\n",
    "\n",
    "            # Fill NaN values only in the new rows\n",
    "            new_rows_indices = new_rows_df.index\n",
    "            dataframe_2.loc[new_rows_indices] = dataframe_2.loc[new_rows_indices].fillna(0)\n",
    "        # Continue with further calculations\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "\n",
    "\n",
    "        # Given RP values list\n",
    "        rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "        # Find the minimum RP in the dataframe\n",
    "        min_rp = dataframe_3['RPs'].min()\n",
    "\n",
    "        # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "        if min_rp > 2:\n",
    "            # Find the RP values that need to be added\n",
    "            missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "            new_rows = []\n",
    "            for rp in missing_rps:\n",
    "                temp_df = dataframe_3.iloc[[-1]].copy()  # Copy the last row\n",
    "                temp_df['RPs'] = rp\n",
    "                new_rows.append(temp_df)\n",
    "\n",
    "            # Concatenate the original dataframe with new rows\n",
    "            new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "            dataframe_3 = pd.concat([dataframe_3, new_rows_df], ignore_index=True)\n",
    "\n",
    "            # Fill NaN values only in the new rows\n",
    "            new_rows_indices = new_rows_df.index\n",
    "            dataframe_3.loc[new_rows_indices] = dataframe_3.loc[new_rows_indices].fillna(0)\n",
    "            \n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "                                \n",
    "                closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "                \n",
    "                # Assign the closest value to the new DataFrames\n",
    "                fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "                fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "                \n",
    "                # Update the closest value to match the rp value exactly\n",
    "                #fdataframe_2.at[closest_index_2, 'RPs'] = float(value)\n",
    "                #fdataframe_3.at[closest_index_2, 'RPs'] = float(value)\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'Admin1Name', 'Admin1Id']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "        # Add LobID and LobName columns\n",
    "        final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "        final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "        final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "        final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].astype('int64')\n",
    "        final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "        \n",
    "\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('Admin1Id',pa.int64(), nullable=True),\n",
    "            pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "            pa.field('LOBName', pa.string(), nullable=True),\n",
    "            pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "        underscore_count = parquet_file_path.count('_')\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Admin1_Lob',Cat)\n",
    "        parquet_file_path = os.path.join(export_path, f\"{os.path.splitext(parquet_file_path)[0]}_{idx}.parquet\")\n",
    "\n",
    "        # If there are 9 or more underscores, modify the file path\n",
    "        if underscore_count >= hiphen_count: # anything wrong replace hiphen count with 9\n",
    "            parts = parquet_file_path.split('_')\n",
    "            # Remove the second last part which contains the number and the underscore before it\n",
    "            parts = parts[:-2] + parts[-1:]\n",
    "            parquet_file_path = '_'.join(parts)\n",
    "\n",
    "        # Write the table to the parquet file\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "#GU\n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_{region}_EP_Admin1_Lob_GU_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GU\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "#for GR\n",
    "    \n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_{region}_EP_Admin1_Lob_GR_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_2(parquet_files_grp_gr, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GR\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GU_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Admin1_GR_15.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "def process_parquet_files_Port_2(parquet_files, speriod, samples, rps_values,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate', 'Admin1Name', 'Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['Admin1Id']\n",
    "        admin1_name = row['Admin1Name']\n",
    "\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['Admin1Id'], admin1_id))\n",
    "\n",
    "        # Convert to pandas DataFrame\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "\n",
    "        # Given RP values list\n",
    "        rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "        # Find the minimum RP in the dataframe\n",
    "        min_rp = dataframe_2['RPs'].min()\n",
    "\n",
    "        # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "        if min_rp > 2:\n",
    "            # Find the RP values that need to be added\n",
    "            missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "            new_rows = []\n",
    "            for rp in missing_rps:\n",
    "                temp_df = dataframe_2.iloc[[-1]].copy()  # Copy the last row as a template\n",
    "                temp_df['RPs'] = rp\n",
    "                new_rows.append(temp_df)\n",
    "\n",
    "            # Concatenate the original dataframe with new rows\n",
    "            new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "            dataframe_2 = pd.concat([dataframe_2, new_rows_df], ignore_index=True)\n",
    "\n",
    "            # Fill NaN values only in the new rows\n",
    "            new_rows_indices = new_rows_df.index\n",
    "            dataframe_2.loc[new_rows_indices] = dataframe_2.loc[new_rows_indices].fillna(0)\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "\n",
    "\n",
    "        # Given RP values list\n",
    "        rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "        # Find the minimum RP in the dataframe\n",
    "        min_rp = dataframe_3['RPs'].min()\n",
    "\n",
    "        # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "        if min_rp > 2:\n",
    "            # Find the RP values that need to be added\n",
    "            missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "            new_rows = []\n",
    "            for rp in missing_rps:\n",
    "                temp_df = dataframe_3.iloc[[-1]].copy()  # Copy the last row\n",
    "                temp_df['RPs'] = rp\n",
    "                new_rows.append(temp_df)\n",
    "\n",
    "            # Concatenate the original dataframe with new rows\n",
    "            new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "            dataframe_3 = pd.concat([dataframe_3, new_rows_df], ignore_index=True)\n",
    "\n",
    "            # Fill NaN values only in the new rows\n",
    "            new_rows_indices = new_rows_df.index\n",
    "            dataframe_3.loc[new_rows_indices] = dataframe_3.loc[new_rows_indices].fillna(0)\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().sort_values().index[0]\n",
    "            fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "            fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "                \n",
    "            # Update the closest value to match the rp value exactly\n",
    "            fdataframe_2.at[closest_index_2, 'RPs'] = float(value)\n",
    "            fdataframe_3.at[closest_index_2, 'RPs'] = float(value)\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs','Admin1Name','Admin1Id']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs','Admin1Name','Admin1Id']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "        final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].astype('int64')\n",
    "        final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('Admin1Id', pa.decimal128(38, 0), nullable=True),\n",
    "            pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "        #FOR GU\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Admin1',Cat)\n",
    "        parquet_file_path = os.path.join(export_path,f'{proname}_{region}_EP_Admin1_{Cat}_{idx}.parquet')\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "try:\n",
    "    process_parquet_files_Port_2(parquet_files_grp, speriod, samples, rps_values,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "#FOR GR\n",
    "try:\n",
    "    process_parquet_files_Port_2(parquet_files_grp_gr, speriod, samples, rps_values,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_0_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_1_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_2_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_3_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GU_4_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_0_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_1_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_2_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_3_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Cresta_Lob_GR_4_15.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "#EP_ cresta Lob updated below\n",
    "\n",
    "\n",
    "# In[129]:\n",
    "\n",
    "\n",
    "def process_parquet_files_EP_Cresta_lob_2(parquet_files, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        \n",
    "        # Skip if the filtered table is empty\n",
    "        \n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['CrestaName','CrestaId']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['CrestaId']\n",
    "        admin1_name = row['CrestaName']\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['CrestaId'], admin1_id))\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "\n",
    "        # Given RP values list\n",
    "        rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "        # Find the minimum RP in the dataframe\n",
    "        min_rp = dataframe_2['RPs'].min()\n",
    "\n",
    "        # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "        if min_rp > 2:\n",
    "            # Find the RP values that need to be added\n",
    "            missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "            new_rows = []\n",
    "            for rp in missing_rps:\n",
    "                temp_df = dataframe_2.iloc[[-1]].copy()  # Copy the last row as a template\n",
    "                temp_df['RPs'] = rp\n",
    "                new_rows.append(temp_df)\n",
    "\n",
    "            # Concatenate the original dataframe with new rows\n",
    "            new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "            dataframe_2 = pd.concat([dataframe_2, new_rows_df], ignore_index=True)\n",
    "\n",
    "            # Fill NaN values only in the new rows\n",
    "            new_rows_indices = new_rows_df.index\n",
    "            dataframe_2.loc[new_rows_indices] = dataframe_2.loc[new_rows_indices].fillna(0)        \n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "\n",
    "\n",
    "        # Given RP values list\n",
    "        rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "        # Find the minimum RP in the dataframe\n",
    "        min_rp = dataframe_3['RPs'].min()\n",
    "\n",
    "        # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "        if min_rp > 2:\n",
    "            # Find the RP values that need to be added\n",
    "            missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "            new_rows = []\n",
    "            for rp in missing_rps:\n",
    "                temp_df = dataframe_3.iloc[[-1]].copy()  # Copy the last row\n",
    "                temp_df['RPs'] = rp\n",
    "                new_rows.append(temp_df)\n",
    "\n",
    "            # Concatenate the original dataframe with new rows\n",
    "            new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "            dataframe_3 = pd.concat([dataframe_3, new_rows_df], ignore_index=True)\n",
    "\n",
    "            # Fill NaN values only in the new rows\n",
    "            new_rows_indices = new_rows_df.index\n",
    "            dataframe_3.loc[new_rows_indices] = dataframe_3.loc[new_rows_indices].fillna(0)\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "                                \n",
    "                closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "                \n",
    "                # Assign the closest value to the new DataFrames\n",
    "                fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "                fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "                \n",
    "                # Update the closest value to match the rp value exactly\n",
    "                fdataframe_2.at[closest_index_2, 'RPs'] = float(value)\n",
    "                fdataframe_3.at[closest_index_2, 'RPs'] = float(value)\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs', 'CrestaName','CrestaId']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'CrestaName','CrestaId']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs', 'CrestaName','CrestaId']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "        final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "        # Add LobID and LobName columns\n",
    "        final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "        final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "        final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "        final_df_EP_LOB_GU['CrestaId'] = final_df_EP_LOB_GU['CrestaId'].astype('int64')\n",
    "        final_df_EP_LOB_GU['CrestaId'] = final_df_EP_LOB_GU['CrestaId'].apply(lambda x: Decimal(x))\n",
    "        \n",
    "\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('CrestaId',pa.int64(), nullable=True),\n",
    "            pa.field('CrestaName', pa.string(), nullable=True),\n",
    "            pa.field('LOBName', pa.string(), nullable=True),\n",
    "            pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "        underscore_count = parquet_file_path.count('_')\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Cresta_Lob',Cat)\n",
    "        parquet_file_path = os.path.join(export_path, f\"{os.path.splitext(parquet_file_path)[0]}_{idx}.parquet\")\n",
    "\n",
    "        # If there are 21 or more underscores, modify the file path\n",
    "        if underscore_count >= hiphen_count: # anything wrong replace hiphen count with 9\n",
    "            parts = parquet_file_path.split('_')\n",
    "            # Remove the second last part which contains the number and the underscore before it\n",
    "            parts = parts[:-2] + parts[-1:]\n",
    "            parquet_file_path = '_'.join(parts)\n",
    "\n",
    "        # Write the table to the parquet file\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "#GU\n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_{region}_EP_Cresta_Lob_GU_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_EP_Cresta_lob_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GU\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "#for GR\n",
    "    \n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_{region}_EP_Cresta_Lob_GR_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_EP_Cresta_lob_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GR\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "flush_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_EP_Cresta_GU_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_EP_Cresta_GR_15.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#for cresta portfolio\n",
    "\n",
    "def process_parquet_files_Port_EP_Cresta_2(parquet_files, speriod, samples, rps_values,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate', 'CrestaName','CrestaId']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['CrestaName','CrestaId']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['CrestaId']\n",
    "        admin1_name = row['CrestaName']\n",
    "\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['CrestaId'], admin1_id))\n",
    "\n",
    "        # Convert to pandas DataFrame\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "\n",
    "        # Given RP values list\n",
    "        rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "        # Find the minimum RP in the dataframe\n",
    "        min_rp = dataframe_2['RPs'].min()\n",
    "\n",
    "        # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "        if min_rp > 2:\n",
    "            # Find the RP values that need to be added\n",
    "            missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "            new_rows = []\n",
    "            for rp in missing_rps:\n",
    "                temp_df = dataframe_2.iloc[[-1]].copy()  # Copy the last row as a template\n",
    "                temp_df['RPs'] = rp\n",
    "                new_rows.append(temp_df)\n",
    "\n",
    "            # Concatenate the original dataframe with new rows\n",
    "            new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "            dataframe_2 = pd.concat([dataframe_2, new_rows_df], ignore_index=True)\n",
    "\n",
    "            # Fill NaN values only in the new rows\n",
    "            new_rows_indices = new_rows_df.index\n",
    "            dataframe_2.loc[new_rows_indices] = dataframe_2.loc[new_rows_indices].fillna(0)\n",
    "\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "\n",
    "\n",
    "        # Given RP values list\n",
    "        rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "        # Find the minimum RP in the dataframe\n",
    "        min_rp = dataframe_3['RPs'].min()\n",
    "\n",
    "        # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "        if min_rp > 2:\n",
    "            # Find the RP values that need to be added\n",
    "            missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "            new_rows = []\n",
    "            for rp in missing_rps:\n",
    "                temp_df = dataframe_3.iloc[[-1]].copy()  # Copy the last row\n",
    "                temp_df['RPs'] = rp\n",
    "                new_rows.append(temp_df)\n",
    "\n",
    "            # Concatenate the original dataframe with new rows\n",
    "            new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "            dataframe_3 = pd.concat([dataframe_3, new_rows_df], ignore_index=True)\n",
    "\n",
    "            # Fill NaN values only in the new rows\n",
    "            new_rows_indices = new_rows_df.index\n",
    "            dataframe_3.loc[new_rows_indices] = dataframe_3.loc[new_rows_indices].fillna(0)\n",
    "\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "            fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "            fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "            # Update the closest value to match the rp value exactly\n",
    "            fdataframe_2.at[closest_index_2, 'RPs'] = float(value)\n",
    "            fdataframe_3.at[closest_index_2, 'RPs'] = float(value)\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs','CrestaName','CrestaId']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs','CrestaName','CrestaId']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "        final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "        final_df_EP_Portfolio_GU['CrestaId'] = final_df_EP_Portfolio_GU['CrestaId'].astype('int64')\n",
    "        final_df_EP_Portfolio_GU['CrestaId'] = final_df_EP_Portfolio_GU['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('CrestaId', pa.decimal128(38, 0), nullable=True),\n",
    "            pa.field('CrestaName', pa.string(), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "        #FOR GU\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Cresta',Cat)\n",
    "        parquet_file_path = os.path.join(export_path,f'{proname}_EP_Cresta_{Cat}_{idx}.parquet')\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "try:\n",
    "    process_parquet_files_Port_EP_Cresta_2(parquet_files_grp, speriod, samples, rps_values, \"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_Port_EP_Cresta_2(parquet_files_grp_gr, speriod, samples, rps_values,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# In[136]:\n",
    "\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n",
    "# In[137]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_STATS_Admin1_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Admin1_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_STATS_Admin1_Lob_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Admin1\\GU\\ILC2024_NZFL_EP_PLA_STATS_Admin1_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Admin1\\GR\\ILC2024_NZFL_EP_PLA_STATS_Admin1_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "#now for stats\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "#now for stats LOB GU admin\n",
    "\n",
    "\n",
    "def process_lob_stats_Admin1_Lob(parquet_files, parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "    aggregated_tables_lob_stats = []\n",
    "\n",
    "    \n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by(['LobName','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL','Admin1Name','Admin1Id'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    \n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Group the final Table again to ensure all groups are combined\n",
    "    final_grouped = final_table.group_by(['LobName','Admin1Name','Admin1Id']).aggregate([('AAL', 'sum')])\n",
    "\n",
    "    # Sort the final grouped Table by 'AAL' in descending order\n",
    "    final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "    pq.write_table(final_grouped, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    flush_cache()\n",
    "\n",
    "    # Convert the final grouped Table to a Pandas DataFrame\n",
    "    final_df = final_grouped.to_pandas()\n",
    "    \n",
    "\n",
    "    # Map LobName to LobId\n",
    "    final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].astype('int64')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "    # Define the columns with NaN values for 'Std' and 'CV'\n",
    "    final_df_STATS_Lob['Std'] = np.nan\n",
    "    final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "    # Reorder the columns to match the specified format\n",
    "    final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName','Admin1Name','Admin1Id']]\n",
    "    final_df_STATS_Lob[\"LOBId\"] = final_df_STATS_Lob[\"LobId\"]\n",
    "    final_df_STATS_Lob[\"LOBName\"]=final_df_STATS_Lob[\"LobName\"]\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('Admin1Id', pa.decimal128(38)),\n",
    "        pa.field('Admin1Name', pa.string()),\n",
    "        pa.field('LOBName', pa.string()),\n",
    "        pa.field('LOBId', pa.decimal128(38)),\n",
    "\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1_Lob', 'GU', f'{proname}_{region}_STATS_Admin1_Lob_GU_0.parquet')\n",
    "process_lob_stats_Admin1_Lob(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1_Lob', 'GR', f'{proname}_{region}_STATS_Admin1_Lob_GR_0.parquet')\n",
    "process_lob_stats_Admin1_Lob(parquet_files_grp_gr, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "#stats admin1\n",
    "\n",
    "def process_portfolio_stats_Admin1(parquet_files, export_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by(['Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select([ 'AAL','Admin1Name','Admin1Id'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        \n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Concatenate all the grouped Tables\n",
    "    final_table=final_table.group_by(['Admin1Name','Admin1Id']).aggregate([('AAL', 'sum')])\n",
    "    final_table=final_table.sort_by([('AAL_sum', 'descending')])\n",
    "    final_table=final_table.rename_columns(['Admin1Name','Admin1Id','AAL'])\n",
    "\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "    final_df=final_df.sort_values(by='AAL')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].astype('int64')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "    final_df['Std']=np.nan\n",
    "    final_df['CV']=np.nan\n",
    "    # Get the exact values of Admin1Id and Admin1Name\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    #total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('Admin1Id', pa.decimal128(38)),\n",
    "        pa.field('Admin1Name', pa.string()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1', 'GU', f'{proname}_STATS_Admin1_GU_0.parquet')\n",
    "process_portfolio_stats_Admin1(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "flush_cache()\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Admin1', 'GR', f'{proname}_STATS_Admin1_GR_0.parquet')\n",
    "process_portfolio_stats_Admin1(parquet_files_grp_gr, parquet_file_path_gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Cresta_Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_STATS_Cresta_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Cresta_Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_STATS_Cresta_Lob_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Cresta\\GU\\ILC2024_NZFL_EP_PLA_NZD_STATS_Cresta_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Cresta\\GR\\ILC2024_NZFL_EP_PLA_NZD_STATS_Cresta_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "flush_cache()\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "#now for stats LOB GU Cresta_Lob\n",
    "\n",
    "\n",
    "def process_lob_stats_Cresta_Lob(parquet_files, parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    aggregated_tables_lob_stats = []\n",
    "\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by(['LobName','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL','CrestaName','CrestaId'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "    # Group the final Table again to ensure all groups are combined\n",
    "    final_grouped = final_table.group_by(['LobName','CrestaName','CrestaId']).aggregate([('AAL', 'sum')])\n",
    "\n",
    "    # Sort the final grouped Table by 'AAL' in descending order\n",
    "    final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "    # Convert the final grouped Table to a Pandas DataFrame\n",
    "    final_df = final_grouped.to_pandas()\n",
    "\n",
    "    # Map LobName to LobId\n",
    "    final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].astype('int64')\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "    # Define the columns with NaN values for 'Std' and 'CV'\n",
    "    final_df_STATS_Lob['Std'] = np.nan\n",
    "    final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "    # Reorder the columns to match the specified format\n",
    "    final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName','CrestaName','CrestaId']]\n",
    "    final_df_STATS_Lob[\"LOBId\"] = final_df_STATS_Lob[\"LobId\"]\n",
    "    final_df_STATS_Lob[\"LOBName\"]=final_df_STATS_Lob[\"LobName\"]\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('CrestaId', pa.decimal128(38)),\n",
    "        pa.field('CrestaName', pa.string()),\n",
    "        pa.field('LOBName', pa.string()),\n",
    "        pa.field('LOBId', pa.decimal128(38)),\n",
    "       \n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta_Lob', 'GU', f'{proname}_{region}_STATS_Cresta_Lob_GU_0.parquet')\n",
    "process_lob_stats_Cresta_Lob(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta_Lob', 'GR', f'{proname}_{region}_STATS_Cresta_Lob_GR_0.parquet')\n",
    "process_lob_stats_Cresta_Lob(parquet_files_grp_gr, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "def process_portfolio_stats_Cresta(parquet_files, export_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by(['CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select([ 'AAL','CrestaName','CrestaId'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "    final_table=final_table.group_by(['CrestaName','CrestaId']).aggregate([('AAL', 'sum')])\n",
    "    final_table=final_table.sort_by([('AAL_sum', 'descending')])\n",
    "    final_table=final_table.rename_columns(['CrestaName','CrestaId','AAL'])\n",
    "\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "    final_df=final_df.sort_values(by='AAL')\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].astype('int64')\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].apply(lambda x: Decimal(x))\n",
    "    final_df['Std']=np.nan\n",
    "    final_df['CV']=np.nan\n",
    "    # Get the exact values of Admin1Id and Admin1Name\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    #total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('CrestaId', pa.decimal128(38)),\n",
    "        pa.field('CrestaName', pa.string()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta', 'GU', f'{proname}_{region}_STATS_Cresta_GU_0.parquet')\n",
    "process_portfolio_stats_Cresta(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "#GR.\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Cresta', 'GR', f'{proname}_{region}_STATS_Cresta_GR_0.parquet')\n",
    "process_portfolio_stats_Cresta(parquet_files_grp_gr, parquet_file_path_gr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Lob_GU_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Lob_GU_1.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Lob_GU_2.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Lob_GU_3.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Lob_GU_4.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Lob_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Lob_GR_1.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Lob_GR_2.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Lob_GR_3.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Lob_GR_4.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Portfolio\\GU\\ILC2024_NZFL_EP_PLA_NZD_EP_Portfolio_GU_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Portfolio\\GR\\ILC2024_NZFL_EP_PLA_NZD_EP_Portfolio_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "flush_cache()\n",
    "def process_parquet_files_EP_lob(parquet_files, export_path, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path):\n",
    "    processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(processing_folder_path, exist_ok=True)\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        # Skip if the filtered table is empty\n",
    "        if len(table) == 0:\n",
    "            continue\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "\n",
    "    # Given RP values list\n",
    "    rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "    # Find the minimum RP in the dataframe\n",
    "    min_rp = dataframe_2['RPs'].min()\n",
    "\n",
    "    # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "    if min_rp > 2:\n",
    "        # Find the RP values that need to be added\n",
    "        missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "        new_rows = []\n",
    "        for rp in missing_rps:\n",
    "            temp_df = dataframe_2.iloc[[-1]].copy()  # Copy the last row as a template\n",
    "            temp_df['RPs'] = rp\n",
    "            new_rows.append(temp_df)\n",
    "\n",
    "        # Concatenate the original dataframe with new rows\n",
    "        new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "        dataframe_2 = pd.concat([dataframe_2, new_rows_df], ignore_index=True)\n",
    "\n",
    "        # Fill NaN values only in the new rows\n",
    "        new_rows_indices = new_rows_df.index\n",
    "        dataframe_2.loc[new_rows_indices] = dataframe_2.loc[new_rows_indices].fillna(0)\n",
    "\n",
    "\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "\n",
    "\n",
    "    # Given RP values list\n",
    "    rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "    # Find the minimum RP in the dataframe\n",
    "    min_rp = dataframe_3['RPs'].min()\n",
    "\n",
    "    # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "    if min_rp > 2:\n",
    "        # Find the RP values that need to be added\n",
    "        missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "        new_rows = []\n",
    "        for rp in missing_rps:\n",
    "            temp_df = dataframe_3.iloc[[-1]].copy()  # Copy the last row\n",
    "            temp_df['RPs'] = rp\n",
    "            new_rows.append(temp_df)\n",
    "\n",
    "        # Concatenate the original dataframe with new rows\n",
    "        new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "        dataframe_3 = pd.concat([dataframe_3, new_rows_df], ignore_index=True)\n",
    "\n",
    "        # Fill NaN values only in the new rows\n",
    "        new_rows_indices = new_rows_df.index\n",
    "        dataframe_3.loc[new_rows_indices] = dataframe_3.loc[new_rows_indices].fillna(0)\n",
    "\n",
    "\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Add LobID and LobName columns\n",
    "    final_df_EP_LOB_GU['LobId'] = lob_id\n",
    "    final_df_EP_LOB_GU['LobName'] = filter_string\n",
    "    final_df_EP_LOB_GU['LobId'] = final_df_EP_LOB_GU['LobId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LobName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "    parquet_file_path = os.path.join(export_path, parquet_file_path)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "#GU\n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    export_path =os.path.join(main_folder_path, 'EP', 'Lob','GU')\n",
    "\n",
    "    parquet_file_path = f'{proname}_{region}_EP_Lob_GU_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_EP_lob(parquet_files,export_path, lobname, lobid, speriod, samples, rps_values, parquet_file_path)\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "#for GR\n",
    "    \n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    export_path =os.path.join(main_folder_path, 'EP', 'Lob','GR')\n",
    "\n",
    "    parquet_file_path = f'{proname}_{region}_EP_Lob_GR_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_EP_lob(parquet_files_gr,export_path, lobname, lobid, speriod, samples, rps_values, parquet_file_path)\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "def process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values,parquet_file_path):\n",
    "    processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(processing_folder_path, exist_ok=True)\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "\n",
    "    # Given RP values list\n",
    "    rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "    # Find the minimum RP in the dataframe\n",
    "    min_rp = dataframe_2['RPs'].min()\n",
    "\n",
    "    # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "    if min_rp > 2:\n",
    "        # Find the RP values that need to be added\n",
    "        missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "        new_rows = []\n",
    "        for rp in missing_rps:\n",
    "            temp_df = dataframe_2.iloc[[-1]].copy()  # Copy the last row as a template\n",
    "            temp_df['RPs'] = rp\n",
    "            new_rows.append(temp_df)\n",
    "\n",
    "        # Concatenate the original dataframe with new rows\n",
    "        new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "        dataframe_2 = pd.concat([dataframe_2, new_rows_df], ignore_index=True)\n",
    "\n",
    "        # Fill NaN values only in the new rows\n",
    "        new_rows_indices = new_rows_df.index\n",
    "        dataframe_2.loc[new_rows_indices] = dataframe_2.loc[new_rows_indices].fillna(0)\n",
    "\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "\n",
    "\n",
    "    # Given RP values list\n",
    "    rp_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "    # Find the minimum RP in the dataframe\n",
    "    min_rp = dataframe_3['RPs'].min()\n",
    "\n",
    "    # If the lowest RP in dataframe_2 is greater than 2, add new rows with updated RP values\n",
    "    if min_rp > 2:\n",
    "        # Find the RP values that need to be added\n",
    "        missing_rps = [rp for rp in rp_values if rp < min_rp]\n",
    "\n",
    "        new_rows = []\n",
    "        for rp in missing_rps:\n",
    "            temp_df = dataframe_3.iloc[[-1]].copy()  # Copy the last row\n",
    "            temp_df['RPs'] = rp\n",
    "            new_rows.append(temp_df)\n",
    "\n",
    "        # Concatenate the original dataframe with new rows\n",
    "        new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "        dataframe_3 = pd.concat([dataframe_3, new_rows_df], ignore_index=True)\n",
    "\n",
    "        # Fill NaN values only in the new rows\n",
    "        new_rows_indices = new_rows_df.index\n",
    "        dataframe_3.loc[new_rows_indices] = dataframe_3.loc[new_rows_indices].fillna(0)\n",
    "    \n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path =os.path.join(main_folder_path, 'EP', 'Portfolio','GU')\n",
    "parquet_file_path = os.path.join(export_path, f'{proname}_{region}_EP_Portfolio_GU_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values, parquet_file_path)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path_GR =os.path.join(main_folder_path,'EP','Portfolio','GR')\n",
    "parquet_file_path_GR = os.path.join(export_path_GR, f'{proname}_{region}_EP_Portfolio_GR_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files_gr, export_path_GR, speriod, samples, rps_values, parquet_file_path_GR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_STATS_Lob_GU_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_STATS_Lob_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Portfolio\\GU\\ILC2024_NZFL_EP_PLA_NZD_STATS_Portfolio_GU_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\STATS\\Portfolio\\GR\\ILC2024_NZFL_EP_PLA_NZD_STATS_Portfolio_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\PLT\\Lob\\GU\\ILC2024_NZFL_EP_PLA_NZD_PLT_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\PLT\\Lob\\GR\\ILC2024_NZFL_EP_PLA_NZD_PLT_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\PLT\\Portfolio\\GU\\ILC2024_NZFL_EP_PLA_NZD_PLT_Portfolio_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\PLT\\Portfolio\\GR\\ILC2024_NZFL_EP_PLA_NZD_PLT_Portfolio_GR_0.parquet\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\concatenated does not exist.\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_15\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\n",
      "Process finished in 1676.64 minutes\n"
     ]
    }
   ],
   "source": [
    "#now for stats LOB GU \n",
    "\n",
    "\n",
    "\n",
    "def process_lob_stats(parquet_files, parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    \n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Group the final Table again to ensure all groups are combined\n",
    "    final_grouped = final_table.group_by('LobName').aggregate([('AAL', 'sum')])\n",
    "\n",
    "    # Sort the final grouped Table by 'AAL' in descending order\n",
    "    final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "    # Convert the final grouped Table to a Pandas DataFrame\n",
    "    final_df = final_grouped.to_pandas()\n",
    "\n",
    "    # Map LobName to LobId\n",
    "    final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "\n",
    "    final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "    # Define the columns with NaN values for 'Std' and 'CV'\n",
    "    final_df_STATS_Lob['Std'] = np.nan\n",
    "    final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "    # Reorder the columns to match the specified format\n",
    "    final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName']]\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('LobId', pa.decimal128(38)),\n",
    "        pa.field('LobName', pa.string())\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GU', f'{proname}_{region}_STATS_Lob_GU_0.parquet')\n",
    "process_lob_stats(parquet_files, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "# In[93]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GR', f'{proname}_{region}_STATS_Lob_GR_0.parquet')\n",
    "process_lob_stats(parquet_files_gr, parquet_file_path)\n",
    "\n",
    "\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "#Portfolio STATS \n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "def process_portfolio_stats(parquet_files, export_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select(['LobName', 'AAL'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Create a DataFrame with the specified columns\n",
    "    final_df_STATS_Portfolio = pd.DataFrame({\n",
    "        'AAL': [total_aal],\n",
    "        'Std': [np.nan],\n",
    "        'CV': [np.nan],\n",
    "    })\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df_STATS_Portfolio, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[106]:\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GU', f'{proname}_{region}_STATS_Portfolio_GU_0.parquet')\n",
    "process_portfolio_stats(parquet_files, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "# In[107]:\n",
    "\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GR', f'{proname}_{region}_STATS_Portfolio_GR_0.parquet')\n",
    "process_portfolio_stats(parquet_files_gr, parquet_file_path_gr)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#NOW FOR PLT LOB\n",
    "\n",
    "\n",
    "# In[96]:\n",
    "\n",
    "\n",
    "def process_PLT_lob(parquet_files, export_path):\n",
    "    # Directory to store intermediate results\n",
    "    intermediate_dir = os.path.join(main_folder_path, 'intermediate_results')\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file in chunks and write intermediate results to disk\n",
    "    for i, file in enumerate(parquet_files):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            \n",
    "            # Cast columns to the desired types\n",
    "            table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('LobId'), 'LobId', pa.compute.cast(table['LobId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('LobName'), 'LobName', pa.compute.cast(table['LobName'], pa.string()))\n",
    "            \n",
    "            grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "            pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "    # Read intermediate results and combine them\n",
    "    intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "    intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "    combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "    # Perform the final group by and aggregation\n",
    "    final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "    final_grouped_table = final_grouped_table.sort_by([('Loss_sum_sum', 'descending')])\n",
    "\n",
    "    # Rename the aggregated column\n",
    "    final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "    # Reorder the columns in the desired order\n",
    "    final_grouped_table = final_grouped_table.select(ordered_columns)\n",
    "\n",
    "    # Save the final table to a Parquet file\n",
    "        # Delete intermediate files\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "        # Remove the intermediate directory\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {intermediate_dir}\")\n",
    "    except OSError:\n",
    "        print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "    try:\n",
    "        pq.write_table(final_grouped_table, export_path)\n",
    "        print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Parquet file: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('LobName', pa.string(), nullable=True)\n",
    "])\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for GU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GU', f'{proname}_{region}_PLT_Lob_GU_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files, export_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GR', f'{proname}_{region}_PLT_Lob_GR_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files_gr, export_path)\n",
    "\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#PLT Portfolio\n",
    "\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight']\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight']\n",
    "\n",
    "def process_PLT_portfolio_2(parquet_files, export_path):\n",
    "    # Flush memory at the beginning\n",
    "    gc.collect()\n",
    "\n",
    "    # Directory to store intermediate results\n",
    "    intermediate_dir = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', 'intermediate_results')\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file in chunks and write intermediate results to disk\n",
    "    for i, file in enumerate(parquet_files):\n",
    "        parquet_file = pq.ParquetFile(file)\n",
    "        for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            # Cast columns to the desired types\n",
    "            table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "            \n",
    "\n",
    "\n",
    "            grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "            pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "    # Read intermediate results and combine them\n",
    "    intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "    intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "    combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "    # Perform the final group by and aggregation\n",
    "    final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "\n",
    "    # Rename the aggregated column\n",
    "    final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "    \n",
    "    # Convert the table to the specified schema\n",
    "    final_grouped_table = pa.Table.from_arrays(\n",
    "        [final_grouped_table.column(name).cast(schema.field(name).type) for name in schema.names],\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    final_grouped_table = final_grouped_table.sort_by([('Loss', 'descending')])\n",
    "\n",
    "    # Save the final table to a Parquet file\n",
    "    # Delete intermediate files\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "        # Remove the intermediate directory\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {intermediate_dir}\")\n",
    "    except OSError:\n",
    "        print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "    try:\n",
    "        pq.write_table(final_grouped_table, export_path)\n",
    "        print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Parquet file: {e}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', f'{proname}_{region}_PLT_Portfolio_GU_0.parquet')\n",
    "\n",
    "process_PLT_portfolio_2(parquet_files, export_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GR', f'{proname}_{region}_PLT_Portfolio_GR_0.parquet')\n",
    "\n",
    "process_PLT_portfolio_2(parquet_files_gr, export_path)\n",
    "\n",
    "\n",
    "#updates made after here \n",
    "\n",
    "# # Define the folder to be zipped\n",
    "# folder_to_zip = main_folder_path  # Change this to your folder path\n",
    "\n",
    "# # Get the parent directory and zip file name\n",
    "# parent_dir, folder_name = os.path.split(folder_to_zip)\n",
    "# output_zip = os.path.join(parent_dir, folder_name)  # Same name as folder\n",
    "\n",
    "# # Create a zip archive\n",
    "# shutil.make_archive(output_zip, 'zip', folder_to_zip)\n",
    "\n",
    "# # Remove the original folder after zipping\n",
    "# shutil.rmtree(folder_to_zip)\n",
    "\n",
    "# print(f\"Replaced '{folder_to_zip}' with '{output_zip}.zip'\")\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "\n",
    "# def delete_files_with_multiple_extensions(folder_path):\n",
    "#     \"\"\"Delete files with multiple '.parquet' extensions.\"\"\"\n",
    "#     for root, dirs, files in os.walk(folder_path):\n",
    "#         for file in files:\n",
    "#             if file.count('.parquet') > 1:\n",
    "#                 os.remove(os.path.join(root, file))\n",
    "\n",
    "\n",
    "# def concatenate_parquet_files(main_folder_path):\n",
    "#     subfolders = [\n",
    "#         'EP/Admin1_Lob/GU',\n",
    "#         'EP/Admin1_Lob/GR',\n",
    "#         'EP/Cresta_Lob/GU',\n",
    "#         'EP/Cresta_Lob/GR'\n",
    "#     ]\n",
    "\n",
    "#     for subfolder in subfolders:\n",
    "#         folder_path = os.path.join(main_folder_path, subfolder)\n",
    "#         files = [f for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "        \n",
    "#         file_groups = {}\n",
    "#         for file in files:\n",
    "#             parts = file.split('_')\n",
    "#             if len(parts) >= 9:\n",
    "#                 key = parts[8]\n",
    "#                 if key not in file_groups:\n",
    "#                     file_groups[key] = []\n",
    "#                 file_groups[key].append(file)\n",
    "        \n",
    "#         for key, group_files in file_groups.items():\n",
    "#             tables = [pq.read_table(os.path.join(folder_path, f)) for f in group_files]\n",
    "#             concatenated_table = pa.concat_tables(tables)\n",
    "#             new_file_name = '_'.join(group_files[0].split('_')[:8]) + f'_{key}.parquet'\n",
    "#             pq.write_table(concatenated_table, os.path.join(folder_path, new_file_name))\n",
    "            \n",
    "#             for file in group_files:\n",
    "#                 os.remove(os.path.join(folder_path, file))\n",
    "#     delete_files_with_multiple_extensions(main_folder_path)\n",
    "\n",
    "\n",
    "# concatenate_parquet_files(main_folder_path)\n",
    "\n",
    "\n",
    "def delete_files_with_multiple_extensions(folder_path):\n",
    "    \"\"\"Delete files with multiple '.parquet' extensions.\"\"\"\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.count('.parquet') > 1:\n",
    "                os.remove(os.path.join(root, file))\n",
    "\n",
    "\n",
    "def concatenate_parquet_files(main_folder_path):\n",
    "    subfolders = [\n",
    "        'EP/Admin1_Lob/GU',\n",
    "        'EP/Admin1_Lob/GR',\n",
    "        'EP/Cresta_Lob/GU',\n",
    "        'EP/Cresta_Lob/GR'\n",
    "    ]\n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        folder_path = os.path.join(main_folder_path, subfolder)\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "        \n",
    "        file_groups = {}\n",
    "        for file in files:\n",
    "            parts = file.split('_')\n",
    "            if len(parts) >= hiphen_count:\n",
    "                key = parts[hiphen_count-1]\n",
    "                if key not in file_groups:\n",
    "                    file_groups[key] = []\n",
    "                file_groups[key].append(file)\n",
    "        \n",
    "        for key, group_files in file_groups.items():\n",
    "            tables = [pq.read_table(os.path.join(folder_path, f)) for f in group_files]\n",
    "            concatenated_table = pa.concat_tables(tables)\n",
    "            new_file_name = '_'.join(group_files[0].split('_')[:hiphen_count-1]) + f'_{key}.parquet'\n",
    "            pq.write_table(concatenated_table, os.path.join(folder_path, new_file_name))\n",
    "            \n",
    "            for file in group_files:\n",
    "                os.remove(os.path.join(folder_path, file))\n",
    "    delete_files_with_multiple_extensions(main_folder_path)\n",
    "\n",
    "\n",
    "concatenate_parquet_files(main_folder_path)\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "delete_folder_and_files(resolution_folder_path)\n",
    "delete_folder_and_files(resolution_folder_path_gr)\n",
    "delete_folder_and_files(processing_folder_path)\n",
    "\n",
    "\n",
    "end_time = time.time()  # End time\n",
    "elapsed_time = (end_time - start_time) / 60  # Convert seconds to minutes\n",
    "\n",
    "print(f\"Process finished in {elapsed_time:.2f} minutes\")\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
