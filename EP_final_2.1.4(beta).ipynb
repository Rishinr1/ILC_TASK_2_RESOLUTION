{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_0_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_2_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_5_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_3_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_4_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added\\PLT_7_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_2_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_0_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_4_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_5_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_3_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\processing\\Resolution Added_gr\\PLT_7_100.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "from decimal import Decimal  # Add this import statement\n",
    "import pyarrow.dataset as ds\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import sqlalchemy as sa\n",
    "import pyodbc\n",
    "import concurrent.futures\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()  # Start time\n",
    "\n",
    "\n",
    "# Function to flush the cache\n",
    "def flush_cache():\n",
    "    gc.collect()\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n",
    "folder_path = r'D:\\RISHIN\\14_2_1ILC_NZFL\\PLT\\Risk_Lob\\GU\\PeriodRange=1-250000'\n",
    "folder_path_gr = r'D:\\RISHIN\\14_2_1ILC_NZFL\\PLT\\Risk_Lob\\GR\\PeriodRange=1-250000'\n",
    "\n",
    "output_folder_path = r\"D:\\RISHIN\\TESTING\\TEST_10\"\n",
    "# folder_path = r'D:\\RISHIN\\13_ILC_resolution\\input\\PARQUET_FILES'\n",
    "# folder_path_gr = r'D:\\RISHIN\\13_ILC_TASK\\input\\PARQUET_FILES_GR'\n",
    "\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# speriod=int(input(\"Enter the simulation period: \"))\n",
    "# samples=int(input(\"Enter the number of samples: \"))\n",
    "# proname=(input(\"enter file suffix example : example ILC2024_NZFL_EP_PLA \"))\n",
    "# region=input(\"enter region example : example NZD  \")\n",
    "# database = input('Enter the database name IED2024_NZFL_PC_NZD_EDM240_ILCRun')\n",
    "\n",
    "speriod=50000\n",
    "samples=5\n",
    "proname=\"ILC2024_NZFL_EP_PLA\"\n",
    "region=\"NZD\"\n",
    "database = \"IED2024_NZFL_PC_NZD_EDM240_ILCRun\"\n",
    "\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "parquet_files_gr = [os.path.join(folder_path_gr, f) for f in os.listdir(folder_path_gr) if f.endswith('.parquet')]\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def delete_folder_and_files(folder_path):\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        # Delete all files inside the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        \n",
    "        # Delete the folder itself\n",
    "        os.rmdir(folder_path)\n",
    "        print(f'Successfully deleted the folder: {folder_path}')\n",
    "    else:\n",
    "        print(f'The folder {folder_path} does not exist.')\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'{proname}_{region}_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio','Admin1','Admin1_Lob','Cresta','Cresta_Lob',]\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            # Filter nested folders for 'PLT'\n",
    "            if subfolder == 'PLT':\n",
    "                filtered_nested_folders = ['Lob', 'Portfolio']\n",
    "            else:\n",
    "                filtered_nested_folders = nested_folders\n",
    "            \n",
    "            for nested_folder in filtered_nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "main_folder_path = os.path.join(output_folder_path, f'{proname}_{region}_Losses')\n",
    "\n",
    "processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "resolution_folder_path = os.path.join(processing_folder_path, 'Resolution Added')\n",
    "resolution_folder_path_gr = os.path.join(processing_folder_path, 'Resolution Added_gr')\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'Partial')   \n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'Concatenated')\n",
    "\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "def connect_to_database(server, database):\n",
    "    connection_string = f'mssql+pyodbc://{server}/{database}?driver=SQL+Server+Native+Client+11.0'\n",
    "    engine = sa.create_engine(connection_string)\n",
    "    connection = engine.connect()\n",
    "    return connection\n",
    "\n",
    "def read_parquet_file(file_path):\n",
    "    table = pq.read_table(file_path)\n",
    "    return table\n",
    "\n",
    "def fetch_database_data(connection):\n",
    "    address_query = 'SELECT ADDRESSID, ADMIN1GEOID AS Admin1Id, Admin1Name, zone1GEOID AS CrestaId, Zone1 AS CrestaName FROM Address'\n",
    "    address_df = pd.read_sql(address_query, connection)\n",
    "    address_table = pa.Table.from_pandas(address_df)\n",
    "    return address_table\n",
    "\n",
    "\n",
    "\n",
    "def join_dataframes(parquet_table, address_table):\n",
    "    parquet_df = parquet_table.to_pandas()\n",
    "    address_df = address_table.to_pandas()\n",
    "    df = parquet_df.merge(address_df, left_on='LocationId', right_on='ADDRESSID', how='left')\n",
    "    return pa.Table.from_pandas(df)\n",
    "\n",
    "def save_joined_dataframe(joined_table, output_file):\n",
    "    pq.write_table(joined_table, output_file)\n",
    "    print(f\"Saved joined file to {output_file}\")\n",
    "\n",
    "def process_file(file, address_table, output_folder):\n",
    "    gc.collect()\n",
    "\n",
    "    parquet_table = read_parquet_file(file)\n",
    "    joined_table = join_dataframes(parquet_table, address_table)\n",
    "    output_file = os.path.join(output_folder, os.path.basename(file))\n",
    "    save_joined_dataframe(joined_table, output_file)\n",
    "    del parquet_table\n",
    "    del joined_table\n",
    "    gc.collect()\n",
    "\n",
    "def process_parquet_files(folder_path, output_folder, server, database, batch_size=3):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    connection = connect_to_database(server, database)\n",
    "    address_table = fetch_database_data(connection)\n",
    "\n",
    "    parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "    connection.close()\n",
    "\n",
    "    # Process files in batches\n",
    "    for i in range(0, len(parquet_files), batch_size):\n",
    "        batch_files = parquet_files[i:i + batch_size]\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_file, file, address_table, output_folder) for file in batch_files]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                future.result()\n",
    "\n",
    "server = 'localhost'\n",
    "\n",
    "process_parquet_files(folder_path, resolution_folder_path, server, database)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "process_parquet_files(folder_path_gr, resolution_folder_path_gr, server, database)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#Admin_1 Lob\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "parquet_files_grp = [os.path.join(resolution_folder_path, f) for f in os.listdir(resolution_folder_path) if f.endswith('.parquet')]\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "parquet_files_grp_gr = [os.path.join(resolution_folder_path_gr, f) for f in os.listdir(resolution_folder_path_gr) if f.endswith('.parquet')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_lobdet_data(server, database):\n",
    "    connection = connect_to_database(server, database)\n",
    "    try:\n",
    "        lobdet_query = 'SELECT LOBNAME, LOBDETID FROM lobdet'\n",
    "        lobdet_df = pd.read_sql(lobdet_query, connection)\n",
    "        lobname_to_lobid_2 = dict(zip(lobdet_df.LOBNAME, lobdet_df.LOBDETID))\n",
    "    finally:\n",
    "        connection.close()\n",
    "    return lobname_to_lobid_2\n",
    "lobname_to_lobid=fetch_lobdet_data(server, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\RISHIN\\\\TESTING\\\\TEST_10\\\\ILC2024_NZFL_EP_PLA_NZD_Losses'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_14.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_0_15.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_11.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_12.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_13.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_10\\ILC2024_NZFL_EP_PLA_NZD_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_NZFL_EP_PLA_EP_Admin1_Lob_GU_1_14.parquet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_resolution\\EP_final_2.1.4(beta).ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W6sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m parquet_file_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mproname\u001b[39m}\u001b[39;00m\u001b[39m_EP_Admin1_Lob_GU_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.parquet\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W6sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W6sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m     process_parquet_files_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \u001b[39m\"\u001b[39;49m\u001b[39mGU\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W6sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mNameError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W6sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError processing \u001b[39m\u001b[39m{\u001b[39;00mlobname\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\RISHIN\\13_ILC_resolution\\EP_final_2.1.4(beta).ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W6sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m dataframe_2[\u001b[39m'\u001b[39m\u001b[39mTCE_OEP_2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (dataframe_2[\u001b[39m'\u001b[39m\u001b[39mTCE_OEP_1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshift()\u001b[39m.\u001b[39mcumsum() \u001b[39m*\u001b[39m dataframe_2[\u001b[39m'\u001b[39m\u001b[39mRPs\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W6sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m dataframe_2[\u001b[39m'\u001b[39m\u001b[39mTCE_OEP_Final\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (dataframe_2[\u001b[39m'\u001b[39m\u001b[39mTCE_OEP_2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m dataframe_2[\u001b[39m'\u001b[39m\u001b[39mMax_Loss\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W6sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m dataframe_3 \u001b[39m=\u001b[39m dataframe_1\u001b[39m.\u001b[39;49mgroupby([\u001b[39m'\u001b[39;49m\u001b[39mPeriodId\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mAdmin1Name\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mAdmin1Id\u001b[39;49m\u001b[39m'\u001b[39;49m], as_index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39;49magg({\u001b[39m'\u001b[39;49m\u001b[39mSum_Loss_sum\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W6sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m dataframe_3\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mSum_Loss_sum\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mS_Sum_Loss\u001b[39m\u001b[39m'\u001b[39m}, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W6sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m dataframe_3 \u001b[39m=\u001b[39m dataframe_3\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mS_Sum_Loss\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1429\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mengine_kwargs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m engine_kwargs\n\u001b[0;32m   1431\u001b[0m op \u001b[39m=\u001b[39m GroupByApply(\u001b[39mself\u001b[39m, func, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m-> 1432\u001b[0m result \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39;49magg()\n\u001b[0;32m   1433\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dict_like(func) \u001b[39mand\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     \u001b[39m# GH #52849\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mas_index \u001b[39mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m is_dict_like(func):\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magg_dict_like()\n\u001b[0;32m    191\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(func):\n\u001b[0;32m    192\u001b[0m     \u001b[39m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magg_list_like()\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39magg_dict_like\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m    416\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[39m    Result of aggregation.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magg_or_apply_dict_like(op_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39magg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[1;34m(self, op_name)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mengine\u001b[39m\u001b[39m\"\u001b[39m: engine, \u001b[39m\"\u001b[39m\u001b[39mengine_kwargs\u001b[39m\u001b[39m\"\u001b[39m: engine_kwargs})\n\u001b[0;32m   1605\u001b[0m \u001b[39mwith\u001b[39;00m com\u001b[39m.\u001b[39mtemp_setattr(\n\u001b[0;32m   1606\u001b[0m     obj, \u001b[39m\"\u001b[39m\u001b[39mas_index\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m, condition\u001b[39m=\u001b[39m\u001b[39mhasattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39mas_index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1607\u001b[0m ):\n\u001b[1;32m-> 1608\u001b[0m     result_index, result_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_dict_like(\n\u001b[0;32m   1609\u001b[0m         op_name, selected_obj, selection, kwargs\n\u001b[0;32m   1610\u001b[0m     )\n\u001b[0;32m   1611\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[0;32m   1612\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\apply.py:497\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[1;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m         results \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m key_data\n\u001b[0;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    495\u001b[0m     \u001b[39m# key used for column selection and output\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 497\u001b[0m         \u001b[39mgetattr\u001b[39;49m(obj\u001b[39m.\u001b[39;49m_gotitem(key, ndim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), op_name)(how, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    498\u001b[0m         \u001b[39mfor\u001b[39;00m key, how \u001b[39min\u001b[39;00m func\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(func\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m    502\u001b[0m \u001b[39mreturn\u001b[39;00m keys, results\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:249\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[39mif\u001b[39;00m engine_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mengine_kwargs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m engine_kwargs\n\u001b[1;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, func)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    251\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, abc\u001b[39m.\u001b[39mIterable):\n\u001b[0;32m    252\u001b[0m     \u001b[39m# Catch instances of lists / tuples\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[39m# but not the class list / tuple itself.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     func \u001b[39m=\u001b[39m maybe_mangle_lambdas(func)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:3146\u001b[0m, in \u001b[0;36mGroupBy.sum\u001b[1;34m(self, numeric_only, min_count, engine, engine_kwargs)\u001b[0m\n\u001b[0;32m   3141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3142\u001b[0m     \u001b[39m# If we are grouping on categoricals we want unobserved categories to\u001b[39;00m\n\u001b[0;32m   3143\u001b[0m     \u001b[39m# return zero, rather than the default of NaN which the reindexing in\u001b[39;00m\n\u001b[0;32m   3144\u001b[0m     \u001b[39m# _agg_general() returns. GH #31422\u001b[39;00m\n\u001b[0;32m   3145\u001b[0m     \u001b[39mwith\u001b[39;00m com\u001b[39m.\u001b[39mtemp_setattr(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mobserved\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m-> 3146\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_agg_general(\n\u001b[0;32m   3147\u001b[0m             numeric_only\u001b[39m=\u001b[39;49mnumeric_only,\n\u001b[0;32m   3148\u001b[0m             min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m   3149\u001b[0m             alias\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   3150\u001b[0m             npfunc\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49msum,\n\u001b[0;32m   3151\u001b[0m         )\n\u001b[0;32m   3153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_output(result, fill_value\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1906\u001b[0m, in \u001b[0;36mGroupBy._agg_general\u001b[1;34m(self, numeric_only, min_count, alias, npfunc, **kwargs)\u001b[0m\n\u001b[0;32m   1896\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   1897\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_agg_general\u001b[39m(\n\u001b[0;32m   1898\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1904\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1905\u001b[0m ):\n\u001b[1;32m-> 1906\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cython_agg_general(\n\u001b[0;32m   1907\u001b[0m         how\u001b[39m=\u001b[39;49malias,\n\u001b[0;32m   1908\u001b[0m         alt\u001b[39m=\u001b[39;49mnpfunc,\n\u001b[0;32m   1909\u001b[0m         numeric_only\u001b[39m=\u001b[39;49mnumeric_only,\n\u001b[0;32m   1910\u001b[0m         min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m   1911\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1912\u001b[0m     )\n\u001b[0;32m   1913\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1998\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m   1995\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mndim, alt\u001b[39m=\u001b[39malt)\n\u001b[0;32m   1996\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m-> 1998\u001b[0m new_mgr \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mgrouped_reduce(array_func)\n\u001b[0;32m   1999\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[0;32m   2000\u001b[0m \u001b[39mif\u001b[39;00m how \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39midxmin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39midxmax\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\internals\\base.py:367\u001b[0m, in \u001b[0;36mSingleDataManager.grouped_reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mgrouped_reduce\u001b[39m(\u001b[39mself\u001b[39m, func):\n\u001b[0;32m    366\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray\n\u001b[1;32m--> 367\u001b[0m     res \u001b[39m=\u001b[39m func(arr)\n\u001b[0;32m    368\u001b[0m     index \u001b[39m=\u001b[39m default_index(\u001b[39mlen\u001b[39m(res))\n\u001b[0;32m    370\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfrom_array(res, index)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1973\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1971\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39marray_func\u001b[39m(values: ArrayLike) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[0;32m   1972\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1973\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_grouper\u001b[39m.\u001b[39;49m_cython_operation(\n\u001b[0;32m   1974\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39maggregate\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1975\u001b[0m             values,\n\u001b[0;32m   1976\u001b[0m             how,\n\u001b[0;32m   1977\u001b[0m             axis\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mndim \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1978\u001b[0m             min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m   1979\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1980\u001b[0m         )\n\u001b[0;32m   1981\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m   1982\u001b[0m         \u001b[39m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[0;32m   1983\u001b[0m         \u001b[39m# and non-applicable functions\u001b[39;00m\n\u001b[0;32m   1984\u001b[0m         \u001b[39m# try to python agg\u001b[39;00m\n\u001b[0;32m   1985\u001b[0m         \u001b[39m# TODO: shouldn't min_count matter?\u001b[39;00m\n\u001b[0;32m   1986\u001b[0m         \u001b[39m# TODO: avoid special casing SparseArray here\u001b[39;00m\n\u001b[0;32m   1987\u001b[0m         \u001b[39mif\u001b[39;00m how \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39many\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(values, SparseArray):\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:827\u001b[0m, in \u001b[0;36mBaseGrouper._cython_operation\u001b[1;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[39mReturns the values of a cython operation.\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    825\u001b[0m \u001b[39massert\u001b[39;00m kind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mtransform\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 827\u001b[0m cy_op \u001b[39m=\u001b[39m WrappedCythonOp(kind\u001b[39m=\u001b[39mkind, how\u001b[39m=\u001b[39mhow, has_dropped_na\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhas_dropped_na)\n\u001b[0;32m    829\u001b[0m ids, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_info\n\u001b[0;32m    830\u001b[0m ngroups \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngroups\n",
      "File \u001b[1;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:741\u001b[0m, in \u001b[0;36mBaseGrouper.has_dropped_na\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[39m@cache_readonly\u001b[39m\n\u001b[0;32m    737\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mhas_dropped_na\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m    738\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[39m    Whether grouper has null value(s) that are dropped.\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 741\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup_info[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39many())\n",
      "File \u001b[1;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:745\u001b[0m, in \u001b[0;36mBaseGrouper.group_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[39m@cache_readonly\u001b[39m\n\u001b[0;32m    744\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mgroup_info\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp], npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp], \u001b[39mint\u001b[39m]:\n\u001b[1;32m--> 745\u001b[0m     comp_ids, obs_group_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_compressed_codes()\n\u001b[0;32m    747\u001b[0m     ngroups \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(obs_group_ids)\n\u001b[0;32m    748\u001b[0m     comp_ids \u001b[39m=\u001b[39m ensure_platform_int(comp_ids)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:764\u001b[0m, in \u001b[0;36mBaseGrouper._get_compressed_codes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    759\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_get_compressed_codes\u001b[39m(\n\u001b[0;32m    760\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    761\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39msignedinteger], npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp]]:\n\u001b[0;32m    762\u001b[0m     \u001b[39m# The first returned ndarray may have any signed integer dtype\u001b[39;00m\n\u001b[0;32m    763\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupings) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 764\u001b[0m         group_index \u001b[39m=\u001b[39m get_group_index(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcodes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, sort\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, xnull\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    765\u001b[0m         \u001b[39mreturn\u001b[39;00m compress_group_index(group_index, sort\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort)\n\u001b[0;32m    766\u001b[0m         \u001b[39m# FIXME: compress_group_index's second return value is int64, not intp\u001b[39;00m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pandas\\core\\sorting.py:188\u001b[0m, in \u001b[0;36mget_group_index\u001b[1;34m(labels, shape, sort, xnull)\u001b[0m\n\u001b[0;32m    185\u001b[0m stride \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mprod(lshape[\u001b[39m1\u001b[39m:nlev], dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    186\u001b[0m out \u001b[39m=\u001b[39m stride \u001b[39m*\u001b[39m labels[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m, subok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 188\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m, nlev):\n\u001b[0;32m    189\u001b[0m     \u001b[39mif\u001b[39;00m lshape[i] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    190\u001b[0m         stride \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mint64(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_parquet_files_2(parquet_files, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        \n",
    "        # Skip if the filtered table is empty\n",
    "        \n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['Admin1Id']\n",
    "        admin1_name = row['Admin1Name']\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['Admin1Id'], admin1_id))\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "                                \n",
    "                closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "                \n",
    "                # Assign the closest value to the new DataFrames\n",
    "                fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "                fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "                \n",
    "                # Update the closest value to match the rp value exactly\n",
    "                fdataframe_2.at[closest_index_2, 'RPs'] = float(value)\n",
    "                fdataframe_3.at[closest_index_2, 'RPs'] = float(value)\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'Admin1Name', 'Admin1Id']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "        # Add LobID and LobName columns\n",
    "        final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "        final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "        final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "        final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].astype('int64')\n",
    "        final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "        \n",
    "\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('Admin1Id',pa.int64(), nullable=True),\n",
    "            pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "            pa.field('LOBName', pa.string(), nullable=True),\n",
    "            pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "        underscore_count = parquet_file_path.count('_')\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Admin1_Lob',Cat)\n",
    "        parquet_file_path = os.path.join(export_path, f\"{os.path.splitext(parquet_file_path)[0]}_{idx}.parquet\")\n",
    "\n",
    "        # If there are 21 or more underscores, modify the file path\n",
    "        if underscore_count >= 9:\n",
    "            parts = parquet_file_path.split('_')\n",
    "            # Remove the second last part which contains the number and the underscore before it\n",
    "            parts = parts[:-2] + parts[-1:]\n",
    "            parquet_file_path = '_'.join(parts)\n",
    "\n",
    "        # Write the table to the parquet file\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "#GU\n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_EP_Admin1_Lob_GU_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GU\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "#for GR\n",
    "    \n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_EP_Admin1_Lob_GR_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GR\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_parquet_files(main_folder_path):\n",
    "    subfolders = [\n",
    "        'EP/Admin1_Lob/GU',\n",
    "        'EP/Admin1_Lob/GR',\n",
    "        'EP/Cresta_Lob/GU',\n",
    "        'EP/Cresta_Lob/GR'\n",
    "    ]\n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        folder_path = os.path.join(main_folder_path, subfolder)\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "        \n",
    "        file_groups = {}\n",
    "        for file in files:\n",
    "            parts = file.split('_')\n",
    "            if len(parts) >= 9:\n",
    "                key = parts[8]\n",
    "                if key not in file_groups:\n",
    "                    file_groups[key] = []\n",
    "                file_groups[key].append(file)\n",
    "        \n",
    "        for key, group_files in file_groups.items():\n",
    "            tables = [pq.read_table(os.path.join(folder_path, f)) for f in group_files]\n",
    "            concatenated_table = pa.concat_tables(tables)\n",
    "            new_file_name = '_'.join(group_files[0].split('_')[:8]) + f'_{key}.parquet'\n",
    "            pq.write_table(concatenated_table, os.path.join(folder_path, new_file_name))\n",
    "            \n",
    "            for file in group_files:\n",
    "                os.remove(os.path.join(folder_path, file))\n",
    "\n",
    "concatenate_parquet_files(main_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_resolution\\EP_final_2.1.4(beta).ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W2sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m parquet_file_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mproname\u001b[39m}\u001b[39;00m\u001b[39m_EP_Admin1_Lob_GU_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.parquet\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W2sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W2sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m     process_parquet_files_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \u001b[39m\"\u001b[39;49m\u001b[39mGU\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W2sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mNameError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W2sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError processing \u001b[39m\u001b[39m{\u001b[39;00mlobname\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\RISHIN\\13_ILC_resolution\\EP_final_2.1.4(beta).ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W2sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     os\u001b[39m.\u001b[39mremove(f)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W2sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, row \u001b[39min\u001b[39;00m distinct_admins\u001b[39m.\u001b[39miter_rows(named\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W2sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     admin1_id \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39;49m\u001b[39mAdmin1Id\u001b[39;49m\u001b[39m'\u001b[39;49m]  \u001b[39m# Accessing Admin1Id directly as a key\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     admin1_name \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mAdmin1Name\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# Accessing Admin1Name directly as a key        \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.4%28beta%29.ipynb#W2sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m# Filter the table for the current Admin1Id\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "#EP_ Admin1 Lob updated below\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "# In[129]:\n",
    "\n",
    "\n",
    "def process_parquet_files_2(parquet_files, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pl.read_parquet(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pl.col('LobName') == filter_string)\n",
    "        \n",
    "        # Skip if the filtered table is empty\n",
    "        \n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).agg(pl.sum('Loss').alias('Sum_Loss'))\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        grouped_table_1.write_parquet( os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pl.read_parquet(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pl.concat(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).agg(pl.sum('Sum_Loss').alias('Sum_Loss_sum'))\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort(['Sum_Loss_sum'], descending=True)\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).unique()\n",
    "    distinct_admins = distinct_admins.with_columns(pl.col('Admin1Id').cast(pl.Int64))\n",
    "    sorted_final_table_1.write_parquet(os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    for idx, row in distinct_admins.iter_rows(named=True):\n",
    "        admin1_id = row['Admin1Id']  # Accessing Admin1Id directly as a key\n",
    "        admin1_name = row['Admin1Name']  # Accessing Admin1Name directly as a key        \n",
    "        # Filter the table for the current Admin1Id\n",
    "        filtered_table = sorted_final_table_1.filter(pl.col('Admin1Id') == admin1_id)\n",
    "        dataframe_1 = filtered_table\n",
    "\n",
    "        dataframe_2 = dataframe_1.group_by(['PeriodId', 'Admin1Name', 'Admin1Id']).agg(pl.col('Sum_Loss_sum').max().alias('Max_Loss'))\n",
    "        dataframe_2 = dataframe_2.sort('Max_Loss', reverse=True)\n",
    "        dataframe_2 = dataframe_2.with_columns([\n",
    "        (1 / (speriod * samples)).alias('rate'),\n",
    "        pl.col('rate').cumsum().round(6).alias('cumrate'),\n",
    "        (1 / pl.col('cumrate')).alias('RPs'),\n",
    "        ((pl.col('Max_Loss') - pl.col('Max_Loss').shift(-1)) * (pl.col('cumrate') + pl.col('cumrate').shift(-1)) * 0.5).alias('TCE_OEP_1'),\n",
    "        (pl.col('TCE_OEP_1').shift().cumsum() * pl.col('RPs')).alias('TCE_OEP_2'),\n",
    "        (pl.col('TCE_OEP_2') + pl.col('Max_Loss')).alias('TCE_OEP_Final')\n",
    "        ])\n",
    "\n",
    "        dataframe_3 = dataframe_1.group_by(['PeriodId','Admin1Name','Admin1Id']).agg(pl.col('Sum_Loss_sum').max().alias('S_Sum_Loss'))\n",
    "        dataframe_3 = dataframe_3.sort('S_Sum_Loss', reverse=True)\n",
    "\n",
    "        dataframe_3 = dataframe_3.with_columns([\n",
    "            (1 / (speriod * samples)).alias('rate'),\n",
    "            pl.col('rate').cumsum().round(6).alias('cumrate'),\n",
    "            (1 / pl.col('cumrate')).alias('RPs'),\n",
    "            ((pl.col('S_Sum_Loss') - pl.col('S_Sum_Loss').shift(-1)) * (pl.col('cumrate') + pl.col('cumrate').shift(-1)) * 0.5).alias('TCE_AEP_1'),\n",
    "            (pl.col('TCE_AEP_1').shift().cumsum() * pl.col('RPs')).alias('TCE_AEP_2'),\n",
    "            (pl.col('TCE_AEP_2') + pl.col('S_Sum_Loss')).alias('TCE_AEP_Final')\n",
    "        ])\n",
    "\n",
    "        fdataframe_2 = pl.DataFrame()\n",
    "        fdataframe_3 = pl.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().arg_min()\n",
    "            \n",
    "            # Assign the closest value to the new DataFrames\n",
    "            fdataframe_2 = pl.concat([fdataframe_2, dataframe_2[closest_index_2:closest_index_2+1]])\n",
    "            fdataframe_3 = pl.concat([fdataframe_3, dataframe_3[closest_index_2:closest_index_2+1]])\n",
    "            \n",
    "            # Update the closest value to match the rp value exactly\n",
    "            fdataframe_2 = fdataframe_2.with_columns(pl.lit(float(value)).alias('RPs'))\n",
    "            fdataframe_3 = fdataframe_3.with_columns(pl.lit(float(value)).alias('RPs'))\n",
    "\n",
    "        fdataframe_2 = fdataframe_2.rename({'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'})\n",
    "        columns_to_keep_2 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, variable_name='EPType', value_name='Loss')\n",
    "        melted_df_2 = melted_df_2.rename({'RPs': 'ReturnPeriod'})\n",
    "        final_df_2 = melted_df_2.select(['EPType', 'Loss', 'ReturnPeriod', 'Admin1Name', 'Admin1Id'])\n",
    "\n",
    "        fdataframe_3 = fdataframe_3.rename({'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'})\n",
    "        columns_to_keep_3 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, variable_name='EPType', value_name='Loss')\n",
    "        melted_df_3 = melted_df_3.rename({'RPs': 'ReturnPeriod'})\n",
    "        final_df_3 = melted_df_3.select(['EPType', 'Loss', 'ReturnPeriod', 'Admin1Name', 'Admin1Id'])\n",
    "\n",
    "        final_df_EP_LOB_GU = pl.concat([final_df_2, final_df_3])\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.with_columns(\n",
    "            pl.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        )\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort(['EPType', 'ReturnPeriod'], reverse=[False, True])\n",
    "\n",
    "        # Add LobID and LobName columns\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.with_columns([\n",
    "            pl.lit(lob_id).alias('LOBId'),\n",
    "            pl.lit(filter_string).alias('LOBName')\n",
    "        ])\n",
    "\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.with_columns([\n",
    "            pl.col('LOBId').apply(lambda x: Decimal(x)).alias('LOBId'),\n",
    "            pl.col('Admin1Id').cast(pl.Int64),\n",
    "            pl.col('Admin1Id').apply(lambda x: Decimal(x)).alias('Admin1Id')\n",
    "        ])\n",
    "\n",
    "        # Convert Polars DataFrame to Arrow Table\n",
    "        table = final_df_EP_LOB_GU.to_arrow()\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('Admin1Id', pa.int64(), nullable=True),\n",
    "            pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "            pa.field('LOBName', pa.string(), nullable=True),\n",
    "            pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        ])\n",
    "\n",
    "        underscore_count = parquet_file_path.count('_')\n",
    "\n",
    "        export_path = os.path.join(main_folder_path, 'EP', 'Admin1_Lob', Cat)\n",
    "        parquet_file_path = os.path.join(export_path, f\"{os.path.splitext(parquet_file_path)[0]}_{idx}.parquet\")\n",
    "\n",
    "        # If there are 21 or more underscores, modify the file path\n",
    "        if underscore_count >= 9:\n",
    "            parts = parquet_file_path.split('_')\n",
    "            # Remove the second last part which contains the number and the underscore before it\n",
    "            parts = parts[:-2] + parts[-1:]\n",
    "            parquet_file_path = '_'.join(parts)\n",
    "\n",
    "        # Write the table to the parquet file\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "#GU\n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_EP_Admin1_Lob_GU_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GU\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "#for GR\n",
    "    \n",
    "for i, (lobname, lobid) in enumerate(lobname_to_lobid.items()):\n",
    "    parquet_file_path = f'{proname}_EP_Admin1_Lob_GR_{i}.parquet'\n",
    "    try:\n",
    "        process_parquet_files_2(parquet_files_grp, lobname, lobid, speriod, samples, rps_values, parquet_file_path, \"GR\")\n",
    "    except (NameError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error processing {lobname}: {e}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Get intermediate files\n",
    "intermediate_files = [\n",
    "    os.path.join(partial_folder_path, f) \n",
    "    for f in os.listdir(partial_folder_path) \n",
    "    if f.startswith('grouped_table_1_')\n",
    "]\n",
    "\n",
    "# Initialize dictionary to accumulate sums\n",
    "banana_sums = {}\n",
    "chunk_size = 1000000  # Adjust based on your memory constraints\n",
    "\n",
    "# Process each file in chunks\n",
    "print(\"Processing chunks and accumulating sums...\")\n",
    "for file_path in intermediate_files:\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    \n",
    "    for batch in parquet_file.iter_batches(batch_size=chunk_size):\n",
    "        # Convert batch to pandas for grouping\n",
    "        df_chunk = batch.to_pandas()\n",
    "        \n",
    "        # Group by AppleName and sum Banana\n",
    "        grouped = df_chunk.groupby('AppleName')['Banana'].sum()\n",
    "        \n",
    "        # Update running sums\n",
    "        for apple_name, banana_sum in grouped.items():\n",
    "            if apple_name in banana_sums:\n",
    "                banana_sums[apple_name] += banana_sum\n",
    "            else:\n",
    "                banana_sums[apple_name] = banana_sum\n",
    "        \n",
    "        # Clean up memory\n",
    "        del df_chunk\n",
    "        del grouped\n",
    "        gc.collect()\n",
    "    \n",
    "    # Clean up file objects\n",
    "    del parquet_file\n",
    "    gc.collect()\n",
    "\n",
    "# Convert accumulated sums to DataFrame\n",
    "print(\"Creating final DataFrame...\")\n",
    "final_df_fruit = pd.DataFrame.from_dict(\n",
    "    banana_sums, \n",
    "    orient='index', \n",
    "    columns=['Banana']\n",
    ").reset_index().rename(columns={'index': 'AppleName'})\n",
    "\n",
    "# Sort by Banana in descending order\n",
    "final_df_fruit = final_df_fruit.sort_values('Banana', ascending=False)\n",
    "\n",
    "# Map AppleName to AppleId\n",
    "final_df_fruit['AppleId'] = final_df_fruit['AppleName'].map(apple_name_to_apple_id).apply(lambda x: Decimal(x))\n",
    "\n",
    "# Add NaN columns\n",
    "final_df_fruit['Cherry'] = np.nan\n",
    "final_df_fruit['Date'] = np.nan\n",
    "\n",
    "# Reorder columns\n",
    "final_df_fruit = final_df_fruit[['Banana', 'Cherry', 'Date', 'AppleId', 'AppleName']]\n",
    "\n",
    "# Define schema\n",
    "desired_schema_fruit = pa.schema([\n",
    "    pa.field('Banana', pa.float64()),\n",
    "    pa.field('Cherry', pa.float64()),\n",
    "    pa.field('Date', pa.float64()),\n",
    "    pa.field('AppleId', pa.decimal128(38)),\n",
    "    pa.field('AppleName', pa.string())\n",
    "])\n",
    "\n",
    "# Export in chunks\n",
    "print(\"Exporting data in chunks...\")\n",
    "export_chunk_size = min(chunk_size, len(final_df_fruit))\n",
    "for i in range(0, len(final_df_fruit), export_chunk_size):\n",
    "    chunk = final_df_fruit.iloc[i:i+export_chunk_size]\n",
    "    \n",
    "    # Convert chunk to PyArrow table with schema\n",
    "    table_chunk = pa.Table.from_pandas(chunk, schema=desired_schema_fruit)\n",
    "    \n",
    "    # Write chunk to parquet\n",
    "    if i == 0:\n",
    "        # First chunk: create new file\n",
    "        pq.write_table(table_chunk, parquet_file_path)\n",
    "    else:\n",
    "        # Subsequent chunks: append to existing file\n",
    "        pq.write_table(table_chunk, parquet_file_path, append=True)\n",
    "    \n",
    "    # Clean up\n",
    "    del chunk\n",
    "    del table_chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "# Clean up folders\n",
    "if os.path.exists(partial_folder_path):\n",
    "    for file in os.listdir(partial_folder_path):\n",
    "        file_path = os.path.join(partial_folder_path, file)\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file_path}: {e}\")\n",
    "    try:\n",
    "        os.rmdir(partial_folder_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting folder {partial_folder_path}: {e}\")\n",
    "\n",
    "if os.path.exists(concatenated_folder_path):\n",
    "    for file in os.listdir(concatenated_folder_path):\n",
    "        file_path = os.path.join(concatenated_folder_path, file)\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file_path}: {e}\")\n",
    "    try:\n",
    "        os.rmdir(concatenated_folder_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting folder {concatenated_folder_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_PLT_lob(parquet_files, export_path):\n",
    "    # Directory to store intermediate results\n",
    "    intermediate_dir = os.path.join(main_folder_path, 'intermediate_results')\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file in chunks and write intermediate results to disk\n",
    "    for i, file in enumerate(parquet_files):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            \n",
    "            # Cast columns to the desired types\n",
    "            table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('LobId'), 'LobId', pa.compute.cast(table['LobId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('LobName'), 'LobName', pa.compute.cast(table['LobName'], pa.string()))\n",
    "            \n",
    "            grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "            pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "            import pyarrow as pa\n",
    "\n",
    "\n",
    "# Get intermediate files\n",
    "intermediate_files = [\n",
    "    os.path.join(partial_folder_path, f) \n",
    "    for f in os.listdir(partial_folder_path) \n",
    "    if f.startswith('grouped_table_1_')\n",
    "]\n",
    "\n",
    "# Initialize dictionary to accumulate sums\n",
    "banana_sums = {}\n",
    "chunk_size = 1000000  # Adjust based on your memory constraints\n",
    "\n",
    "# Process each file in chunks\n",
    "print(\"Processing chunks and accumulating sums...\")\n",
    "for file_path in intermediate_files:\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    \n",
    "    for batch in parquet_file.iter_batches(batch_size=chunk_size):\n",
    "        # Convert batch to pandas for grouping\n",
    "        df_chunk = batch.to_pandas()\n",
    "        \n",
    "        # Group by AppleName and sum Banana\n",
    "        grouped = df_chunk.groupby('AppleName')['Banana'].sum()\n",
    "        \n",
    "        # Update running sums\n",
    "        for apple_name, banana_sum in grouped.items():\n",
    "            if apple_name in banana_sums:\n",
    "                banana_sums[apple_name] += banana_sum\n",
    "            else:\n",
    "                banana_sums[apple_name] = banana_sum\n",
    "        \n",
    "        # Clean up memory\n",
    "        del df_chunk\n",
    "        del grouped\n",
    "        gc.collect()\n",
    "    \n",
    "    # Clean up file objects\n",
    "    del parquet_file\n",
    "    gc.collect()\n",
    "\n",
    "# Convert accumulated sums to DataFrame\n",
    "print(\"Creating final DataFrame...\")\n",
    "final_df_fruit = pd.DataFrame.from_dict(\n",
    "    banana_sums, \n",
    "    orient='index', \n",
    "    columns=['Banana']\n",
    ").reset_index().rename(columns={'index': 'AppleName'})\n",
    "\n",
    "# Sort by Banana in descending order\n",
    "final_df_fruit = final_df_fruit.sort_values('Banana', ascending=False)\n",
    "\n",
    "# Map AppleName to AppleId\n",
    "final_df_fruit['AppleId'] = final_df_fruit['AppleName'].map(apple_name_to_apple_id).apply(lambda x: Decimal(x))\n",
    "\n",
    "# Add NaN columns\n",
    "final_df_fruit['Cherry'] = np.nan\n",
    "final_df_fruit['Date'] = np.nan\n",
    "\n",
    "# Reorder columns\n",
    "final_df_fruit = final_df_fruit[['Banana', 'Cherry', 'Date', 'AppleId', 'AppleName']]\n",
    "\n",
    "# Define schema\n",
    "desired_schema_fruit = pa.schema([\n",
    "    pa.field('Banana', pa.float64()),\n",
    "    pa.field('Cherry', pa.float64()),\n",
    "    pa.field('Date', pa.float64()),\n",
    "    pa.field('AppleId', pa.decimal128(38)),\n",
    "    pa.field('AppleName', pa.string())\n",
    "])\n",
    "\n",
    "# Export in chunks\n",
    "print(\"Exporting data in chunks...\")\n",
    "export_chunk_size = min(chunk_size, len(final_df_fruit))\n",
    "for i in range(0, len(final_df_fruit), export_chunk_size):\n",
    "    chunk = final_df_fruit.iloc[i:i+export_chunk_size]\n",
    "    \n",
    "    # Convert chunk to PyArrow table with schema\n",
    "    table_chunk = pa.Table.from_pandas(chunk, schema=desired_schema_fruit)\n",
    "    \n",
    "    # Write chunk to parquet\n",
    "    if i == 0:\n",
    "        # First chunk: create new file\n",
    "        pq.write_table(table_chunk, parquet_file_path)\n",
    "    else:\n",
    "        # Subsequent chunks: append to existing file\n",
    "        pq.write_table(table_chunk, parquet_file_path, append=True)\n",
    "    \n",
    "    # Clean up\n",
    "    del chunk\n",
    "    del table_chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "# Clean up folders\n",
    "if os.path.exists(partial_folder_path):\n",
    "    for file in os.listdir(partial_folder_path):\n",
    "        file_path = os.path.join(partial_folder_path, file)\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file_path}: {e}\")\n",
    "    try:\n",
    "        os.rmdir(partial_folder_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting folder {partial_folder_path}: {e}\")\n",
    "\n",
    "if os.path.exists(concatenated_folder_path):\n",
    "    for file in os.listdir(concatenated_folder_path):\n",
    "        file_path = os.path.join(concatenated_folder_path, file)\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file_path}: {e}\")\n",
    "    try:\n",
    "        os.rmdir(concatenated_folder_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting folder {concatenated_folder_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Read intermediate results and combine them\n",
    "    intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "    intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "    combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "    # Perform the final group by and aggregation\n",
    "    final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "    final_grouped_table = final_grouped_table.sort_by([('Loss_sum_sum', 'descending')])\n",
    "\n",
    "    # Rename the aggregated column\n",
    "    final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "    # Reorder the columns in the desired order\n",
    "    final_grouped_table = final_grouped_table.select(ordered_columns)\n",
    "\n",
    "    # Save the final table to a Parquet file\n",
    "        # Delete intermediate files\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "        # Remove the intermediate directory\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {intermediate_dir}\")\n",
    "    except OSError:\n",
    "        print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "    try:\n",
    "        pq.write_table(final_grouped_table, export_path)\n",
    "        print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Parquet file: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('LobName', pa.string(), nullable=True)\n",
    "])\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for GU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GU', f'{proname}_PLT_Lob_GU_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files, export_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GR', f'{proname}_PLT_Lob_GR_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files_gr, export_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
