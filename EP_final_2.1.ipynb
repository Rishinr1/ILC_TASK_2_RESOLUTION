{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "from decimal import Decimal  # Add this import statement\n",
    "import pyarrow.dataset as ds\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import sqlalchemy as sa\n",
    "import pyodbc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # Start time\n",
    "\n",
    "\n",
    "# Function to flush the cache\n",
    "def flush_cache():\n",
    "    gc.collect()\n",
    "\n",
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = input(\"Enter the output folder path: \")\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_resolution\\input\\PARQUET_FILES'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_gr = r'D:\\RISHIN\\13_ILC_TASK\\input\\PARQUET_FILES_GR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "speriod=int(input(\"Enter the simulation period: \"))\n",
    "samples=int(input(\"Enter the number of samples: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files_gr = [os.path.join(folder_path_gr, f) for f in os.listdir(folder_path_gr) if f.endswith('.parquet')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_folder_and_files(folder_path):\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        # Delete all files inside the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        \n",
    "        # Delete the folder itself\n",
    "        os.rmdir(folder_path)\n",
    "        print(f'Successfully deleted the folder: {folder_path}')\n",
    "    else:\n",
    "        print(f'The folder {folder_path} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio','Admin1','Admin1_Lob','Cresta','Cresta_Lob',]\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            for nested_folder in nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "resolution_folder_path = os.path.join(processing_folder_path, 'Resolution Added')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_folder_path_gr = os.path.join(processing_folder_path, 'Resolution Added_gr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_0_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_0_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_0_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_10_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_10_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_10_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_11_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_11_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_11_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_12_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_12_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_12_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_13_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_13_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_13_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_14_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_14_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_14_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_15_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_15_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_15_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_1_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_1_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_2_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_2_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_2_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_3_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_3_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_3_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_4_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_4_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_4_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_5_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_5_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_5_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_6_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_6_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_7_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_7_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_7_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_8_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_8_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_8_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_9_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_9_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_9_300.parquet\n"
     ]
    }
   ],
   "source": [
    "def connect_to_database(server, database):\n",
    "    connection_string = f'mssql+pyodbc://{server}/{database}?driver=SQL+Server+Native+Client+11.0'\n",
    "    engine = sa.create_engine(connection_string)\n",
    "    connection = engine.connect()\n",
    "    return connection\n",
    "\n",
    "def read_parquet_file(file_path):\n",
    "    table = pq.read_table(file_path)\n",
    "    return table\n",
    "\n",
    "def fetch_database_data(connection):\n",
    "    address_query = 'SELECT ADDRESSID, ADMIN1GEOID AS Admin1Id, Admin1Name, zone1GEOID AS CrestaId, Zone1 AS CrestaName FROM Address'\n",
    "    address_df = pd.read_sql(address_query, connection)\n",
    "    address_table = pa.Table.from_pandas(address_df)\n",
    "    return address_table\n",
    "\n",
    "def join_dataframes(parquet_table, address_table):\n",
    "    parquet_df = parquet_table.to_pandas()\n",
    "    address_df = address_table.to_pandas()\n",
    "    df = parquet_df.merge(address_df, left_on='LocationId', right_on='ADDRESSID', how='left')\n",
    "    return pa.Table.from_pandas(df)\n",
    "\n",
    "def save_joined_dataframe(joined_table, output_file):\n",
    "    pq.write_table(joined_table, output_file)\n",
    "    print(f\"Saved joined file to {output_file}\")\n",
    "\n",
    "def process_parquet_files(folder_path, output_folder, server, database):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    connection = connect_to_database(server, database)\n",
    "    address_table = fetch_database_data(connection)\n",
    "\n",
    "    parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "    for file in parquet_files:\n",
    "        parquet_table = read_parquet_file(file)\n",
    "        joined_table = join_dataframes(parquet_table, address_table)\n",
    "        output_file = os.path.join(output_folder, os.path.basename(file))\n",
    "        save_joined_dataframe(joined_table, output_file)\n",
    "        del parquet_table\n",
    "        del joined_table\n",
    "        gc.collect()\n",
    "\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "output_folder = r'D:\\RISHIN\\TESTING\\TEST_2'\n",
    "server = 'localhost'\n",
    "database = 'IED2024_EUWS_PC_MIX_EDM230_ILC_LOB_UPDATE_20240905'\n",
    "\n",
    "process_parquet_files(folder_path, resolution_folder_path, server, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_0_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_0_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_0_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_10_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_10_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_10_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_11_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_11_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_11_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_12_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_12_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_12_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_13_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_13_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_13_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_14_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_14_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_14_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_15_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_15_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_15_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_1_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_1_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_2_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_2_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_2_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_3_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_3_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_3_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_4_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_4_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_4_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_5_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_5_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_5_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_6_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_6_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_7_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_7_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_7_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_8_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_8_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_8_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_9_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_9_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_9_300.parquet\n"
     ]
    }
   ],
   "source": [
    "process_parquet_files(folder_path_gr, resolution_folder_path_gr, server, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Admin_1 Lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_parquet_files(parquet_files, export_path, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        # Skip if the filtered table is empty\n",
    "        if len(table) == 0:\n",
    "            continue\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'Admin1Name', 'Admin1Id']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Add LobID and LobName columns\n",
    "    final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "    final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "    final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "    final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].astype('int64')\n",
    "    final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('Admin1Id',pa.int64(), nullable=True),\n",
    "        pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "        pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LOBName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files_grp = [os.path.join(resolution_folder_path, f) for f in os.listdir(resolution_folder_path) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files_grp_gr = [os.path.join(resolution_folder_path_gr, f) for f in os.listdir(resolution_folder_path_gr) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "export_path =os.path.join(main_folder_path, 'EP','Admin1_Lob','GU')\n",
    "parquet_file_path_AUTO = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_6.parquet')\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#NEXT FOR GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path_gr =os.path.join(main_folder_path, 'EP', 'Admin1_Lob','GR')\n",
    "parquet_file_path_AUTO = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_6.parquet')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "def process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values,parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs','Admin1Name','Admin1Id']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs','Admin1Name','Admin1Id']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "    final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "    final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].astype('int64')\n",
    "    final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('Admin1Id',pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "export_path =os.path.join(main_folder_path,'EP','Admin1','GU')\n",
    "parquet_file_path = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files_grp, export_path, speriod, samples, rps_values, parquet_file_path)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "export_path_GR =os.path.join(main_folder_path,'EP','Admin1','GR')\n",
    "parquet_file_path_GR = os.path.join(export_path_GR, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files_grp_gr, export_path_GR, speriod, samples, rps_values, parquet_file_path_GR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EP cresta lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_parquet_files_EP_Cresta_lob(parquet_files, export_path, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        # Skip if the filtered table is empty\n",
    "        if len(table) == 0:\n",
    "            continue\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs', 'CrestaName','CrestaId']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'CrestaName','CrestaId']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs', 'CrestaName','CrestaId']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Add LobID and LobName columns\n",
    "    final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "    final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "    final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "    final_df_EP_LOB_GU['CrestaId'] = final_df_EP_LOB_GU['CrestaId'].astype('int64')\n",
    "    final_df_EP_LOB_GU['CrestaId'] = final_df_EP_LOB_GU['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('CrestaId',pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('CrestaName', pa.string(), nullable=True),\n",
    "        pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LOBName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "export_path =os.path.join(main_folder_path, 'EP','Cresta_Lob','GU')\n",
    "parquet_file_path_AUTO = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_6.parquet')\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#NEXT FOR GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path_gr =os.path.join(main_folder_path, 'EP', 'Cresta_Lob','GR')\n",
    "parquet_file_path_AUTO = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_6.parquet')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "def process_parquet_files_Port_EP_Cresta(parquet_files, export_path, speriod, samples, rps_values,parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs','CrestaName','CrestaId']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs','CrestaName','CrestaId']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "    final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "    final_df_EP_Portfolio_GU['CrestaId'] = final_df_EP_Portfolio_GU['CrestaId'].astype('int64')\n",
    "    final_df_EP_Portfolio_GU['CrestaId'] = final_df_EP_Portfolio_GU['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('CrestaId',pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('CrestaName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "export_path =os.path.join(main_folder_path,'EP','Cresta','GU')\n",
    "parquet_file_path = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port_EP_Cresta(parquet_files_grp, export_path, speriod, samples, rps_values, parquet_file_path)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "export_path_GR =os.path.join(main_folder_path,'EP','Cresta','GR')\n",
    "parquet_file_path_GR = os.path.join(export_path_GR, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port_EP_Cresta(parquet_files_grp_gr, export_path_GR, speriod, samples, rps_values, parquet_file_path_GR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_Lob_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "#now for stats LOB GU\n",
    "\n",
    "\n",
    "def process_lob_stats_Admin1_Lob(parquet_files, parquet_file_path):\n",
    "    aggregated_tables_lob_stats = []\n",
    "\n",
    "    # Define the mapping of LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': 1,\n",
    "        'AUTO': 2,\n",
    "        'COM': 3,\n",
    "        'IND': 4,\n",
    "        'SPER': 5,\n",
    "        'FRST': 6,\n",
    "        'GLH': 7\n",
    "    }\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by(['LobName','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL','Admin1Name','Admin1Id'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            aggregated_tables_lob_stats.append(grouped)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    # Check if any tables were aggregated\n",
    "    if not aggregated_tables_lob_stats:\n",
    "        print(\"No tables were aggregated. Please check the input files.\")\n",
    "    else:\n",
    "        # Concatenate all the grouped Tables\n",
    "        final_table = pa.concat_tables(aggregated_tables_lob_stats)\n",
    "\n",
    "        # Group the final Table again to ensure all groups are combined\n",
    "        final_grouped = final_table.group_by(['LobName','Admin1Name','Admin1Id']).aggregate([('AAL', 'sum')])\n",
    "\n",
    "        # Sort the final grouped Table by 'AAL' in descending order\n",
    "        final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "        # Convert the final grouped Table to a Pandas DataFrame\n",
    "        final_df = final_grouped.to_pandas()\n",
    "\n",
    "        # Map LobName to LobId\n",
    "        final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "        final_df['Admin1Id'] = final_df['Admin1Id'].astype('int64')\n",
    "        final_df['Admin1Id'] = final_df['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "        final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "        # Define the columns with NaN values for 'Std' and 'CV'\n",
    "        final_df_STATS_Lob['Std'] = np.nan\n",
    "        final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "        # Reorder the columns to match the specified format\n",
    "        final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName','Admin1Name','Admin1Id']]\n",
    "\n",
    "        # Define the desired schema\n",
    "        desired_schema = pa.schema([\n",
    "            pa.field('AAL', pa.float64()),\n",
    "            pa.field('Std', pa.float64()),\n",
    "            pa.field('CV', pa.float64()),\n",
    "            pa.field('Admin1Id', pa.decimal128(38)),\n",
    "            pa.field('Admin1Name', pa.string()),\n",
    "            pa.field('LobId', pa.decimal128(38)),\n",
    "            pa.field('LobName', pa.string())\n",
    "        ])\n",
    "\n",
    "        # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "        final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "        pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1_Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_Lob_GU_0.parquet')\n",
    "process_lob_stats_Admin1_Lob(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1_Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_Lob_GR_0.parquet')\n",
    "process_lob_stats_Admin1_Lob(parquet_files_grp_gr, parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "def process_portfolio_stats_Admin1(parquet_files, export_path):\n",
    "    aggregated_tables = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by(['Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select([ 'AAL','Admin1Name','Admin1Id'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        aggregated_tables.append(grouped)\n",
    "\n",
    "    # Concatenate all the grouped Tables\n",
    "    final_table = pa.concat_tables(aggregated_tables)\n",
    "    final_table=final_table.group_by(['Admin1Name','Admin1Id']).aggregate([('AAL', 'sum')])\n",
    "    final_table=final_table.sort_by([('AAL_sum', 'descending')])\n",
    "    final_table=final_table.rename_columns(['Admin1Name','Admin1Id','AAL'])\n",
    "\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "    final_df=final_df.sort_values(by='AAL')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].astype('int64')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "    final_df['Std']=np.nan\n",
    "    final_df['CV']=np.nan\n",
    "    # Get the exact values of Admin1Id and Admin1Name\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    #total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('Admin1Id', pa.decimal128(38)),\n",
    "        pa.field('Admin1Name', pa.string()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_GU_0.parquet')\n",
    "process_portfolio_stats_Admin1(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Admin1', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_GR_0.parquet')\n",
    "process_portfolio_stats_Admin1(parquet_files_grp_gr, parquet_file_path_gr)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_table\n",
    "del final_table_STATS_Portfolio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
