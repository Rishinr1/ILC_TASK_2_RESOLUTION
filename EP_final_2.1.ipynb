{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "from decimal import Decimal  # Add this import statement\n",
    "import pyarrow.dataset as ds\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import sqlalchemy as sa\n",
    "import pyodbc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # Start time\n",
    "\n",
    "\n",
    "# Function to flush the cache\n",
    "def flush_cache():\n",
    "    gc.collect()\n",
    "\n",
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = input(\"Enter the output folder path: \")\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_resolution\\input\\PARQUET_FILES'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_gr = r'D:\\RISHIN\\13_ILC_TASK\\input\\PARQUET_FILES_GR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "speriod=int(input(\"Enter the simulation period: \"))\n",
    "samples=int(input(\"Enter the number of samples: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files_gr = [os.path.join(folder_path_gr, f) for f in os.listdir(folder_path_gr) if f.endswith('.parquet')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_folder_and_files(folder_path):\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        # Delete all files inside the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        \n",
    "        # Delete the folder itself\n",
    "        os.rmdir(folder_path)\n",
    "        print(f'Successfully deleted the folder: {folder_path}')\n",
    "    else:\n",
    "        print(f'The folder {folder_path} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio','Admin1','Admin1_Lob','Cresta','Cresta_Lob',]\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            for nested_folder in nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "resolution_folder_path = os.path.join(processing_folder_path, 'Resolution Added')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_folder_path_gr = os.path.join(processing_folder_path, 'Resolution Added_gr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_0_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_0_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_0_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_10_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_10_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_10_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_11_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_11_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_11_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_12_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_12_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_12_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_13_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_13_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_13_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_14_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_14_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_14_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_15_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_15_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_15_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_1_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_1_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_2_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_2_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_2_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_3_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_3_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_3_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_4_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_4_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_4_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_5_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_5_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_5_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_6_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_6_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_7_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_7_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_7_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_8_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_8_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_8_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_9_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_9_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_9_300.parquet\n"
     ]
    }
   ],
   "source": [
    "def connect_to_database(server, database):\n",
    "    connection_string = f'mssql+pyodbc://{server}/{database}?driver=SQL+Server+Native+Client+11.0'\n",
    "    engine = sa.create_engine(connection_string)\n",
    "    connection = engine.connect()\n",
    "    return connection\n",
    "\n",
    "def read_parquet_file(file_path):\n",
    "    table = pq.read_table(file_path)\n",
    "    return table\n",
    "\n",
    "def fetch_database_data(connection):\n",
    "    address_query = 'SELECT ADDRESSID, ADMIN1GEOID AS Admin1Id, Admin1Name, zone1GEOID AS CrestaId, Zone1 AS CrestaName FROM Address'\n",
    "    address_df = pd.read_sql(address_query, connection)\n",
    "    address_table = pa.Table.from_pandas(address_df)\n",
    "    return address_table\n",
    "\n",
    "def join_dataframes(parquet_table, address_table):\n",
    "    parquet_df = parquet_table.to_pandas()\n",
    "    address_df = address_table.to_pandas()\n",
    "    df = parquet_df.merge(address_df, left_on='LocationId', right_on='ADDRESSID', how='left')\n",
    "    return pa.Table.from_pandas(df)\n",
    "\n",
    "def save_joined_dataframe(joined_table, output_file):\n",
    "    pq.write_table(joined_table, output_file)\n",
    "    print(f\"Saved joined file to {output_file}\")\n",
    "\n",
    "def process_parquet_files(folder_path, output_folder, server, database):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    connection = connect_to_database(server, database)\n",
    "    address_table = fetch_database_data(connection)\n",
    "\n",
    "    parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "    for file in parquet_files:\n",
    "        parquet_table = read_parquet_file(file)\n",
    "        joined_table = join_dataframes(parquet_table, address_table)\n",
    "        output_file = os.path.join(output_folder, os.path.basename(file))\n",
    "        save_joined_dataframe(joined_table, output_file)\n",
    "        del parquet_table\n",
    "        del joined_table\n",
    "        gc.collect()\n",
    "\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "output_folder = r'D:\\RISHIN\\TESTING\\TEST_2'\n",
    "server = 'localhost'\n",
    "database = 'IED2024_EUWS_PC_MIX_EDM230_ILC_LOB_UPDATE_20240905'\n",
    "\n",
    "process_parquet_files(folder_path, resolution_folder_path, server, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_0_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_0_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_0_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_10_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_10_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_10_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_11_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_11_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_11_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_12_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_12_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_12_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_13_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_13_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_13_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_14_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_14_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_14_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_15_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_15_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_15_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_1_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_1_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_2_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_2_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_2_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_3_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_3_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_3_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_4_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_4_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_4_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_5_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_5_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_5_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_6_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_6_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_7_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_7_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_7_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_8_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_8_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_8_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_9_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_9_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_9_300.parquet\n"
     ]
    }
   ],
   "source": [
    "process_parquet_files(folder_path_gr, resolution_folder_path_gr, server, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Admin_1 Lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_parquet_files(parquet_files, export_path, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        # Skip if the filtered table is empty\n",
    "        if len(table) == 0:\n",
    "            continue\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'Admin1Name', 'Admin1Id']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Add LobID and LobName columns\n",
    "    final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "    final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "    final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "    final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].astype('int64')\n",
    "    final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('Admin1Id',pa.int64(), nullable=True),\n",
    "        pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "        pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LOBName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files_grp = [os.path.join(resolution_folder_path, f) for f in os.listdir(resolution_folder_path) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files_grp_gr = [os.path.join(resolution_folder_path_gr, f) for f in os.listdir(resolution_folder_path_gr) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "export_path =os.path.join(main_folder_path, 'EP','Admin1_Lob','GU')\n",
    "parquet_file_path_AUTO = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_6.parquet')\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#NEXT FOR GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path_gr =os.path.join(main_folder_path, 'EP', 'Admin1_Lob','GR')\n",
    "parquet_file_path_AUTO = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_6.parquet')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete below if aint working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_parquet_files(parquet_files, export_path, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        # Skip if the filtered table is empty\n",
    "        if len(table) == 0:\n",
    "            continue\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'Admin1Name', 'Admin1Id']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Add LobID and LobName columns\n",
    "    final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "    final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "    final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "    final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].astype('int64')\n",
    "    final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('Admin1Id',pa.int64(), nullable=True),\n",
    "        pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "        pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LOBName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "export_path =os.path.join(main_folder_path, 'EP','Admin1_Lob','GU')\n",
    "parquet_file_path_AUTO = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GU_6.parquet')\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#NEXT FOR GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path_gr =os.path.join(main_folder_path, 'EP', 'Admin1_Lob','GR')\n",
    "parquet_file_path_AUTO = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_Lob_GR_6.parquet')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_grp_gr, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete above if aint working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "# def process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values,parquet_file_path):\n",
    "#     partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "#     concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "#     os.makedirs(partial_folder_path, exist_ok=True)\n",
    "#     os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "#     # Initialize an empty list to store the results\n",
    "#     final_grouped_table_1 = []\n",
    "\n",
    "#     # Process each Parquet file individually\n",
    "#     for file in parquet_files:\n",
    "#         # Read the Parquet file into a PyArrow Table\n",
    "#         table = pq.read_table(file)\n",
    "\n",
    "#         grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "#         grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "#         # Write intermediate results to disk\n",
    "#         pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "#     # Read all intermediate files and concatenate them\n",
    "#     intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "#     final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "#     final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "#     # Perform final grouping and sorting\n",
    "#     f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "#     sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "#     pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "#     # Delete all non-concatenated files\n",
    "#     for f in intermediate_files_1:\n",
    "#         os.remove(f)\n",
    "    \n",
    "#     dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "#     dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "#     dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "#     dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "#     dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "#     dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "#     dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "#     dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "#     dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "#     dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "#     dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "#     dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "#     dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "#     dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "#     dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "#     dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "#     dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "#     dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "#     dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "#     fdataframe_2 = pd.DataFrame()\n",
    "#     fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "#     for value in rps_values:\n",
    "#         closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "#         fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "#         fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "#     fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "#     columns_to_keep_2 = ['RPs','Admin1Name','Admin1Id']\n",
    "#     columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "#     melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "#     melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "#     final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "#     fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "#     columns_to_keep_3 = ['RPs','Admin1Name','Admin1Id']\n",
    "#     columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "#     melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "#     melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "#     final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "#     final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "#     new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "#     final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "#     final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "#     final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].astype('int64')\n",
    "#     final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "#     # Define the schema to match the required Parquet file schema\n",
    "#     schema = pa.schema([\n",
    "#         pa.field('EPType', pa.string(), nullable=True),\n",
    "#         pa.field('Loss', pa.float64(), nullable=True),\n",
    "#         pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "#         pa.field('Admin1Id',pa.decimal128(38, 0), nullable=True),\n",
    "#         pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "#     ])\n",
    "\n",
    "#     # Convert DataFrame to Arrow Table with the specified schema\n",
    "#     table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "\n",
    "#     # Save to Parquet\n",
    "#     pq.write_table(table, parquet_file_path)\n",
    "\n",
    "#     print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "# #FOR GU\n",
    "\n",
    "# export_path =os.path.join(main_folder_path,'EP','Admin1','GU')\n",
    "# parquet_file_path = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_0.parquet')\n",
    "# try:\n",
    "#     process_parquet_files_Port(parquet_files_grp, export_path, speriod, samples, rps_values, parquet_file_path)\n",
    "# except (NameError, AttributeError,ValueError) as e:\n",
    "#     print(f\"Error processing : {e}\")\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #FOR GR\n",
    "\n",
    "\n",
    "# export_path_GR =os.path.join(main_folder_path,'EP','Admin1','GR')\n",
    "# parquet_file_path_GR = os.path.join(export_path_GR, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_0.parquet')\n",
    "# try:\n",
    "#     process_parquet_files_Port(parquet_files_grp_gr, export_path_GR, speriod, samples, rps_values, parquet_file_path_GR)\n",
    "# except (NameError, AttributeError,ValueError) as e:\n",
    "#     print(f\"Error processing : {e}\")\n",
    "#     pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete snippet if not working ps its working dont delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_10.parquet\n"
     ]
    }
   ],
   "source": [
    "def process_parquet_files_Port_2(parquet_files, export_path, speriod, samples, rps_values,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate', 'Admin1Name', 'Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['Admin1Id']\n",
    "        admin1_name = row['Admin1Name']\n",
    "\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['Admin1Id'], admin1_id))\n",
    "\n",
    "        # Convert to pandas DataFrame\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "            fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "            fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs','Admin1Name','Admin1Id']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs','Admin1Name','Admin1Id']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "        final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].astype('int64')\n",
    "        final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('Admin1Id', pa.decimal128(38, 0), nullable=True),\n",
    "            pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "        #FOR GU\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Admin1',Cat)\n",
    "        parquet_file_path = os.path.join(export_path,f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_{Cat}_{idx}.parquet')\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "try:\n",
    "    process_parquet_files_Port_2(parquet_files_grp, export_path, speriod, samples, rps_values,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "#FOR GR\n",
    "try:\n",
    "    process_parquet_files_Port_2(parquet_files_grp_gr, export_path_GR, speriod, samples, rps_values,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete above///// dont deleteeeeeeeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EP cresta lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_parquet_files_EP_Cresta_lob(parquet_files, export_path, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        # Skip if the filtered table is empty\n",
    "        if len(table) == 0:\n",
    "            continue\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs', 'CrestaName','CrestaId']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'CrestaName','CrestaId']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs', 'CrestaName','CrestaId']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Add LobID and LobName columns\n",
    "    final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "    final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "    final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "    final_df_EP_LOB_GU['CrestaId'] = final_df_EP_LOB_GU['CrestaId'].astype('int64')\n",
    "    final_df_EP_LOB_GU['CrestaId'] = final_df_EP_LOB_GU['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('CrestaId',pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('CrestaName', pa.string(), nullable=True),\n",
    "        pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LOBName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "export_path =os.path.join(main_folder_path, 'EP','Cresta_Lob','GU')\n",
    "parquet_file_path_AUTO = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_6.parquet')\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#NEXT FOR GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path_gr =os.path.join(main_folder_path, 'EP', 'Cresta_Lob','GR')\n",
    "parquet_file_path_AUTO = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_6.parquet')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob(parquet_files_grp_gr, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "def process_parquet_files_Port_EP_Cresta(parquet_files, export_path, speriod, samples, rps_values,parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs','CrestaName','CrestaId']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs','CrestaName','CrestaId']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "    final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "    final_df_EP_Portfolio_GU['CrestaId'] = final_df_EP_Portfolio_GU['CrestaId'].astype('int64')\n",
    "    final_df_EP_Portfolio_GU['CrestaId'] = final_df_EP_Portfolio_GU['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('CrestaId',pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('CrestaName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "export_path =os.path.join(main_folder_path,'EP','Cresta','GU')\n",
    "parquet_file_path = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port_EP_Cresta(parquet_files_grp, export_path, speriod, samples, rps_values, parquet_file_path)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "export_path_GR =os.path.join(main_folder_path,'EP','Cresta','GR')\n",
    "parquet_file_path_GR = os.path.join(export_path_GR, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port_EP_Cresta(parquet_files_grp_gr, export_path_GR, speriod, samples, rps_values, parquet_file_path_GR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_Lob_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "#now for stats LOB GU admin\n",
    "\n",
    "\n",
    "def process_lob_stats_Admin1_Lob(parquet_files, parquet_file_path):\n",
    "    aggregated_tables_lob_stats = []\n",
    "\n",
    "    # Define the mapping of LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': 1,\n",
    "        'AUTO': 2,\n",
    "        'COM': 3,\n",
    "        'IND': 4,\n",
    "        'SPER': 5,\n",
    "        'FRST': 6,\n",
    "        'GLH': 7\n",
    "    }\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by(['LobName','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL','Admin1Name','Admin1Id'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            aggregated_tables_lob_stats.append(grouped)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    # Check if any tables were aggregated\n",
    "    if not aggregated_tables_lob_stats:\n",
    "        print(\"No tables were aggregated. Please check the input files.\")\n",
    "    else:\n",
    "        # Concatenate all the grouped Tables\n",
    "        final_table = pa.concat_tables(aggregated_tables_lob_stats)\n",
    "\n",
    "        # Group the final Table again to ensure all groups are combined\n",
    "        final_grouped = final_table.group_by(['LobName','Admin1Name','Admin1Id']).aggregate([('AAL', 'sum')])\n",
    "\n",
    "        # Sort the final grouped Table by 'AAL' in descending order\n",
    "        final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "        # Convert the final grouped Table to a Pandas DataFrame\n",
    "        final_df = final_grouped.to_pandas()\n",
    "\n",
    "        # Map LobName to LobId\n",
    "        final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "        final_df['Admin1Id'] = final_df['Admin1Id'].astype('int64')\n",
    "        final_df['Admin1Id'] = final_df['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "        final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "        # Define the columns with NaN values for 'Std' and 'CV'\n",
    "        final_df_STATS_Lob['Std'] = np.nan\n",
    "        final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "        # Reorder the columns to match the specified format\n",
    "        final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName','Admin1Name','Admin1Id']]\n",
    "\n",
    "        # Define the desired schema\n",
    "        desired_schema = pa.schema([\n",
    "            pa.field('AAL', pa.float64()),\n",
    "            pa.field('Std', pa.float64()),\n",
    "            pa.field('CV', pa.float64()),\n",
    "            pa.field('Admin1Id', pa.decimal128(38)),\n",
    "            pa.field('Admin1Name', pa.string()),\n",
    "            pa.field('LobId', pa.decimal128(38)),\n",
    "            pa.field('LobName', pa.string())\n",
    "        ])\n",
    "\n",
    "        # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "        final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "        pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1_Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_Lob_GU_0.parquet')\n",
    "process_lob_stats_Admin1_Lob(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1_Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_Lob_GR_0.parquet')\n",
    "process_lob_stats_Admin1_Lob(parquet_files_grp_gr, parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "def process_portfolio_stats_Admin1(parquet_files, export_path):\n",
    "    aggregated_tables = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by(['Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select([ 'AAL','Admin1Name','Admin1Id'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        aggregated_tables.append(grouped)\n",
    "\n",
    "    # Concatenate all the grouped Tables\n",
    "    final_table = pa.concat_tables(aggregated_tables)\n",
    "    final_table=final_table.group_by(['Admin1Name','Admin1Id']).aggregate([('AAL', 'sum')])\n",
    "    final_table=final_table.sort_by([('AAL_sum', 'descending')])\n",
    "    final_table=final_table.rename_columns(['Admin1Name','Admin1Id','AAL'])\n",
    "\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "    final_df=final_df.sort_values(by='AAL')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].astype('int64')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "    final_df['Std']=np.nan\n",
    "    final_df['CV']=np.nan\n",
    "    # Get the exact values of Admin1Id and Admin1Name\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    #total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('Admin1Id', pa.decimal128(38)),\n",
    "        pa.field('Admin1Name', pa.string()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_GU_0.parquet')\n",
    "process_portfolio_stats_Admin1(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Admin1', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_GR_0.parquet')\n",
    "process_portfolio_stats_Admin1(parquet_files_grp_gr, parquet_file_path_gr)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean():\n",
    "#     # List of variables to keep\n",
    "#     variables_to_keep = [\n",
    "#         'os', 'pd', 'pa', 'pq', 'np', 'pc', 'gc', 'Decimal', 'ds', 'shutil', 'time', 'sa', 'pyodbc',\n",
    "#         'start_time', 'flush_cache','country','output_folder_path', 'folder_path', 'folder_path_gr', 'speriod', 'samples',\n",
    "#         'parquet_files', 'parquet_files_gr', 'delete_folder_and_files', 'main_folder_path', 'processing_folder_path',\n",
    "#         'resolution_folder_path', 'resolution_folder_path_gr', 'parquet_files_grp', 'parquet_files_grp_gr'\n",
    "#     ]\n",
    "\n",
    "#     # Delete all variables except the ones to keep\n",
    "#     all_vars = list(globals().keys())\n",
    "#     for var in all_vars:\n",
    "#         if var not in variables_to_keep and not var.startswith(\"__\"):\n",
    "#             del globals()[var]\n",
    "\n",
    "#     # Run garbage collection to free up memory\n",
    "#     gc.collect()\n",
    "\n",
    "#     print(\"Memory cleanup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleanup completed.\n"
     ]
    }
   ],
   "source": [
    "# clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Cresta_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Cresta_Lob_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "#now for stats LOB GU Cresta_Lob\n",
    "\n",
    "\n",
    "def process_lob_stats_Cresta_Lob(parquet_files, parquet_file_path):\n",
    "    aggregated_tables_lob_stats = []\n",
    "\n",
    "    # Define the mapping of LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': 1,\n",
    "        'AUTO': 2,\n",
    "        'COM': 3,\n",
    "        'IND': 4,\n",
    "        'SPER': 5,\n",
    "        'FRST': 6,\n",
    "        'GLH': 7\n",
    "    }\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by(['LobName','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL','CrestaName','CrestaId'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            aggregated_tables_lob_stats.append(grouped)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    # Check if any tables were aggregated\n",
    "    if not aggregated_tables_lob_stats:\n",
    "        print(\"No tables were aggregated. Please check the input files.\")\n",
    "    else:\n",
    "        # Concatenate all the grouped Tables\n",
    "        final_table = pa.concat_tables(aggregated_tables_lob_stats)\n",
    "\n",
    "        # Group the final Table again to ensure all groups are combined\n",
    "        final_grouped = final_table.group_by(['LobName','CrestaName','CrestaId']).aggregate([('AAL', 'sum')])\n",
    "\n",
    "        # Sort the final grouped Table by 'AAL' in descending order\n",
    "        final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "        # Convert the final grouped Table to a Pandas DataFrame\n",
    "        final_df = final_grouped.to_pandas()\n",
    "\n",
    "        # Map LobName to LobId\n",
    "        final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "        final_df['CrestaId'] = final_df['CrestaId'].astype('int64')\n",
    "        final_df['CrestaId'] = final_df['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "        final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "        # Define the columns with NaN values for 'Std' and 'CV'\n",
    "        final_df_STATS_Lob['Std'] = np.nan\n",
    "        final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "        # Reorder the columns to match the specified format\n",
    "        final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName','CrestaName','CrestaId']]\n",
    "\n",
    "        # Define the desired schema\n",
    "        desired_schema = pa.schema([\n",
    "            pa.field('AAL', pa.float64()),\n",
    "            pa.field('Std', pa.float64()),\n",
    "            pa.field('CV', pa.float64()),\n",
    "            pa.field('CrestaId', pa.decimal128(38)),\n",
    "            pa.field('CrestaName', pa.string()),\n",
    "            pa.field('LobId', pa.decimal128(38)),\n",
    "            pa.field('LobName', pa.string())\n",
    "        ])\n",
    "\n",
    "        # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "        final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "        pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta_Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Cresta_Lob_GU_0.parquet')\n",
    "process_lob_stats_Cresta_Lob(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta_Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Cresta_Lob_GR_0.parquet')\n",
    "process_lob_stats_Cresta_Lob(parquet_files_grp_gr, parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Cresta_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Cresta_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "def process_portfolio_stats_Cresta(parquet_files, export_path):\n",
    "    aggregated_tables = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by(['CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select([ 'AAL','CrestaName','CrestaId'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        aggregated_tables.append(grouped)\n",
    "\n",
    "    # Concatenate all the grouped Tables\n",
    "    final_table = pa.concat_tables(aggregated_tables)\n",
    "    final_table=final_table.group_by(['CrestaName','CrestaId']).aggregate([('AAL', 'sum')])\n",
    "    final_table=final_table.sort_by([('AAL_sum', 'descending')])\n",
    "    final_table=final_table.rename_columns(['CrestaName','CrestaId','AAL'])\n",
    "\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "    final_df=final_df.sort_values(by='AAL')\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].astype('int64')\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].apply(lambda x: Decimal(x))\n",
    "    final_df['Std']=np.nan\n",
    "    final_df['CV']=np.nan\n",
    "    # Get the exact values of Admin1Id and Admin1Name\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    #total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('CrestaId', pa.decimal128(38)),\n",
    "        pa.field('CrestaName', pa.string()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Cresta_GU_0.parquet')\n",
    "process_portfolio_stats_Cresta(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Cresta', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Cresta_GR_0.parquet')\n",
    "process_portfolio_stats_Cresta(parquet_files_grp_gr, parquet_file_path_gr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Portfolio_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Portfolio_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GR_0.parquet\n",
      "Process finished in 1685.17 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for EP  Lob\n",
    "\n",
    "def process_parquet_files_EP_lob(parquet_files, export_path, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path):\n",
    "    processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(processing_folder_path, exist_ok=True)\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        # Skip if the filtered table is empty\n",
    "        if len(table) == 0:\n",
    "            continue\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Add LobID and LobName columns\n",
    "    final_df_EP_LOB_GU['LobID'] = lob_id\n",
    "    final_df_EP_LOB_GU['LobName'] = filter_string\n",
    "    final_df_EP_LOB_GU['LobID'] = final_df_EP_LOB_GU['LobID'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('LobID', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LobName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "\n",
    "# In[76]:\n",
    "\n",
    "\n",
    "export_path =os.path.join(main_folder_path, 'EP', 'Lob','GU')\n",
    "parquet_file_path_AUTO = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_6.parquet')\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#NEXT FOR GR\n",
    "\n",
    "\n",
    "# In[77]:\n",
    "\n",
    "\n",
    "export_path_gr =os.path.join(main_folder_path, 'EP', 'Lob','GR')\n",
    "parquet_file_path_AUTO = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_6.parquet')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "# In[82]:\n",
    "\n",
    "\n",
    "def process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values,parquet_file_path):\n",
    "    processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(processing_folder_path, exist_ok=True)\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "\n",
    "# In[86]:\n",
    "\n",
    "\n",
    "export_path =os.path.join(main_folder_path, 'EP', 'Portfolio','GU')\n",
    "parquet_file_path = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GU_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values, parquet_file_path)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "# In[87]:\n",
    "\n",
    "\n",
    "export_path_GR =os.path.join(main_folder_path,'EP','Portfolio','GR')\n",
    "parquet_file_path_GR = os.path.join(export_path_GR, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GR_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files_gr, export_path_GR, speriod, samples, rps_values, parquet_file_path_GR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "#now for stats LOB GU \n",
    "\n",
    "\n",
    "\n",
    "def process_lob_stats(parquet_files, parquet_file_path):\n",
    "    aggregated_tables_lob_stats = []\n",
    "\n",
    "    # Define the mapping of LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': 1,\n",
    "        'AUTO': 2,\n",
    "        'COM': 3,\n",
    "        'IND': 4,\n",
    "        'SPER': 5,\n",
    "        'FRST': 6,\n",
    "        'GLH': 7\n",
    "    }\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            aggregated_tables_lob_stats.append(grouped)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    # Check if any tables were aggregated\n",
    "    if not aggregated_tables_lob_stats:\n",
    "        print(\"No tables were aggregated. Please check the input files.\")\n",
    "    else:\n",
    "        # Concatenate all the grouped Tables\n",
    "        final_table = pa.concat_tables(aggregated_tables_lob_stats)\n",
    "\n",
    "        # Group the final Table again to ensure all groups are combined\n",
    "        final_grouped = final_table.group_by('LobName').aggregate([('AAL', 'sum')])\n",
    "\n",
    "        # Sort the final grouped Table by 'AAL' in descending order\n",
    "        final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "        # Convert the final grouped Table to a Pandas DataFrame\n",
    "        final_df = final_grouped.to_pandas()\n",
    "\n",
    "        # Map LobName to LobId\n",
    "        final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "\n",
    "        final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "        # Define the columns with NaN values for 'Std' and 'CV'\n",
    "        final_df_STATS_Lob['Std'] = np.nan\n",
    "        final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "        # Reorder the columns to match the specified format\n",
    "        final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName']]\n",
    "\n",
    "        # Define the desired schema\n",
    "        desired_schema = pa.schema([\n",
    "            pa.field('AAL', pa.float64()),\n",
    "            pa.field('Std', pa.float64()),\n",
    "            pa.field('CV', pa.float64()),\n",
    "            pa.field('LobId', pa.decimal128(38)),\n",
    "            pa.field('LobName', pa.string())\n",
    "        ])\n",
    "\n",
    "        # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "        final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "        pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Lob_GU_0.parquet')\n",
    "process_lob_stats(parquet_files, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "# In[93]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Lob_GR_0.parquet')\n",
    "process_lob_stats(parquet_files_gr, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#Portfolio STATS \n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "def process_portfolio_stats(parquet_files, export_path):\n",
    "    aggregated_tables = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select(['LobName', 'AAL'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        aggregated_tables.append(grouped)\n",
    "\n",
    "    # Concatenate all the grouped Tables\n",
    "    final_table = pa.concat_tables(aggregated_tables)\n",
    "\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Create a DataFrame with the specified columns\n",
    "    final_df_STATS_Portfolio = pd.DataFrame({\n",
    "        'AAL': [total_aal],\n",
    "        'Std': [np.nan],\n",
    "        'CV': [np.nan],\n",
    "    })\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df_STATS_Portfolio, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[106]:\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Portfolio_GU_0.parquet')\n",
    "process_portfolio_stats(parquet_files, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "# In[107]:\n",
    "\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Portfolio_GR_0.parquet')\n",
    "process_portfolio_stats(parquet_files_gr, parquet_file_path_gr)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#NOW FOR PLT LOB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_PLT_lob(parquet_files, export_path):\n",
    "    # Directory to store intermediate results\n",
    "    intermediate_dir = os.path.join(main_folder_path, 'intermediate_results')\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file in chunks and write intermediate results to disk\n",
    "    for i, file in enumerate(parquet_files):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            \n",
    "            # Cast columns to the desired types\n",
    "            table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('LobId'), 'LobId', pa.compute.cast(table['LobId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('LobName'), 'LobName', pa.compute.cast(table['LobName'], pa.string()))\n",
    "            \n",
    "            grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "            pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "    # Read intermediate results and combine them\n",
    "    intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "    intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "    combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "    # Perform the final group by and aggregation\n",
    "    final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "    final_grouped_table = final_grouped_table.sort_by([('Loss_sum_sum', 'descending')])\n",
    "\n",
    "    # Rename the aggregated column\n",
    "    final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "    # Reorder the columns in the desired order\n",
    "    final_grouped_table = final_grouped_table.select(ordered_columns)\n",
    "\n",
    "    # Save the final table to a Parquet file\n",
    "        # Delete intermediate files\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "        # Remove the intermediate directory\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {intermediate_dir}\")\n",
    "    except OSError:\n",
    "        print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "    try:\n",
    "        pq.write_table(final_grouped_table, export_path)\n",
    "        print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Parquet file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('LobName', pa.string(), nullable=True)\n",
    "])\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n",
    "\n",
    "\n",
    "# In[123]:\n",
    "\n",
    "\n",
    "#for GU\n",
    "\n",
    "\n",
    "# In[124]:\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GU_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files, export_path)\n",
    "\n",
    "\n",
    "# In[121]:\n",
    "\n",
    "\n",
    "#for GR\n",
    "\n",
    "\n",
    "# In[125]:\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GR_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files_gr, export_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GR_0.parquet\n",
      "Process finished in 2480.50 minutes\n"
     ]
    }
   ],
   "source": [
    "flush_cache()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#PLT Portfolio\n",
    "\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight']\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight']\n",
    "\n",
    "def process_PLT_portfolio_2(parquet_files, export_path):\n",
    "    # Flush memory at the beginning\n",
    "    gc.collect()\n",
    "\n",
    "    # Directory to store intermediate results\n",
    "    intermediate_dir = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', 'intermediate_results')\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file in chunks and write intermediate results to disk\n",
    "    for i, file in enumerate(parquet_files):\n",
    "        parquet_file = pq.ParquetFile(file)\n",
    "        for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            # Cast columns to the desired types\n",
    "            table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "            \n",
    "\n",
    "\n",
    "            grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "            pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "    # Read intermediate results and combine them\n",
    "    intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "    intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "    combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "    # Perform the final group by and aggregation\n",
    "    final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "\n",
    "    # Rename the aggregated column\n",
    "    final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "    \n",
    "    # Convert the table to the specified schema\n",
    "    final_grouped_table = pa.Table.from_arrays(\n",
    "        [final_grouped_table.column(name).cast(schema.field(name).type) for name in schema.names],\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    final_grouped_table = final_grouped_table.sort_by([('Loss', 'descending')])\n",
    "\n",
    "    # Save the final table to a Parquet file\n",
    "    # Delete intermediate files\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "        # Remove the intermediate directory\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {intermediate_dir}\")\n",
    "    except OSError:\n",
    "        print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "    try:\n",
    "        pq.write_table(final_grouped_table, export_path)\n",
    "        print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Parquet file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_GU_0.parquet')\n",
    "\n",
    "process_PLT_portfolio_2(parquet_files, export_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "# In[129]:\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_GR_0.parquet')\n",
    "\n",
    "process_PLT_portfolio_2(parquet_files_gr, export_path)\n",
    "\n",
    "\n",
    "#updates made after here \n",
    "\n",
    "# # Define the folder to be zipped\n",
    "# folder_to_zip = main_folder_path  # Change this to your folder path\n",
    "\n",
    "# # Get the parent directory and zip file name\n",
    "# parent_dir, folder_name = os.path.split(folder_to_zip)\n",
    "# output_zip = os.path.join(parent_dir, folder_name)  # Same name as folder\n",
    "\n",
    "# # Create a zip archive\n",
    "# shutil.make_archive(output_zip, 'zip', folder_to_zip)\n",
    "\n",
    "# # Remove the original folder after zipping\n",
    "# shutil.rmtree(folder_to_zip)\n",
    "\n",
    "# print(f\"Replaced '{folder_to_zip}' with '{output_zip}.zip'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()  # End time\n",
    "elapsed_time = (end_time - start_time) / 60  # Convert seconds to minutes\n",
    "\n",
    "print(f\"Process finished in {elapsed_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
