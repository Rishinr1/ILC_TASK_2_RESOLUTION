{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_files_Port_2(parquet_files, export_path, speriod, samples, rps_values,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate', 'Admin1Name', 'Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['Admin1Id']\n",
    "        admin1_name = row['Admin1Name']\n",
    "\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['Admin1Id'], admin1_id))\n",
    "\n",
    "        # Convert to pandas DataFrame\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "            fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "            fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs','Admin1Name','Admin1Id']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs','Admin1Name','Admin1Id']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "        final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].astype('int64')\n",
    "        final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('Admin1Id', pa.decimal128(38, 0), nullable=True),\n",
    "            pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "        #FOR GU\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Admin1',Cat)\n",
    "        parquet_file_path = os.path.join(export_path,f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_{Cat}_{idx}.parquet')\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "try:\n",
    "    process_parquet_files_Port_2(parquet_files_grp, export_path, speriod, samples, rps_values,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "#FOR GR\n",
    "try:\n",
    "    process_parquet_files_Port_2(parquet_files_grp_gr, export_path_GR, speriod, samples, rps_values,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values,parquet_file_path):\n",
    "#     partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "#     concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "#     os.makedirs(partial_folder_path, exist_ok=True)\n",
    "#     os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "#     # Initialize an empty list to store the results\n",
    "#     final_grouped_table_1 = []\n",
    "\n",
    "#     # Process each Parquet file individually\n",
    "#     for file in parquet_files:\n",
    "#         # Read the Parquet file into a PyArrow Table\n",
    "#         table = pq.read_table(file)\n",
    "\n",
    "#         grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "#         grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "#         # Write intermediate results to disk\n",
    "#         pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "#     # Read all intermediate files and concatenate them\n",
    "#     intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "#     final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "#     final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "#     # Perform final grouping and sorting\n",
    "#     f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "#     sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "#     pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "#     # Delete all non-concatenated files\n",
    "#     for f in intermediate_files_1:\n",
    "#         os.remove(f)\n",
    "    \n",
    "#     dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "#     dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "#     dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "#     dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "#     dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "#     dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "#     dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "#     dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "#     dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "#     dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "#     dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "#     dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "#     dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "#     dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "#     dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "#     dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "#     dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "#     dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "#     dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "#     fdataframe_2 = pd.DataFrame()\n",
    "#     fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "#     for value in rps_values:\n",
    "#         closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "#         fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "#         fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "#     fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "#     columns_to_keep_2 = ['RPs','Admin1Name','Admin1Id']\n",
    "#     columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "#     melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "#     melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "#     final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "#     fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "#     columns_to_keep_3 = ['RPs','Admin1Name','Admin1Id']\n",
    "#     columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "#     melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "#     melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "#     final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "#     final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "#     new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "#     final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "#     final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "#     final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].astype('int64')\n",
    "#     final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "#     # Define the schema to match the required Parquet file schema\n",
    "#     schema = pa.schema([\n",
    "#         pa.field('EPType', pa.string(), nullable=True),\n",
    "#         pa.field('Loss', pa.float64(), nullable=True),\n",
    "#         pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "#         pa.field('Admin1Id',pa.decimal128(38, 0), nullable=True),\n",
    "#         pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "#     ])\n",
    "\n",
    "#     # Convert DataFrame to Arrow Table with the specified schema\n",
    "#     table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "\n",
    "#     # Save to Parquet\n",
    "#     pq.write_table(table, parquet_file_path)\n",
    "\n",
    "#     print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "# #FOR GU\n",
    "\n",
    "# export_path =os.path.join(main_folder_path,'EP','Admin1','GU')\n",
    "# parquet_file_path = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_0.parquet')\n",
    "# try:\n",
    "#     process_parquet_files_Port(parquet_files_grp, export_path, speriod, samples, rps_values, parquet_file_path)\n",
    "# except (NameError, AttributeError,ValueError) as e:\n",
    "#     print(f\"Error processing : {e}\")\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #FOR GR\n",
    "\n",
    "\n",
    "# export_path_GR =os.path.join(main_folder_path,'EP','Admin1','GR')\n",
    "# parquet_file_path_GR = os.path.join(export_path_GR, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_0.parquet')\n",
    "# try:\n",
    "#     process_parquet_files_Port(parquet_files_grp_gr, export_path_GR, speriod, samples, rps_values, parquet_file_path_GR)\n",
    "# except (NameError, AttributeError,ValueError) as e:\n",
    "#     print(f\"Error processing : {e}\")\n",
    "#     pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_parquet_files_2(parquet_files, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        \n",
    "        # Skip if the filtered table is empty\n",
    "        \n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['Admin1Id']\n",
    "        admin1_name = row['Admin1Name']\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['Admin1Id'], admin1_id))\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "            fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "            fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'Admin1Name', 'Admin1Id']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "        # Add LobID and LobName columns\n",
    "        final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "        final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "        final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "        final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].astype('int64')\n",
    "        final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('Admin1Id',pa.int64(), nullable=True),\n",
    "            pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "            pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "            pa.field('LOBName', pa.string(), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "        export_path =os.path.join(main_folder_path,'EP','Admin1_Lob',Cat)\n",
    "        parquet_file_path = os.path.join(export_path, f\"{os.path.splitext(parquet_file_path)[0]}_{idx}.parquet\")\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "parquet_file_path_AUTO = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_1.parquet'\n",
    "parquet_file_path_AGR =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_0.parquet'\n",
    "parquet_file_path_COM =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_2.parquet'\n",
    "parquet_file_path_IND =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_3.parquet'\n",
    "parquet_file_path_SPER =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_4.parquet'\n",
    "parquet_file_path_FRST=  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_5.parquet'\n",
    "parquet_file_path_GLH = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_6.parquet'\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp,  'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp,  'COM', 3, speriod, samples, rps_values, parquet_file_path_COM,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp,  'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp,  'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "#for GR\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr,  'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr,  'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr,  'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_parquet_files_2(parquet_files, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['A'], filter_string))\n",
    "        \n",
    "        # Skip if the filtered table is empty\n",
    "        \n",
    "        grouped_table_1 = table.group_by(['A', 'B', 'C','d','e']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['A', 'B', 'C','d','e','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['A', 'B', 'C','d','e']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['d', 'e']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['d']\n",
    "        admin1_name = row['e']\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['D'], admin1_id))\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "        final_df_EP_LOB_GU=dataframe_1\n",
    "        \n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('A', pa.string(), nullable=True),\n",
    "            pa.field('b', pa.float64(), nullable=True),\n",
    "            pa.field('c', pa.float64(), nullable=True),\n",
    "            pa.field('d',pa.int64(), nullable=True),\n",
    "            pa.field('e', pa.string(), nullable=True),\n",
    "            pa.field('f', pa.decimal128(38, 0), nullable=True),\n",
    "            pa.field('g', pa.string(), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "        export_path =os.path.join(main_folder_path,'EP','Admin1_Lob',Cat)\n",
    "        parquet_file_path = os.path.join(export_path, f\"{parquet_file_path}_{idx}.parquet\")\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "parquet_file_path_AUTO = f'1.parquet'\n",
    "parquet_file_path_AGR =  f'0.parquet'\n",
    "parquet_file_path_COM =  f'2.parquet'\n",
    "parquet_file_path_IND =  f'3.parquet'\n",
    "parquet_file_path_SPER =  f'4.parquet'\n",
    "parquet_file_path_FRST=  f'5.parquet'\n",
    "parquet_file_path_GLH = f'6.parquet'\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp,  'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for GR\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr,  'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr,  'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
