{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "from decimal import Decimal  # Add this import statement\n",
    "import pyarrow.dataset as ds\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import sqlalchemy as sa\n",
    "import pyodbc\n",
    "import concurrent.futures\n",
    "\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # Start time\n",
    "\n",
    "\n",
    "# Function to flush the cache\n",
    "def flush_cache():\n",
    "    gc.collect()\n",
    "\n",
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = input(\"Enter the output folder path: \")\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_resolution\\input\\PARQUET_FILES'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_gr = r'D:\\RISHIN\\13_ILC_TASK\\input\\PARQUET_FILES_GR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speriod=int(input(\"Enter the simulation period: \"))\n",
    "samples=int(input(\"Enter the number of samples: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files_gr = [os.path.join(folder_path_gr, f) for f in os.listdir(folder_path_gr) if f.endswith('.parquet')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_folder_and_files(folder_path):\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        # Delete all files inside the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        \n",
    "        # Delete the folder itself\n",
    "        os.rmdir(folder_path)\n",
    "        print(f'Successfully deleted the folder: {folder_path}')\n",
    "    else:\n",
    "        print(f'The folder {folder_path} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio','Admin1','Admin1_Lob','Cresta','Cresta_Lob',]\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            # Filter nested folders for 'PLT'\n",
    "            if subfolder == 'PLT':\n",
    "                filtered_nested_folders = ['Lob', 'Portfolio']\n",
    "            else:\n",
    "                filtered_nested_folders = nested_folders\n",
    "            \n",
    "            for nested_folder in filtered_nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "\n",
    "processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "resolution_folder_path = os.path.join(processing_folder_path, 'Resolution Added')\n",
    "resolution_folder_path_gr = os.path.join(processing_folder_path, 'Resolution Added_gr')\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'Partial')   \n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'Concatenated')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_0_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_10_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_10_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_0_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_0_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_10_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_11_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_11_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_12_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_11_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_12_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_13_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_12_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_13_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_13_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_14_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_14_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_15_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_15_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_14_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_15_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_1_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_1_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_2_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_3_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_2_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_3_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_2_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_3_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_4_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_4_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_5_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_5_100.parquetSaved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_4_100.parquet\n",
      "\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_6_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_5_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_7_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_6_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_7_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_8_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_7_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_8_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_8_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_9_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_9_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added\\PLT_9_200.parquet\n"
     ]
    }
   ],
   "source": [
    "def connect_to_database(server, database):\n",
    "    connection_string = f'mssql+pyodbc://{server}/{database}?driver=SQL+Server+Native+Client+11.0'\n",
    "    engine = sa.create_engine(connection_string)\n",
    "    connection = engine.connect()\n",
    "    return connection\n",
    "\n",
    "def read_parquet_file(file_path):\n",
    "    table = pq.read_table(file_path)\n",
    "    return table\n",
    "\n",
    "def fetch_database_data(connection):\n",
    "    address_query = 'SELECT ADDRESSID, ADMIN1GEOID AS Admin1Id, Admin1Name, zone1GEOID AS CrestaId, Zone1 AS CrestaName FROM Address'\n",
    "    address_df = pd.read_sql(address_query, connection)\n",
    "    address_table = pa.Table.from_pandas(address_df)\n",
    "    return address_table\n",
    "\n",
    "def join_dataframes(parquet_table, address_table):\n",
    "    parquet_df = parquet_table.to_pandas()\n",
    "    address_df = address_table.to_pandas()\n",
    "    df = parquet_df.merge(address_df, left_on='LocationId', right_on='ADDRESSID', how='left')\n",
    "    return pa.Table.from_pandas(df)\n",
    "\n",
    "def save_joined_dataframe(joined_table, output_file):\n",
    "    pq.write_table(joined_table, output_file)\n",
    "    print(f\"Saved joined file to {output_file}\")\n",
    "\n",
    "def process_file(file, address_table, output_folder):\n",
    "    gc.collect()\n",
    "\n",
    "    parquet_table = read_parquet_file(file)\n",
    "    joined_table = join_dataframes(parquet_table, address_table)\n",
    "    output_file = os.path.join(output_folder, os.path.basename(file))\n",
    "    save_joined_dataframe(joined_table, output_file)\n",
    "    del parquet_table\n",
    "    del joined_table\n",
    "    gc.collect()\n",
    "\n",
    "def process_parquet_files(folder_path, output_folder, server, database, batch_size=5):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    connection = connect_to_database(server, database)\n",
    "    address_table = fetch_database_data(connection)\n",
    "\n",
    "    parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "    connection.close()\n",
    "\n",
    "    # Process files in batches\n",
    "    for i in range(0, len(parquet_files), batch_size):\n",
    "        batch_files = parquet_files[i:i + batch_size]\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_file, file, address_table, output_folder) for file in batch_files]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                future.result()\n",
    "\n",
    "server = 'localhost'\n",
    "database = 'IED2024_EUWS_PC_MIX_EDM230_ILC_LOB_UPDATE_20240905'\n",
    "process_parquet_files(folder_path, resolution_folder_path, server, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_0_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_0_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_10_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_0_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_10_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_10_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_11_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_11_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_12_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_11_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_13_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_12_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_13_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_13_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_12_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_14_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_15_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_14_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_15_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_14_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_15_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_1_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_2_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_1_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_1_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_3_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_2_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_3_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_3_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_2_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_4_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_4_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_5_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_5_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_4_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_5_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_6_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_7_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_6_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_6_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_7_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_8_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_8_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_7_200.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_8_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_9_300.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_9_100.parquet\n",
      "Saved joined file to D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Resolution Added_gr\\PLT_9_200.parquet\n"
     ]
    }
   ],
   "source": [
    "process_parquet_files(folder_path_gr, resolution_folder_path_gr, server, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Admin_1 Lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files_grp = [os.path.join(resolution_folder_path, f) for f in os.listdir(resolution_folder_path) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files_grp_gr = [os.path.join(resolution_folder_path_gr, f) for f in os.listdir(resolution_folder_path_gr) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Partial\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EP_ Admin1 Lob updated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "('int or Decimal object expected, got str', 'Conversion failed for column LOBName with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_resolution\\EP_final_2.1.1.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m rps_values \u001b[39m=\u001b[39m [\u001b[39m10000\u001b[39m, \u001b[39m5000\u001b[39m, \u001b[39m1000\u001b[39m, \u001b[39m500\u001b[39m, \u001b[39m250\u001b[39m, \u001b[39m200\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m25\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m2\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m     process_parquet_files_2(parquet_files_grp,  \u001b[39m'\u001b[39;49m\u001b[39mAGR\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m1\u001b[39;49m, speriod, samples, rps_values, parquet_file_path_AGR,\u001b[39m\"\u001b[39;49m\u001b[39mGU\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mNameError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m,\u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError processing AGR: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\RISHIN\\13_ILC_resolution\\EP_final_2.1.1.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m schema \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mschema([\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     pa\u001b[39m.\u001b[39mfield(\u001b[39m'\u001b[39m\u001b[39mEPType\u001b[39m\u001b[39m'\u001b[39m, pa\u001b[39m.\u001b[39mstring(), nullable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     pa\u001b[39m.\u001b[39mfield(\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m, pa\u001b[39m.\u001b[39mfloat64(), nullable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     pa\u001b[39m.\u001b[39mfield(\u001b[39m'\u001b[39m\u001b[39mLOBId\u001b[39m\u001b[39m'\u001b[39m, pa\u001b[39m.\u001b[39mstring(), nullable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m ])\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39m# Convert DataFrame to Arrow Table with the specified schema\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_pandas(final_df_EP_LOB_GU, schema\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m export_path \u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(main_folder_path,\u001b[39m'\u001b[39m\u001b[39mEP\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mAdmin1_Lob\u001b[39m\u001b[39m'\u001b[39m,Cat)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_resolution/EP_final_2.1.1.ipynb#X26sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m parquet_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(export_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(parquet_file_path)[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m.parquet\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\table.pxi:4751\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\pandas_compat.py:639\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[1;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39misinstance\u001b[39m(arr, np\u001b[39m.\u001b[39mndarray) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    635\u001b[0m             arr\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mcontiguous \u001b[39mand\u001b[39;00m\n\u001b[0;32m    636\u001b[0m             \u001b[39missubclass\u001b[39m(arr\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, np\u001b[39m.\u001b[39minteger))\n\u001b[0;32m    638\u001b[0m \u001b[39mif\u001b[39;00m nthreads \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 639\u001b[0m     arrays \u001b[39m=\u001b[39m [convert_column(c, f)\n\u001b[0;32m    640\u001b[0m               \u001b[39mfor\u001b[39;00m c, f \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(columns_to_convert, convert_fields)]\n\u001b[0;32m    641\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    642\u001b[0m     arrays \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\pandas_compat.py:626\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[1;34m(col, field)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid,\n\u001b[0;32m    622\u001b[0m         pa\u001b[39m.\u001b[39mArrowNotImplementedError,\n\u001b[0;32m    623\u001b[0m         pa\u001b[39m.\u001b[39mArrowTypeError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    624\u001b[0m     e\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mConversion failed for column \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    625\u001b[0m                \u001b[39m.\u001b[39mformat(col\u001b[39m.\u001b[39mname, col\u001b[39m.\u001b[39mdtype),)\n\u001b[1;32m--> 626\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m field_nullable \u001b[39mand\u001b[39;00m result\u001b[39m.\u001b[39mnull_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    628\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mField \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m was non-nullable but pandas column \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    629\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mhad \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m null values\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mstr\u001b[39m(field),\n\u001b[0;32m    630\u001b[0m                                                  result\u001b[39m.\u001b[39mnull_count))\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\pandas_compat.py:620\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[1;34m(col, field)\u001b[0m\n\u001b[0;32m    617\u001b[0m     type_ \u001b[39m=\u001b[39m field\u001b[39m.\u001b[39mtype\n\u001b[0;32m    619\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m     result \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49marray(col, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49mtype_, from_pandas\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, safe\u001b[39m=\u001b[39;49msafe)\n\u001b[0;32m    621\u001b[0m \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid,\n\u001b[0;32m    622\u001b[0m         pa\u001b[39m.\u001b[39mArrowNotImplementedError,\n\u001b[0;32m    623\u001b[0m         pa\u001b[39m.\u001b[39mArrowTypeError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    624\u001b[0m     e\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mConversion failed for column \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    625\u001b[0m                \u001b[39m.\u001b[39mformat(col\u001b[39m.\u001b[39mname, col\u001b[39m.\u001b[39mdtype),)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\array.pxi:362\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\array.pxi:87\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_resolution\\env\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowTypeError\u001b[0m: ('int or Decimal object expected, got str', 'Conversion failed for column LOBName with type object')"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_parquet_files_2(parquet_files, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        \n",
    "        # Skip if the filtered table is empty\n",
    "        \n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['Admin1Id']\n",
    "        admin1_name = row['Admin1Name']\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['Admin1Id'], admin1_id))\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "            fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "            fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'Admin1Name', 'Admin1Id']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs', 'Admin1Name', 'Admin1Id']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "        # Add LobID and LobName columns\n",
    "        final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "        final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "        final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "        final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].astype('int64')\n",
    "        final_df_EP_LOB_GU['Admin1Id'] = final_df_EP_LOB_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('Admin1Id',pa.int64(), nullable=True),\n",
    "            pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "            pa.field('LOBName', pa.string(38, 0), nullable=True),\n",
    "            pa.field('LOBId', pa.decimal128(38, 0)(), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "        export_path =os.path.join(main_folder_path,'EP','Admin1_Lob',Cat)\n",
    "        parquet_file_path = os.path.join(export_path, f\"{os.path.splitext(parquet_file_path)[0]}_{idx}.parquet\")\n",
    "        # Count the number of underscores in the file path\n",
    "        underscore_count = parquet_file_path.count('_')\n",
    "\n",
    "        # If there are 21 or more underscores, modify the file path\n",
    "        if underscore_count >= 22:\n",
    "            parts = parquet_file_path.split('_')\n",
    "            # Remove the second last part which contains the number and the underscore before it\n",
    "            parts = parts[:-2] + parts[-1:]\n",
    "            parquet_file_path = '_'.join(parts)\n",
    "\n",
    "        # Write the table to the parquet file\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "parquet_file_path_AUTO = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_1.parquet'\n",
    "parquet_file_path_AGR =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_0.parquet'\n",
    "parquet_file_path_COM =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_2.parquet'\n",
    "parquet_file_path_IND =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_3.parquet'\n",
    "parquet_file_path_SPER =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_4.parquet'\n",
    "parquet_file_path_FRST=  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_5.parquet'\n",
    "parquet_file_path_GLH = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GU_6.parquet'\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp,  'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp,  'COM', 3, speriod, samples, rps_values, parquet_file_path_COM,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp,  'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp,  'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "parquet_file_path_AUTO = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GR_1.parquet'\n",
    "parquet_file_path_AGR =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GR_0.parquet'\n",
    "parquet_file_path_COM =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GR_2.parquet'\n",
    "parquet_file_path_IND =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GR_3.parquet'\n",
    "parquet_file_path_SPER =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GR_4.parquet'\n",
    "parquet_file_path_FRST=  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GR_5.parquet'\n",
    "parquet_file_path_GLH = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_Lob_GR_6.parquet'\n",
    "\n",
    "#for GR\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr,  'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr,  'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr,  'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_2(parquet_files_grp_gr, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated file above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Admin 1 (portfolio)modified according to unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GU_10.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_9.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Admin1_GR_10.parquet\n"
     ]
    }
   ],
   "source": [
    "def process_parquet_files_Port_2(parquet_files, speriod, samples, rps_values,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Admin1Name','Admin1Id','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate', 'Admin1Name', 'Admin1Id']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['Admin1Id', 'Admin1Name']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['Admin1Id']\n",
    "        admin1_name = row['Admin1Name']\n",
    "\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['Admin1Id'], admin1_id))\n",
    "\n",
    "        # Convert to pandas DataFrame\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','Admin1Name','Admin1Id'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "            fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "            fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs','Admin1Name','Admin1Id']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs','Admin1Name','Admin1Id']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','Admin1Name','Admin1Id']]\n",
    "\n",
    "        final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "        final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].astype('int64')\n",
    "        final_df_EP_Portfolio_GU['Admin1Id'] = final_df_EP_Portfolio_GU['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('Admin1Id', pa.decimal128(38, 0), nullable=True),\n",
    "            pa.field('Admin1Name', pa.string(), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "        #FOR GU\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Admin1',Cat)\n",
    "        parquet_file_path = os.path.join(export_path,f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Admin1_{Cat}_{idx}.parquet')\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "try:\n",
    "    process_parquet_files_Port_2(parquet_files_grp, speriod, samples, rps_values,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "#FOR GR\n",
    "try:\n",
    "    process_parquet_files_Port_2(parquet_files_grp_gr, speriod, samples, rps_values,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#above modified according to unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EP cresta lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_0_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_1_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_UR_EP_Cresta_Lob_GU_2_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_UR_EP_Cresta_Lob_GU_2_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_UR_EP_Cresta_Lob_GU_2_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_UR_EP_Cresta_Lob_GU_2_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_UR_EP_Cresta_Lob_GU_2_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_UR_EP_Cresta_Lob_GU_2_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_UR_EP_Cresta_Lob_GU_2_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_UR_EP_Cresta_Lob_GU_2_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_UR_EP_Cresta_Lob_GU_2_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_3_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GU_4_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_0_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_1_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_2_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_3_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_Lob_GR_4_8.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_parquet_files_EP_Cresta_lob_2(parquet_files, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        \n",
    "        # Skip if the filtered table is empty\n",
    "        \n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['CrestaName','CrestaId']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['CrestaId']\n",
    "        admin1_name = row['CrestaName']\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['CrestaId'], admin1_id))\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "            fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "            fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs', 'CrestaName','CrestaId']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs', 'CrestaName', 'CrestaId']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "        final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "        # Add LobID and LobName columns\n",
    "        final_df_EP_LOB_GU['LOBId'] = lob_id\n",
    "        final_df_EP_LOB_GU['LOBName'] = filter_string\n",
    "        final_df_EP_LOB_GU['LOBId'] = final_df_EP_LOB_GU['LOBId'].apply(lambda x: Decimal(x))\n",
    "        final_df_EP_LOB_GU['CrestaId'] = final_df_EP_LOB_GU['CrestaId'].astype('int64')\n",
    "        final_df_EP_LOB_GU['CrestaId'] = final_df_EP_LOB_GU['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('CrestaId',pa.int64(), nullable=True),\n",
    "            pa.field('CrestaName', pa.string(), nullable=True),\n",
    "            pa.field('LOBName', pa.string(), nullable=True),\n",
    "            pa.field('LOBId', pa.decimal128(38, 0), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "        export_path =os.path.join(main_folder_path,'EP','Cresta_Lob',Cat)\n",
    "        parquet_file_path = os.path.join(export_path, f\"{os.path.splitext(parquet_file_path)[0]}_{idx}.parquet\")\n",
    "        # Count the number of underscores in the file path\n",
    "        underscore_count = parquet_file_path.count('_')\n",
    "\n",
    "        # If there are 21 or more underscores, modify the file path\n",
    "        if underscore_count >= 22:\n",
    "            parts = parquet_file_path.split('_')\n",
    "            # Remove the second last part which contains the number and the underscore before it\n",
    "            parts = parts[:-2] + parts[-1:]\n",
    "            parquet_file_path = '_'.join(parts)\n",
    "\n",
    "        # Write the table to the parquet file\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path_AUTO =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GU_1.parquet'\n",
    "parquet_file_path_AGR = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GU_0.parquet'\n",
    "parquet_file_path_COM = f'ILC2024_EUWS_PLA_WI_EP_{country}_UR_EP_Cresta_Lob_GU_2.parquet'\n",
    "parquet_file_path_IND = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GU_3.parquet'\n",
    "parquet_file_path_SPER = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GU_4.parquet'\n",
    "parquet_file_path_FRST= f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GU_5.parquet'\n",
    "parquet_file_path_GLH = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GU_6.parquet'\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp,  'COM', 3, speriod, samples, rps_values, parquet_file_path_COM,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp,  'IND', 4, speriod, samples, rps_values, parquet_file_path_IND,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp,  'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp,  'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH,\"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#NEXT FOR GR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path_AUTO =  f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GR_1.parquet'\n",
    "parquet_file_path_AGR = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GR_0.parquet'\n",
    "parquet_file_path_COM = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GR_2.parquet'\n",
    "parquet_file_path_IND = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GR_3.parquet'\n",
    "parquet_file_path_SPER = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GR_4.parquet'\n",
    "parquet_file_path_FRST= f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GR_5.parquet'\n",
    "parquet_file_path_GLH = f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_Lob_GR_6.parquet'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp_gr,  'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp_gr,  'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp_gr,  'COM', 3, speriod, samples, rps_values, parquet_file_path_COM,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp_gr,  'IND', 4, speriod, samples, rps_values, parquet_file_path_IND,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp_gr,  'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp_gr, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_Cresta_lob_2(parquet_files_grp_gr, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GU_8.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_4.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_5.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_6.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_7.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Cresta_GR_8.parquet\n"
     ]
    }
   ],
   "source": [
    "#for cresta portfolio\n",
    "\n",
    "def process_parquet_files_Port_EP_Cresta_2(parquet_files, speriod, samples, rps_values,Cat):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','CrestaName','CrestaId','Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate', 'CrestaName','CrestaId']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "     # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Get distinct Admin1Id and Admin1Name\n",
    "    distinct_admins = sorted_final_table_1.select(['CrestaName','CrestaId']).to_pandas().drop_duplicates()\n",
    "    distinct_admins = distinct_admins.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    for idx, row in distinct_admins.iterrows():\n",
    "        admin1_id = row['CrestaId']\n",
    "        admin1_name = row['CrestaName']\n",
    "\n",
    "        # Filter the table for the current Admin1Id and Admin1Name\n",
    "        filtered_table = sorted_final_table_1.filter(pa.compute.equal(sorted_final_table_1['CrestaId'], admin1_id))\n",
    "\n",
    "        # Convert to pandas DataFrame\n",
    "        dataframe_1 = filtered_table.to_pandas()\n",
    "        dataframe_2 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "        dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "        dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "        dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "        dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "        dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "        dataframe_3 = dataframe_1.groupby(['PeriodId','CrestaName','CrestaId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "        dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "        dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "        dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "        dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "        dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "        dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "        dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "        fdataframe_2 = pd.DataFrame()\n",
    "        fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "        for value in rps_values:\n",
    "            closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "            fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "            fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "        fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "        columns_to_keep_2 = ['RPs','CrestaName','CrestaId']\n",
    "        columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "        melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "        melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "        fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "        columns_to_keep_3 = ['RPs','CrestaName','CrestaId']\n",
    "        columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "        melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "        melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "        final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod','CrestaName','CrestaId']]\n",
    "\n",
    "        final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "        new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "        final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "        final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "        final_df_EP_Portfolio_GU['CrestaId'] = final_df_EP_Portfolio_GU['CrestaId'].astype('int64')\n",
    "        final_df_EP_Portfolio_GU['CrestaId'] = final_df_EP_Portfolio_GU['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "        # Define the schema to match the required Parquet file schema\n",
    "        schema = pa.schema([\n",
    "            pa.field('EPType', pa.string(), nullable=True),\n",
    "            pa.field('Loss', pa.float64(), nullable=True),\n",
    "            pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "            pa.field('CrestaId', pa.decimal128(38, 0), nullable=True),\n",
    "            pa.field('CrestaName', pa.string(), nullable=True),\n",
    "        ])\n",
    "\n",
    "        # Convert DataFrame to Arrow Table with the specified schema\n",
    "        table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "        #FOR GU\n",
    "\n",
    "        export_path =os.path.join(main_folder_path,'EP','Cresta',Cat)\n",
    "        parquet_file_path = os.path.join(export_path,f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Cresta_{Cat}_{idx}.parquet')\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "try:\n",
    "    process_parquet_files_Port_EP_Cresta_2(parquet_files_grp, speriod, samples, rps_values, \"GU\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_Port_EP_Cresta_2(parquet_files_grp_gr, speriod, samples, rps_values,\"GR\")\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_Lob_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "#now for stats LOB GU admin\n",
    "\n",
    "\n",
    "def process_lob_stats_Admin1_Lob(parquet_files, parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "    aggregated_tables_lob_stats = []\n",
    "\n",
    "    # Define the mapping of LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': 1,\n",
    "        'AUTO': 2,\n",
    "        'COM': 3,\n",
    "        'IND': 4,\n",
    "        'SPER': 5,\n",
    "        'FRST': 6,\n",
    "        'GLH': 7\n",
    "    }\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by(['LobName','Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL','Admin1Name','Admin1Id'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    \n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Group the final Table again to ensure all groups are combined\n",
    "    final_grouped = final_table.group_by(['LobName','Admin1Name','Admin1Id']).aggregate([('AAL', 'sum')])\n",
    "\n",
    "    # Sort the final grouped Table by 'AAL' in descending order\n",
    "    final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "    pq.write_table(final_grouped, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    flush_cache()\n",
    "\n",
    "    # Convert the final grouped Table to a Pandas DataFrame\n",
    "    final_df = final_grouped.to_pandas()\n",
    "    \n",
    "\n",
    "    # Map LobName to LobId\n",
    "    final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].astype('int64')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "    # Define the columns with NaN values for 'Std' and 'CV'\n",
    "    final_df_STATS_Lob['Std'] = np.nan\n",
    "    final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "    # Reorder the columns to match the specified format\n",
    "    final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName','Admin1Name','Admin1Id']]\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('Admin1Id', pa.decimal128(38)),\n",
    "        pa.field('Admin1Name', pa.string()),\n",
    "        pa.field('LobId', pa.decimal128(38)),\n",
    "        pa.field('LobName', pa.string())\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1_Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_Lob_GU_0.parquet')\n",
    "process_lob_stats_Admin1_Lob(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1_Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_Lob_GR_0.parquet')\n",
    "process_lob_stats_Admin1_Lob(parquet_files_grp_gr, parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\Concatenated\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Admin1\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Admin1_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "#stats admin1\n",
    "\n",
    "def process_portfolio_stats_Admin1(parquet_files, export_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by(['Admin1Name','Admin1Id']).aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select([ 'AAL','Admin1Name','Admin1Id'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        \n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Concatenate all the grouped Tables\n",
    "    final_table=final_table.group_by(['Admin1Name','Admin1Id']).aggregate([('AAL', 'sum')])\n",
    "    final_table=final_table.sort_by([('AAL_sum', 'descending')])\n",
    "    final_table=final_table.rename_columns(['Admin1Name','Admin1Id','AAL'])\n",
    "\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "    final_df=final_df.sort_values(by='AAL')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].astype('int64')\n",
    "    final_df['Admin1Id'] = final_df['Admin1Id'].apply(lambda x: Decimal(x))\n",
    "    final_df['Std']=np.nan\n",
    "    final_df['CV']=np.nan\n",
    "    # Get the exact values of Admin1Id and Admin1Name\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    #total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('Admin1Id', pa.decimal128(38)),\n",
    "        pa.field('Admin1Name', pa.string()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Admin1', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_GU_0.parquet')\n",
    "process_portfolio_stats_Admin1(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Admin1', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Admin1_GR_0.parquet')\n",
    "process_portfolio_stats_Admin1(parquet_files_grp_gr, parquet_file_path_gr)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean():\n",
    "#     # List of variables to keep\n",
    "#     variables_to_keep = [\n",
    "#         'os', 'pd', 'pa', 'pq', 'np', 'pc', 'gc', 'Decimal', 'ds', 'shutil', 'time', 'sa', 'pyodbc',\n",
    "#         'start_time', 'flush_cache','country','output_folder_path', 'folder_path', 'folder_path_gr', 'speriod', 'samples',\n",
    "#         'parquet_files', 'parquet_files_gr', 'delete_folder_and_files', 'main_folder_path', 'processing_folder_path',\n",
    "#         'resolution_folder_path', 'resolution_folder_path_gr', 'parquet_files_grp', 'parquet_files_grp_gr'\n",
    "#     ]\n",
    "\n",
    "#     # Delete all variables except the ones to keep\n",
    "#     all_vars = list(globals().keys())\n",
    "#     for var in all_vars:\n",
    "#         if var not in variables_to_keep and not var.startswith(\"__\"):\n",
    "#             del globals()[var]\n",
    "\n",
    "#     # Run garbage collection to free up memory\n",
    "#     gc.collect()\n",
    "\n",
    "#     print(\"Memory cleanup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleanup completed.\n"
     ]
    }
   ],
   "source": [
    "# clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Cresta_Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Cresta_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Cresta_Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Cresta_Lob_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "#now for stats LOB GU Cresta_Lob\n",
    "\n",
    "\n",
    "def process_lob_stats_Cresta_Lob(parquet_files, parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    aggregated_tables_lob_stats = []\n",
    "\n",
    "    # Define the mapping of LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': 1,\n",
    "        'AUTO': 2,\n",
    "        'COM': 3,\n",
    "        'IND': 4,\n",
    "        'SPER': 5,\n",
    "        'FRST': 6,\n",
    "        'GLH': 7\n",
    "    }\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by(['LobName','CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL','CrestaName','CrestaId'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "    # Group the final Table again to ensure all groups are combined\n",
    "    final_grouped = final_table.group_by(['LobName','CrestaName','CrestaId']).aggregate([('AAL', 'sum')])\n",
    "\n",
    "    # Sort the final grouped Table by 'AAL' in descending order\n",
    "    final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "    # Convert the final grouped Table to a Pandas DataFrame\n",
    "    final_df = final_grouped.to_pandas()\n",
    "\n",
    "    # Map LobName to LobId\n",
    "    final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].astype('int64')\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "    # Define the columns with NaN values for 'Std' and 'CV'\n",
    "    final_df_STATS_Lob['Std'] = np.nan\n",
    "    final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "    # Reorder the columns to match the specified format\n",
    "    final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName','CrestaName','CrestaId']]\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('CrestaId', pa.decimal128(38)),\n",
    "        pa.field('CrestaName', pa.string()),\n",
    "        pa.field('LobId', pa.decimal128(38)),\n",
    "        pa.field('LobName', pa.string())\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta_Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Cresta_Lob_GU_0.parquet')\n",
    "process_lob_stats_Cresta_Lob(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta_Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Cresta_Lob_GR_0.parquet')\n",
    "process_lob_stats_Cresta_Lob(parquet_files_grp_gr, parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Cresta\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Cresta_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_6\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Cresta\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Cresta_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "def process_portfolio_stats_Cresta(parquet_files, export_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by(['CrestaName','CrestaId']).aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select([ 'AAL','CrestaName','CrestaId'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "    final_table=final_table.group_by(['CrestaName','CrestaId']).aggregate([('AAL', 'sum')])\n",
    "    final_table=final_table.sort_by([('AAL_sum', 'descending')])\n",
    "    final_table=final_table.rename_columns(['CrestaName','CrestaId','AAL'])\n",
    "\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "    final_df=final_df.sort_values(by='AAL')\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].astype('int64')\n",
    "    final_df['CrestaId'] = final_df['CrestaId'].apply(lambda x: Decimal(x))\n",
    "    final_df['Std']=np.nan\n",
    "    final_df['CV']=np.nan\n",
    "    # Get the exact values of Admin1Id and Admin1Name\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    #total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('CrestaId', pa.decimal128(38)),\n",
    "        pa.field('CrestaName', pa.string()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Cresta', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Cresta_GU_0.parquet')\n",
    "process_portfolio_stats_Cresta(parquet_files_grp, parquet_file_path)\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Cresta', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Cresta_GR_0.parquet')\n",
    "process_portfolio_stats_Cresta(parquet_files_grp_gr, parquet_file_path_gr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GR_0.parquet\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Portfolio_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Portfolio_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GR_0.parquet\n",
      "Process finished in 1685.17 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for EP  Lob\n",
    "\n",
    "def process_parquet_files_EP_lob(parquet_files, export_path, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path):\n",
    "    processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(processing_folder_path, exist_ok=True)\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        # Skip if the filtered table is empty\n",
    "        if len(table) == 0:\n",
    "            continue\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Add LobID and LobName columns\n",
    "    final_df_EP_LOB_GU['LobID'] = lob_id\n",
    "    final_df_EP_LOB_GU['LobName'] = filter_string\n",
    "    final_df_EP_LOB_GU['LobID'] = final_df_EP_LOB_GU['LobID'].apply(lambda x: Decimal(x))\n",
    "\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('LobID', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LobName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "\n",
    "# In[76]:\n",
    "\n",
    "\n",
    "export_path =os.path.join(main_folder_path, 'EP', 'Lob','GU')\n",
    "parquet_file_path_AUTO = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_6.parquet')\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#NEXT FOR GR\n",
    "\n",
    "\n",
    "# In[77]:\n",
    "\n",
    "\n",
    "export_path_gr =os.path.join(main_folder_path, 'EP', 'Lob','GR')\n",
    "parquet_file_path_AUTO = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_6.parquet')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files_EP_lob(parquet_files_gr, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "# In[82]:\n",
    "\n",
    "\n",
    "def process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values,parquet_file_path):\n",
    "    processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(processing_folder_path, exist_ok=True)\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "\n",
    "# In[86]:\n",
    "\n",
    "\n",
    "export_path =os.path.join(main_folder_path, 'EP', 'Portfolio','GU')\n",
    "parquet_file_path = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GU_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values, parquet_file_path)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "# In[87]:\n",
    "\n",
    "\n",
    "export_path_GR =os.path.join(main_folder_path,'EP','Portfolio','GR')\n",
    "parquet_file_path_GR = os.path.join(export_path_GR, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GR_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files_gr, export_path_GR, speriod, samples, rps_values, parquet_file_path_GR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "#now for stats LOB GU \n",
    "\n",
    "\n",
    "\n",
    "def process_lob_stats(parquet_files, parquet_file_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    # Define the mapping of LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': 1,\n",
    "        'AUTO': 2,\n",
    "        'COM': 3,\n",
    "        'IND': 4,\n",
    "        'SPER': 5,\n",
    "        'FRST': 6,\n",
    "        'GLH': 7\n",
    "    }\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    \n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Group the final Table again to ensure all groups are combined\n",
    "    final_grouped = final_table.group_by('LobName').aggregate([('AAL', 'sum')])\n",
    "\n",
    "    # Sort the final grouped Table by 'AAL' in descending order\n",
    "    final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "    # Convert the final grouped Table to a Pandas DataFrame\n",
    "    final_df = final_grouped.to_pandas()\n",
    "\n",
    "    # Map LobName to LobId\n",
    "    final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "\n",
    "    final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "    # Define the columns with NaN values for 'Std' and 'CV'\n",
    "    final_df_STATS_Lob['Std'] = np.nan\n",
    "    final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "    # Reorder the columns to match the specified format\n",
    "    final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName']]\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('LobId', pa.decimal128(38)),\n",
    "        pa.field('LobName', pa.string())\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "#LOB GU STATS\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Lob_GU_0.parquet')\n",
    "process_lob_stats(parquet_files, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#LOB GR STATS\n",
    "\n",
    "\n",
    "# In[93]:\n",
    "\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Lob_GR_0.parquet')\n",
    "process_lob_stats(parquet_files_gr, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#Portfolio STATS \n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "def process_portfolio_stats(parquet_files, export_path):\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select(['LobName', 'AAL'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        pq.write_table(grouped, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "    final_table = pa.concat_tables(final_grouped_table_1)\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Create a DataFrame with the specified columns\n",
    "    final_df_STATS_Portfolio = pd.DataFrame({\n",
    "        'AAL': [total_aal],\n",
    "        'Std': [np.nan],\n",
    "        'CV': [np.nan],\n",
    "    })\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df_STATS_Portfolio, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    delete_folder_and_files(partial_folder_path)\n",
    "    delete_folder_and_files(concatenated_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[106]:\n",
    "\n",
    "\n",
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Portfolio_GU_0.parquet')\n",
    "process_portfolio_stats(parquet_files, parquet_file_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#GR.\n",
    "\n",
    "\n",
    "# In[107]:\n",
    "\n",
    "\n",
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Portfolio_GR_0.parquet')\n",
    "process_portfolio_stats(parquet_files_gr, parquet_file_path_gr)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW FOR PLT LOB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Lob_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "def process_PLT_lob(parquet_files, export_path):\n",
    "    # Directory to store intermediate results\n",
    "    intermediate_dir = os.path.join(main_folder_path, 'intermediate_results')\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file in chunks and write intermediate results to disk\n",
    "    for i, file in enumerate(parquet_files):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            \n",
    "            # Cast columns to the desired types\n",
    "            table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('LobId'), 'LobId', pa.compute.cast(table['LobId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('LobName'), 'LobName', pa.compute.cast(table['LobName'], pa.string()))\n",
    "            \n",
    "            grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "            pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "    # Read intermediate results and combine them\n",
    "    intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "    intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "    combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "    # Perform the final group by and aggregation\n",
    "    final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "    final_grouped_table = final_grouped_table.sort_by([('Loss_sum_sum', 'descending')])\n",
    "\n",
    "    # Rename the aggregated column\n",
    "    final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "    # Reorder the columns in the desired order\n",
    "    final_grouped_table = final_grouped_table.select(ordered_columns)\n",
    "\n",
    "    # Save the final table to a Parquet file\n",
    "        # Delete intermediate files\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "        # Remove the intermediate directory\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {intermediate_dir}\")\n",
    "    except OSError:\n",
    "        print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "    try:\n",
    "        pq.write_table(final_grouped_table, export_path)\n",
    "        print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Parquet file: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('LobName', pa.string(), nullable=True)\n",
    "])\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n",
    "\n",
    "\n",
    "# In[123]:\n",
    "\n",
    "\n",
    "#for GU\n",
    "\n",
    "\n",
    "# In[124]:\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GU_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files, export_path)\n",
    "\n",
    "\n",
    "# In[121]:\n",
    "\n",
    "\n",
    "#for GR\n",
    "\n",
    "\n",
    "# In[125]:\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GR_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files_gr, export_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "flush_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GR_0.parquet\n",
      "Process finished in 2480.50 minutes\n"
     ]
    }
   ],
   "source": [
    "flush_cache()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#PLT Portfolio\n",
    "\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight']\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight']\n",
    "\n",
    "def process_PLT_portfolio_2(parquet_files, export_path):\n",
    "    # Flush memory at the beginning\n",
    "    gc.collect()\n",
    "\n",
    "    # Directory to store intermediate results\n",
    "    intermediate_dir = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', 'intermediate_results')\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file in chunks and write intermediate results to disk\n",
    "    for i, file in enumerate(parquet_files):\n",
    "        parquet_file = pq.ParquetFile(file)\n",
    "        for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            # Cast columns to the desired types\n",
    "            table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "            \n",
    "\n",
    "\n",
    "            grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "            pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "    # Read intermediate results and combine them\n",
    "    intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "    intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "    combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "    # Perform the final group by and aggregation\n",
    "    final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "\n",
    "    # Rename the aggregated column\n",
    "    final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "    \n",
    "    # Convert the table to the specified schema\n",
    "    final_grouped_table = pa.Table.from_arrays(\n",
    "        [final_grouped_table.column(name).cast(schema.field(name).type) for name in schema.names],\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    final_grouped_table = final_grouped_table.sort_by([('Loss', 'descending')])\n",
    "\n",
    "    # Save the final table to a Parquet file\n",
    "    # Delete intermediate files\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "        # Remove the intermediate directory\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {intermediate_dir}\")\n",
    "    except OSError:\n",
    "        print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "    try:\n",
    "        pq.write_table(final_grouped_table, export_path)\n",
    "        print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Parquet file: {e}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR GU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_GU_0.parquet')\n",
    "\n",
    "process_PLT_portfolio_2(parquet_files, export_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#FOR GR\n",
    "\n",
    "\n",
    "# In[129]:\n",
    "\n",
    "\n",
    "export_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_GR_0.parquet')\n",
    "\n",
    "process_PLT_portfolio_2(parquet_files_gr, export_path)\n",
    "\n",
    "\n",
    "#updates made after here \n",
    "\n",
    "# # Define the folder to be zipped\n",
    "# folder_to_zip = main_folder_path  # Change this to your folder path\n",
    "\n",
    "# # Get the parent directory and zip file name\n",
    "# parent_dir, folder_name = os.path.split(folder_to_zip)\n",
    "# output_zip = os.path.join(parent_dir, folder_name)  # Same name as folder\n",
    "\n",
    "# # Create a zip archive\n",
    "# shutil.make_archive(output_zip, 'zip', folder_to_zip)\n",
    "\n",
    "# # Remove the original folder after zipping\n",
    "# shutil.rmtree(folder_to_zip)\n",
    "\n",
    "# print(f\"Replaced '{folder_to_zip}' with '{output_zip}.zip'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()  # End time\n",
    "elapsed_time = (end_time - start_time) / 60  # Convert seconds to minutes\n",
    "\n",
    "print(f\"Process finished in {elapsed_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial does not exist.\n",
      "The folder D:\\RISHIN\\TESTING\\TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated does not exist.\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
